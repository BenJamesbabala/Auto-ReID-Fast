08/02 06:13:54 PM | Logger is set 
08/02 06:13:54 PM | Logger with distribution
08/02 06:15:12 PM | Initializing dataset used 79.83948588371277 basic time unit
08/02 06:15:12 PM | Initializing dataset used 79.84849834442139 basic time unit
08/02 06:15:12 PM | Initializing dataset used 79.8395779132843 basic time unit
08/02 06:15:12 PM | The training classes labels length :  751
08/02 06:15:12 PM | The training classes labels length :  751
08/02 06:15:12 PM | The training classes labels length :  751
08/02 06:15:20 PM | Initializing dataset used 87.03619456291199 basic time unit
08/02 06:15:20 PM | The training classes labels length :  751
08/02 06:15:20 PM | Initializing dataset used 86.31032586097717 basic time unit
08/02 06:15:20 PM | The training classes labels length :  751
08/02 06:15:20 PM | Initializing dataset used 87.07275319099426 basic time unit
08/02 06:15:20 PM | The training classes labels length :  751
08/02 06:15:20 PM | Initializing dataset used 87.10305881500244 basic time unit
08/02 06:15:20 PM | The training classes labels length :  751
08/02 06:15:21 PM | Initializing dataset used 87.60062646865845 basic time unit
08/02 06:15:21 PM | The training classes labels length :  751
08/02 06:17:21 PM | batch loading time example is 129.47983765602112
08/02 06:17:21 PM | batch loading time example is 129.48002791404724
08/02 06:17:21 PM | batch loading time example is 120.39552640914917
08/02 06:17:21 PM | batch loading time example is 120.89736866950989
08/02 06:17:21 PM | batch loading time example is 120.93507528305054
08/02 06:17:21 PM | batch loading time example is 120.9437460899353
08/02 06:17:21 PM | batch loading time example is 120.92814540863037
08/02 06:17:21 PM | batch loading time example is 129.48042845726013
####### ALPHA #######
# Alpha - normal
tensor([[0.1428, 0.1428, 0.1430, 0.1426, 0.1429, 0.1430, 0.1428],
        [0.1429, 0.1428, 0.1429, 0.1430, 0.1429, 0.1429, 0.1427]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1430, 0.1428, 0.1429, 0.1427, 0.1426, 0.1428, 0.1431],
        [0.1430, 0.1429, 0.1430, 0.1431, 0.1426, 0.1427, 0.1428],
        [0.1427, 0.1428, 0.1431, 0.1428, 0.1428, 0.1429, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1428, 0.1428, 0.1429, 0.1428, 0.1428, 0.1428, 0.1431],
        [0.1431, 0.1428, 0.1428, 0.1428, 0.1427, 0.1429, 0.1429],
        [0.1428, 0.1427, 0.1428, 0.1428, 0.1429, 0.1429, 0.1431],
        [0.1427, 0.1429, 0.1427, 0.1430, 0.1428, 0.1429, 0.1429]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1428, 0.1429, 0.1427, 0.1429, 0.1429, 0.1428, 0.1429],
        [0.1426, 0.1427, 0.1431, 0.1430, 0.1428, 0.1430, 0.1428],
        [0.1428, 0.1427, 0.1432, 0.1430, 0.1427, 0.1428, 0.1427],
        [0.1429, 0.1427, 0.1429, 0.1428, 0.1428, 0.1431, 0.1429],
        [0.1429, 0.1426, 0.1429, 0.1429, 0.1428, 0.1430, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1428, 0.1428, 0.1429, 0.1428, 0.1429, 0.1426, 0.1432],
        [0.1428, 0.1427, 0.1430, 0.1429, 0.1431, 0.1429, 0.1427]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1430, 0.1429, 0.1429, 0.1428, 0.1428, 0.1429, 0.1428],
        [0.1430, 0.1428, 0.1429, 0.1428, 0.1430, 0.1428, 0.1428],
        [0.1431, 0.1427, 0.1429, 0.1427, 0.1428, 0.1429, 0.1429]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1429, 0.1427, 0.1427, 0.1429, 0.1426, 0.1430, 0.1431],
        [0.1430, 0.1430, 0.1431, 0.1426, 0.1427, 0.1427, 0.1430],
        [0.1431, 0.1428, 0.1426, 0.1429, 0.1429, 0.1430, 0.1427],
        [0.1426, 0.1430, 0.1428, 0.1426, 0.1432, 0.1428, 0.1430]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1428, 0.1427, 0.1427, 0.1431, 0.1429, 0.1428, 0.1429],
        [0.1432, 0.1430, 0.1427, 0.1428, 0.1427, 0.1430, 0.1427],
        [0.1426, 0.1428, 0.1430, 0.1427, 0.1429, 0.1429, 0.1430],
        [0.1427, 0.1430, 0.1430, 0.1427, 0.1427, 0.1428, 0.1430],
        [0.1429, 0.1429, 0.1428, 0.1428, 0.1428, 0.1429, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
08/02 06:21:28 PM | Train: [ 1/210] Step 000/404 Loss 6.632 Prec@(1,5) (0.0%, 0.0%)
08/02 06:22:17 PM | Train: [ 1/210] Step 010/404 Loss 6.618 Prec@(1,5) (0.3%, 4.0%)
08/02 06:23:04 PM | Train: [ 1/210] Step 020/404 Loss 6.657 Prec@(1,5) (0.1%, 2.1%)
08/02 06:23:32 PM | Train: [ 1/210] Final Prec@1 0.3906%
08/02 06:25:18 PM | Valid: [ 1/210] Step 000/023 Loss 23.486 Prec@(1,5) (0.0%, 0.0%)
08/02 06:25:21 PM | Valid: [ 1/210] Step 010/023 Loss 23.247 Prec@(1,5) (0.0%, 0.6%)
08/02 06:25:24 PM | Valid: [ 1/210] Step 020/023 Loss 23.401 Prec@(1,5) (0.0%, 0.4%)
08/02 06:25:32 PM | Valid: [ 1/210] Final Prec@1 0.1359%, Prec@5 0.5435%, Prec@10 1.7663%
08/02 06:25:32 PM | Final best Prec@1 = 0.1359%
08/02 06:25:33 PM | genotype = Genotype(normal=[[('dil_conv_3x3', 0), ('partial_aware_1x1', 1)], [('sep_conv_3x3', 1), ('dil_conv_3x3', 2)], [('sep_conv_3x3', 0), ('max_pool_3x3', 2)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('partial_aware_1x1', 0), ('partial_aware_1x1', 1)], [('partial_aware_1x1', 2), ('partial_aware_1x1', 0)], [('sep_conv_3x3', 3), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 4), ('sep_conv_3x3', 3)]], reduce_concat=range(2, 6))
08/02 08:45:41 PM | Logger is set 
08/02 08:45:41 PM | Logger with distribution
08/02 08:46:31 PM | Initializing dataset used 51.16460299491882 basic time unit
08/02 08:46:31 PM | Initializing dataset used 50.5233850479126 basic time unit
08/02 08:46:31 PM | Initializing dataset used 51.191869020462036 basic time unit
08/02 08:46:31 PM | Initializing dataset used 51.19189500808716 basic time unit
08/02 08:46:31 PM | Initializing dataset used 51.17135834693909 basic time unit
08/02 08:46:31 PM | Initializing dataset used 51.198506116867065 basic time unit
08/02 08:46:31 PM | Initializing dataset used 51.178205728530884 basic time unit
08/02 08:46:31 PM | Initializing dataset used 51.1849627494812 basic time unit
08/02 08:46:31 PM | The training classes labels length :  751
08/02 08:46:31 PM | The training classes labels length :  751
08/02 08:46:31 PM | The training classes labels length :  751
08/02 08:46:31 PM | The training classes labels length :  751
08/02 08:46:31 PM | The training classes labels length :  751
08/02 08:46:31 PM | The training classes labels length :  751
08/02 08:46:31 PM | The training classes labels length :  751
08/02 08:46:31 PM | The training classes labels length :  751
08/02 08:48:37 PM | batch loading time example is 125.87721157073975
08/02 08:48:37 PM | batch loading time example is 125.87724828720093
08/02 08:48:37 PM | batch loading time example is 125.87214279174805
08/02 08:48:37 PM | batch loading time example is 125.87725925445557
08/02 08:48:37 PM | batch loading time example is 125.87788581848145
08/02 08:48:37 PM | batch loading time example is 125.8774356842041
08/02 08:48:37 PM | batch loading time example is 125.87936544418335
08/02 08:48:37 PM | batch loading time example is 125.8795998096466
####### ALPHA #######
# Alpha - normal
tensor([[0.1428, 0.1428, 0.1430, 0.1426, 0.1429, 0.1430, 0.1428],
        [0.1429, 0.1428, 0.1429, 0.1430, 0.1429, 0.1429, 0.1427]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1430, 0.1428, 0.1429, 0.1427, 0.1426, 0.1428, 0.1431],
        [0.1430, 0.1429, 0.1430, 0.1431, 0.1426, 0.1427, 0.1428],
        [0.1427, 0.1428, 0.1431, 0.1428, 0.1428, 0.1429, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1428, 0.1428, 0.1429, 0.1428, 0.1428, 0.1428, 0.1431],
        [0.1431, 0.1428, 0.1428, 0.1428, 0.1427, 0.1429, 0.1429],
        [0.1428, 0.1427, 0.1428, 0.1428, 0.1429, 0.1429, 0.1431],
        [0.1427, 0.1429, 0.1427, 0.1430, 0.1428, 0.1429, 0.1429]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1428, 0.1429, 0.1427, 0.1429, 0.1429, 0.1428, 0.1429],
        [0.1426, 0.1427, 0.1431, 0.1430, 0.1428, 0.1430, 0.1428],
        [0.1428, 0.1427, 0.1432, 0.1430, 0.1427, 0.1428, 0.1427],
        [0.1429, 0.1427, 0.1429, 0.1428, 0.1428, 0.1431, 0.1429],
        [0.1429, 0.1426, 0.1429, 0.1429, 0.1428, 0.1430, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1428, 0.1428, 0.1429, 0.1428, 0.1429, 0.1426, 0.1432],
        [0.1428, 0.1427, 0.1430, 0.1429, 0.1431, 0.1429, 0.1427]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1430, 0.1429, 0.1429, 0.1428, 0.1428, 0.1429, 0.1428],
        [0.1430, 0.1428, 0.1429, 0.1428, 0.1430, 0.1428, 0.1428],
        [0.1431, 0.1427, 0.1429, 0.1427, 0.1428, 0.1429, 0.1429]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1429, 0.1427, 0.1427, 0.1429, 0.1426, 0.1430, 0.1431],
        [0.1430, 0.1430, 0.1431, 0.1426, 0.1427, 0.1427, 0.1430],
        [0.1431, 0.1428, 0.1426, 0.1429, 0.1429, 0.1430, 0.1427],
        [0.1426, 0.1430, 0.1428, 0.1426, 0.1432, 0.1428, 0.1430]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1428, 0.1427, 0.1427, 0.1431, 0.1429, 0.1428, 0.1429],
        [0.1432, 0.1430, 0.1427, 0.1428, 0.1427, 0.1430, 0.1427],
        [0.1426, 0.1428, 0.1430, 0.1427, 0.1429, 0.1429, 0.1430],
        [0.1427, 0.1430, 0.1430, 0.1427, 0.1427, 0.1428, 0.1430],
        [0.1429, 0.1429, 0.1428, 0.1428, 0.1428, 0.1429, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
08/02 08:52:43 PM | Train: [ 1/210] Step 000/404 Loss 6.632 Prec@(1,5) (0.0%, 0.0%)
08/02 08:53:28 PM | Train: [ 1/210] Step 010/404 Loss 6.617 Prec@(1,5) (0.3%, 4.0%)
08/02 08:54:13 PM | Train: [ 1/210] Step 020/404 Loss 6.656 Prec@(1,5) (0.1%, 2.4%)
08/02 08:54:39 PM | Train: [ 1/210] Final Prec@1 0.3906%
08/02 08:56:22 PM | Valid: [ 1/210] Step 000/023 Loss 29.729 Prec@(1,5) (0.0%, 0.0%)
08/02 08:56:25 PM | Valid: [ 1/210] Step 010/023 Loss 28.879 Prec@(1,5) (0.3%, 0.6%)
08/02 08:56:28 PM | Valid: [ 1/210] Step 020/023 Loss 28.886 Prec@(1,5) (0.1%, 0.6%)
08/02 08:56:32 PM | Valid: [ 1/210] Final Prec@1 0.1359%, Prec@5 0.6793%, Prec@10 1.3587%
08/02 08:56:32 PM | Final best Prec@1 = 0.1359%
08/02 08:56:32 PM | genotype = Genotype(normal=[[('dil_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('max_pool_3x3', 2), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('partial_aware_1x1', 0), ('partial_aware_1x1', 1)], [('partial_aware_1x1', 2), ('partial_aware_1x1', 0)], [('sep_conv_3x3', 3), ('partial_aware_1x1', 2)], [('sep_conv_3x3', 4), ('partial_aware_1x1', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1131, 0.1107, 0.1116, 0.1784, 0.1298, 0.2170, 0.1392],
        [0.1157, 0.1124, 0.1150, 0.1746, 0.1744, 0.1521, 0.1558]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1262, 0.1132, 0.1160, 0.1958, 0.1352, 0.1677, 0.1461],
        [0.1143, 0.1112, 0.1126, 0.1919, 0.1322, 0.1654, 0.1724],
        [0.1392, 0.1195, 0.1195, 0.1609, 0.1243, 0.2032, 0.1333]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1168, 0.1121, 0.1152, 0.1795, 0.1565, 0.1696, 0.1503],
        [0.1383, 0.1305, 0.1344, 0.1259, 0.1544, 0.1502, 0.1662],
        [0.1767, 0.1614, 0.1437, 0.1143, 0.1189, 0.1380, 0.1471],
        [0.1580, 0.1460, 0.1518, 0.1539, 0.1212, 0.1351, 0.1339]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1201, 0.1154, 0.1169, 0.1665, 0.1722, 0.1544, 0.1545],
        [0.1298, 0.1257, 0.1281, 0.1615, 0.1430, 0.1559, 0.1559],
        [0.1485, 0.1343, 0.1341, 0.1956, 0.1329, 0.1311, 0.1235],
        [0.1489, 0.1395, 0.1332, 0.1842, 0.1443, 0.1284, 0.1216],
        [0.1369, 0.1307, 0.1384, 0.1370, 0.1639, 0.1503, 0.1429]],
       grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1082, 0.1040, 0.1480, 0.1294, 0.1738, 0.1252, 0.2114],
        [0.1006, 0.0987, 0.1472, 0.1647, 0.1707, 0.1569, 0.1612]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0987, 0.0959, 0.1427, 0.1342, 0.1759, 0.1489, 0.2037],
        [0.1030, 0.1022, 0.1483, 0.1558, 0.1712, 0.1347, 0.1849],
        [0.0933, 0.0857, 0.0909, 0.1365, 0.1958, 0.1904, 0.2073]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0965, 0.0952, 0.1270, 0.1697, 0.1846, 0.1111, 0.2160],
        [0.0972, 0.0966, 0.1427, 0.1888, 0.1539, 0.1431, 0.1778],
        [0.0906, 0.0886, 0.0928, 0.1553, 0.1914, 0.1777, 0.2036],
        [0.0813, 0.0815, 0.0837, 0.2040, 0.1898, 0.1804, 0.1792]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0884, 0.0883, 0.1422, 0.1494, 0.1868, 0.1545, 0.1904],
        [0.0873, 0.0885, 0.1690, 0.1433, 0.1786, 0.1494, 0.1839],
        [0.0929, 0.0884, 0.0954, 0.1639, 0.1953, 0.1570, 0.2070],
        [0.0828, 0.0840, 0.0835, 0.1908, 0.1912, 0.1707, 0.1970],
        [0.0828, 0.0833, 0.0872, 0.2098, 0.1784, 0.1661, 0.1924]],
       grad_fn=<SoftmaxBackward>)
#####################
08/02 09:00:26 PM | Train: [ 2/210] Step 000/404 Loss 6.412 Prec@(1,5) (0.0%, 3.1%)
08/02 09:01:14 PM | Train: [ 2/210] Step 010/404 Loss 5.950 Prec@(1,5) (1.7%, 9.4%)
08/02 09:02:00 PM | Train: [ 2/210] Step 020/404 Loss 5.934 Prec@(1,5) (1.3%, 7.7%)
08/02 09:02:47 PM | Train: [ 2/210] Step 030/404 Loss 6.248 Prec@(1,5) (1.0%, 5.5%)
08/02 09:03:32 PM | Train: [ 2/210] Step 040/404 Loss 6.363 Prec@(1,5) (0.8%, 4.5%)
08/02 09:04:16 PM | Train: [ 2/210] Step 050/404 Loss 6.454 Prec@(1,5) (0.7%, 4.2%)
08/02 09:05:03 PM | Train: [ 2/210] Step 060/404 Loss 6.530 Prec@(1,5) (0.6%, 3.5%)
08/02 09:05:52 PM | Train: [ 2/210] Step 070/404 Loss 6.544 Prec@(1,5) (0.5%, 3.4%)
08/02 09:06:36 PM | Train: [ 2/210] Step 080/404 Loss 6.578 Prec@(1,5) (0.5%, 3.2%)
08/02 09:07:24 PM | Train: [ 2/210] Step 090/404 Loss 6.615 Prec@(1,5) (0.4%, 2.8%)
08/02 09:08:10 PM | Train: [ 2/210] Step 100/404 Loss 6.637 Prec@(1,5) (0.4%, 2.6%)
08/02 09:08:58 PM | Train: [ 2/210] Step 110/404 Loss 6.655 Prec@(1,5) (0.5%, 2.4%)
08/02 09:09:43 PM | Train: [ 2/210] Step 120/404 Loss 6.666 Prec@(1,5) (0.5%, 2.4%)
08/02 09:10:31 PM | Train: [ 2/210] Step 130/404 Loss 6.670 Prec@(1,5) (0.5%, 2.3%)
08/02 09:11:15 PM | Train: [ 2/210] Step 140/404 Loss 6.678 Prec@(1,5) (0.4%, 2.2%)
08/02 09:12:02 PM | Train: [ 2/210] Step 150/404 Loss 6.685 Prec@(1,5) (0.4%, 2.2%)
08/02 09:12:48 PM | Train: [ 2/210] Step 160/404 Loss 6.701 Prec@(1,5) (0.4%, 2.1%)
08/02 09:13:36 PM | Train: [ 2/210] Step 170/404 Loss 6.706 Prec@(1,5) (0.3%, 2.2%)
08/02 09:14:22 PM | Train: [ 2/210] Step 180/404 Loss 6.706 Prec@(1,5) (0.3%, 2.1%)
08/02 09:15:11 PM | Train: [ 2/210] Step 190/404 Loss 6.704 Prec@(1,5) (0.4%, 2.1%)
08/02 09:15:58 PM | Train: [ 2/210] Step 200/404 Loss 6.704 Prec@(1,5) (0.4%, 2.0%)
08/02 09:16:45 PM | Train: [ 2/210] Step 210/404 Loss 6.712 Prec@(1,5) (0.3%, 1.9%)
08/02 09:17:30 PM | Train: [ 2/210] Step 220/404 Loss 6.714 Prec@(1,5) (0.4%, 1.9%)
08/02 09:18:19 PM | Train: [ 2/210] Step 230/404 Loss 6.718 Prec@(1,5) (0.3%, 1.9%)
08/02 09:19:03 PM | Train: [ 2/210] Step 240/404 Loss 6.720 Prec@(1,5) (0.3%, 1.9%)
08/02 09:19:48 PM | Train: [ 2/210] Step 250/404 Loss 6.723 Prec@(1,5) (0.3%, 1.9%)
08/02 09:20:35 PM | Train: [ 2/210] Step 260/404 Loss 6.721 Prec@(1,5) (0.4%, 1.9%)
08/02 09:21:21 PM | Train: [ 2/210] Step 270/404 Loss 6.720 Prec@(1,5) (0.4%, 1.9%)
08/02 09:22:07 PM | Train: [ 2/210] Step 280/404 Loss 6.720 Prec@(1,5) (0.4%, 1.9%)
08/02 09:22:51 PM | Train: [ 2/210] Step 290/404 Loss 6.721 Prec@(1,5) (0.4%, 1.9%)
08/02 09:23:37 PM | Train: [ 2/210] Step 300/404 Loss 6.723 Prec@(1,5) (0.4%, 1.9%)
08/02 09:24:24 PM | Train: [ 2/210] Step 310/404 Loss 6.725 Prec@(1,5) (0.4%, 1.9%)
08/02 09:25:11 PM | Train: [ 2/210] Step 320/404 Loss 6.722 Prec@(1,5) (0.4%, 1.8%)
08/02 09:25:57 PM | Train: [ 2/210] Step 330/404 Loss 6.720 Prec@(1,5) (0.4%, 1.9%)
08/02 09:26:41 PM | Train: [ 2/210] Step 340/404 Loss 6.723 Prec@(1,5) (0.4%, 1.9%)
08/02 09:27:28 PM | Train: [ 2/210] Step 350/404 Loss 6.723 Prec@(1,5) (0.4%, 1.8%)
08/02 09:28:15 PM | Train: [ 2/210] Step 360/404 Loss 6.721 Prec@(1,5) (0.4%, 1.9%)
08/02 09:29:03 PM | Train: [ 2/210] Step 370/404 Loss 6.718 Prec@(1,5) (0.4%, 1.9%)
08/02 09:29:49 PM | Train: [ 2/210] Step 380/404 Loss 6.716 Prec@(1,5) (0.4%, 1.9%)
08/02 09:30:38 PM | Train: [ 2/210] Step 390/404 Loss 6.719 Prec@(1,5) (0.4%, 1.9%)
08/02 09:31:25 PM | Train: [ 2/210] Step 400/404 Loss 6.719 Prec@(1,5) (0.4%, 2.0%)
08/02 09:31:49 PM | Train: [ 2/210] Final Prec@1 0.4409%
08/02 09:33:38 PM | Valid: [ 2/210] Step 000/023 Loss 6.392 Prec@(1,5) (0.0%, 9.4%)
08/02 09:33:41 PM | Valid: [ 2/210] Step 010/023 Loss 6.672 Prec@(1,5) (1.4%, 4.8%)
08/02 09:33:44 PM | Valid: [ 2/210] Step 020/023 Loss 6.697 Prec@(1,5) (0.7%, 4.0%)
08/02 09:33:50 PM | Valid: [ 2/210] Final Prec@1 0.6793%, Prec@5 3.9402%, Prec@10 6.3859%
08/02 09:33:50 PM | Final best Prec@1 = 0.6793%
08/02 09:33:50 PM | genotype = Genotype(normal=[[('dil_conv_3x3', 0), ('partial_aware_1x1', 1)], [('dil_conv_3x3', 1), ('sep_conv_3x3', 0)], [('max_pool_3x3', 2), ('sep_conv_3x3', 3)], [('sep_conv_3x3', 2), ('partial_aware_1x1', 0)]], normal_concat=range(2, 6), reduce=[[('skip_connect', 1), ('skip_connect', 0)], [('dil_conv_3x3', 2), ('skip_connect', 1)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 3), ('sep_conv_3x3', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1219, 0.0875, 0.0905, 0.1860, 0.1173, 0.1934, 0.2034],
        [0.1333, 0.0950, 0.1001, 0.1674, 0.1975, 0.1703, 0.1364]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1340, 0.1009, 0.1049, 0.2190, 0.1136, 0.1684, 0.1592],
        [0.1141, 0.0931, 0.0954, 0.1896, 0.1408, 0.2247, 0.1423],
        [0.1515, 0.1067, 0.1112, 0.1757, 0.1125, 0.1855, 0.1569]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1233, 0.0928, 0.0960, 0.2058, 0.1513, 0.1582, 0.1726],
        [0.1640, 0.1223, 0.1278, 0.1172, 0.1513, 0.1420, 0.1755],
        [0.2218, 0.1392, 0.1168, 0.1088, 0.1071, 0.1383, 0.1680],
        [0.1287, 0.1036, 0.0940, 0.2325, 0.1105, 0.2278, 0.1029]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1205, 0.0982, 0.1006, 0.1548, 0.2052, 0.1722, 0.1486],
        [0.1435, 0.1161, 0.1185, 0.1661, 0.1230, 0.1799, 0.1529],
        [0.1453, 0.1158, 0.1073, 0.2290, 0.1253, 0.1578, 0.1196],
        [0.1623, 0.1319, 0.1090, 0.1824, 0.1250, 0.1683, 0.1210],
        [0.1805, 0.1368, 0.1274, 0.1431, 0.1756, 0.1268, 0.1099]],
       grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.0930, 0.0789, 0.1321, 0.0871, 0.0336, 0.0812, 0.4941],
        [0.0750, 0.0642, 0.1364, 0.1337, 0.0319, 0.1178, 0.4410]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0840, 0.0693, 0.0869, 0.0715, 0.0283, 0.0803, 0.5797],
        [0.0704, 0.0649, 0.1552, 0.1154, 0.0291, 0.0996, 0.4654],
        [0.0346, 0.0288, 0.0429, 0.1008, 0.0327, 0.1222, 0.6381]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0829, 0.0694, 0.1214, 0.0890, 0.0299, 0.0640, 0.5434],
        [0.0615, 0.0593, 0.1445, 0.1580, 0.0261, 0.0699, 0.4807],
        [0.0372, 0.0325, 0.0555, 0.1029, 0.0303, 0.0666, 0.6751],
        [0.0346, 0.0304, 0.0349, 0.0820, 0.0272, 0.0713, 0.7196]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0678, 0.0606, 0.0897, 0.0706, 0.0258, 0.0851, 0.6004],
        [0.0530, 0.0534, 0.1255, 0.0847, 0.0230, 0.0960, 0.5644],
        [0.0355, 0.0302, 0.0545, 0.0897, 0.0251, 0.0558, 0.7091],
        [0.0296, 0.0277, 0.0300, 0.1592, 0.0270, 0.0986, 0.6279],
        [0.0250, 0.0226, 0.0241, 0.1132, 0.0240, 0.0816, 0.7096]],
       grad_fn=<SoftmaxBackward>)
#####################
08/02 09:37:55 PM | Train: [ 3/210] Step 000/404 Loss 6.961 Prec@(1,5) (0.0%, 3.1%)
08/02 09:38:40 PM | Train: [ 3/210] Step 010/404 Loss 6.722 Prec@(1,5) (0.6%, 2.6%)
08/02 09:39:25 PM | Train: [ 3/210] Step 020/404 Loss 6.642 Prec@(1,5) (0.4%, 1.9%)
08/02 09:40:14 PM | Train: [ 3/210] Step 030/404 Loss 6.629 Prec@(1,5) (0.3%, 1.8%)
08/02 09:41:00 PM | Train: [ 3/210] Step 040/404 Loss 6.585 Prec@(1,5) (0.4%, 2.1%)
08/02 09:41:44 PM | Train: [ 3/210] Step 050/404 Loss 6.635 Prec@(1,5) (0.6%, 2.4%)
08/02 09:42:32 PM | Train: [ 3/210] Step 060/404 Loss 6.653 Prec@(1,5) (0.6%, 2.2%)
08/02 09:43:17 PM | Train: [ 3/210] Step 070/404 Loss 6.630 Prec@(1,5) (0.5%, 2.5%)
08/02 09:44:03 PM | Train: [ 3/210] Step 080/404 Loss 6.633 Prec@(1,5) (0.4%, 2.2%)
08/02 09:44:49 PM | Train: [ 3/210] Step 090/404 Loss 6.649 Prec@(1,5) (0.4%, 2.1%)
08/02 09:45:35 PM | Train: [ 3/210] Step 100/404 Loss 6.666 Prec@(1,5) (0.4%, 2.0%)
08/02 09:46:19 PM | Train: [ 3/210] Step 110/404 Loss 6.664 Prec@(1,5) (0.5%, 2.3%)
08/02 09:47:07 PM | Train: [ 3/210] Step 120/404 Loss 6.662 Prec@(1,5) (0.4%, 2.3%)
08/02 09:47:56 PM | Train: [ 3/210] Step 130/404 Loss 6.654 Prec@(1,5) (0.4%, 2.4%)
08/02 09:48:41 PM | Train: [ 3/210] Step 140/404 Loss 6.656 Prec@(1,5) (0.4%, 2.3%)
08/02 09:49:26 PM | Train: [ 3/210] Step 150/404 Loss 6.657 Prec@(1,5) (0.4%, 2.3%)
08/02 09:50:11 PM | Train: [ 3/210] Step 160/404 Loss 6.662 Prec@(1,5) (0.4%, 2.3%)
08/02 09:50:55 PM | Train: [ 3/210] Step 170/404 Loss 6.659 Prec@(1,5) (0.5%, 2.5%)
08/02 09:51:40 PM | Train: [ 3/210] Step 180/404 Loss 6.654 Prec@(1,5) (0.5%, 2.4%)
08/02 09:52:32 PM | Train: [ 3/210] Step 190/404 Loss 6.648 Prec@(1,5) (0.5%, 2.6%)
08/02 09:53:19 PM | Train: [ 3/210] Step 200/404 Loss 6.639 Prec@(1,5) (0.4%, 2.5%)
08/02 09:54:07 PM | Train: [ 3/210] Step 210/404 Loss 6.642 Prec@(1,5) (0.4%, 2.4%)
08/02 09:54:54 PM | Train: [ 3/210] Step 220/404 Loss 6.639 Prec@(1,5) (0.4%, 2.4%)
08/02 09:55:40 PM | Train: [ 3/210] Step 230/404 Loss 6.639 Prec@(1,5) (0.4%, 2.4%)
08/02 09:56:25 PM | Train: [ 3/210] Step 240/404 Loss 6.635 Prec@(1,5) (0.4%, 2.4%)
08/02 09:57:09 PM | Train: [ 3/210] Step 250/404 Loss 6.635 Prec@(1,5) (0.5%, 2.4%)
08/02 09:57:53 PM | Train: [ 3/210] Step 260/404 Loss 6.630 Prec@(1,5) (0.5%, 2.5%)
08/02 09:58:38 PM | Train: [ 3/210] Step 270/404 Loss 6.626 Prec@(1,5) (0.5%, 2.5%)
08/02 09:59:22 PM | Train: [ 3/210] Step 280/404 Loss 6.624 Prec@(1,5) (0.5%, 2.5%)
08/02 10:00:07 PM | Train: [ 3/210] Step 290/404 Loss 6.623 Prec@(1,5) (0.5%, 2.5%)
08/02 10:00:56 PM | Train: [ 3/210] Step 300/404 Loss 6.623 Prec@(1,5) (0.5%, 2.5%)
08/02 10:01:42 PM | Train: [ 3/210] Step 310/404 Loss 6.623 Prec@(1,5) (0.5%, 2.5%)
08/02 10:02:30 PM | Train: [ 3/210] Step 320/404 Loss 6.618 Prec@(1,5) (0.5%, 2.5%)
08/02 10:03:16 PM | Train: [ 3/210] Step 330/404 Loss 6.613 Prec@(1,5) (0.6%, 2.5%)
08/02 10:04:02 PM | Train: [ 3/210] Step 340/404 Loss 6.615 Prec@(1,5) (0.6%, 2.5%)
08/02 10:04:49 PM | Train: [ 3/210] Step 350/404 Loss 6.613 Prec@(1,5) (0.6%, 2.5%)
08/02 10:05:38 PM | Train: [ 3/210] Step 360/404 Loss 6.609 Prec@(1,5) (0.6%, 2.5%)
08/02 10:06:26 PM | Train: [ 3/210] Step 370/404 Loss 6.604 Prec@(1,5) (0.6%, 2.6%)
08/02 10:07:11 PM | Train: [ 3/210] Step 380/404 Loss 6.601 Prec@(1,5) (0.6%, 2.7%)
08/02 10:07:55 PM | Train: [ 3/210] Step 390/404 Loss 6.601 Prec@(1,5) (0.6%, 2.7%)
08/02 10:08:42 PM | Train: [ 3/210] Step 400/404 Loss 6.600 Prec@(1,5) (0.6%, 2.7%)
08/02 10:09:06 PM | Train: [ 3/210] Final Prec@1 0.5956%
08/02 10:11:00 PM | Valid: [ 3/210] Step 000/023 Loss 6.197 Prec@(1,5) (6.2%, 15.6%)
08/02 10:11:03 PM | Valid: [ 3/210] Step 010/023 Loss 6.575 Prec@(1,5) (2.3%, 6.5%)
08/02 10:11:06 PM | Valid: [ 3/210] Step 020/023 Loss 6.588 Prec@(1,5) (1.5%, 5.4%)
08/02 10:11:10 PM | Valid: [ 3/210] Final Prec@1 1.3587%, Prec@5 5.1630%, Prec@10 8.4239%
08/02 10:11:10 PM | Final best Prec@1 = 1.3587%
08/02 10:11:10 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 2), ('sep_conv_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 1)], [('dil_conv_3x3', 1), ('sep_conv_3x3', 2)]], normal_concat=range(2, 6), reduce=[[('dil_conv_3x3', 1), ('max_pool_3x3', 0)], [('dil_conv_3x3', 2), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 3), ('sep_conv_3x3', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1696, 0.0734, 0.0747, 0.2205, 0.1350, 0.1599, 0.1670],
        [0.2092, 0.0818, 0.0888, 0.0975, 0.1830, 0.1925, 0.1472]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1762, 0.0875, 0.0903, 0.2461, 0.0969, 0.1791, 0.1238],
        [0.1399, 0.0786, 0.0820, 0.2008, 0.1779, 0.2105, 0.1104],
        [0.2233, 0.0930, 0.1165, 0.1153, 0.1248, 0.1326, 0.1945]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1654, 0.0854, 0.0891, 0.2225, 0.1282, 0.1781, 0.1313],
        [0.2498, 0.1166, 0.1228, 0.0944, 0.1396, 0.1301, 0.1468],
        [0.3445, 0.1149, 0.0967, 0.0897, 0.0959, 0.1014, 0.1568],
        [0.1566, 0.1060, 0.0946, 0.2151, 0.0994, 0.2225, 0.1057]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1680, 0.0980, 0.1017, 0.1137, 0.2176, 0.1872, 0.1137],
        [0.1480, 0.0938, 0.0959, 0.1445, 0.1224, 0.2970, 0.0984],
        [0.1648, 0.1036, 0.1060, 0.2258, 0.1309, 0.1191, 0.1498],
        [0.1821, 0.1188, 0.1058, 0.1768, 0.1140, 0.1441, 0.1585],
        [0.2028, 0.1049, 0.1204, 0.1490, 0.1497, 0.1178, 0.1553]],
       grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1376, 0.0733, 0.1304, 0.0767, 0.0118, 0.0897, 0.4806],
        [0.0895, 0.0658, 0.1166, 0.1254, 0.0114, 0.1294, 0.4619]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1042, 0.0554, 0.0916, 0.0498, 0.0079, 0.0688, 0.6223],
        [0.0820, 0.0606, 0.1350, 0.1375, 0.0086, 0.1324, 0.4438],
        [0.0442, 0.0249, 0.0402, 0.1217, 0.0099, 0.1491, 0.6102]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1548, 0.0605, 0.1491, 0.1070, 0.0069, 0.0612, 0.4605],
        [0.0553, 0.0420, 0.1241, 0.1802, 0.0052, 0.0450, 0.5482],
        [0.0265, 0.0178, 0.0311, 0.0596, 0.0057, 0.0347, 0.8247],
        [0.0421, 0.0270, 0.0298, 0.0824, 0.0089, 0.1015, 0.7083]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0763, 0.0432, 0.0742, 0.0557, 0.0047, 0.0587, 0.6872],
        [0.0387, 0.0315, 0.0798, 0.0583, 0.0035, 0.0595, 0.7287],
        [0.0336, 0.0216, 0.0403, 0.0762, 0.0052, 0.0449, 0.7781],
        [0.0296, 0.0212, 0.0211, 0.0977, 0.0059, 0.0844, 0.7401],
        [0.0279, 0.0210, 0.0186, 0.0859, 0.0061, 0.0659, 0.7746]],
       grad_fn=<SoftmaxBackward>)
#####################
08/02 10:15:02 PM | Train: [ 4/210] Step 000/404 Loss 6.926 Prec@(1,5) (0.0%, 3.1%)
08/02 10:15:48 PM | Train: [ 4/210] Step 010/404 Loss 6.656 Prec@(1,5) (1.7%, 3.7%)
08/02 10:16:35 PM | Train: [ 4/210] Step 020/404 Loss 6.575 Prec@(1,5) (1.0%, 3.1%)
08/02 10:17:20 PM | Train: [ 4/210] Step 030/404 Loss 6.577 Prec@(1,5) (0.8%, 2.7%)
08/02 10:18:04 PM | Train: [ 4/210] Step 040/404 Loss 6.517 Prec@(1,5) (0.7%, 3.0%)
08/02 10:18:50 PM | Train: [ 4/210] Step 050/404 Loss 6.519 Prec@(1,5) (0.8%, 2.9%)
08/02 10:19:36 PM | Train: [ 4/210] Step 060/404 Loss 6.543 Prec@(1,5) (0.7%, 2.6%)
08/02 10:20:22 PM | Train: [ 4/210] Step 070/404 Loss 6.523 Prec@(1,5) (0.6%, 2.5%)
08/02 10:21:07 PM | Train: [ 4/210] Step 080/404 Loss 6.524 Prec@(1,5) (0.5%, 2.5%)
08/02 10:21:53 PM | Train: [ 4/210] Step 090/404 Loss 6.536 Prec@(1,5) (0.5%, 2.5%)
08/02 10:22:41 PM | Train: [ 4/210] Step 100/404 Loss 6.553 Prec@(1,5) (0.5%, 2.4%)
08/02 10:23:28 PM | Train: [ 4/210] Step 110/404 Loss 6.549 Prec@(1,5) (0.4%, 2.5%)
08/02 10:24:13 PM | Train: [ 4/210] Step 120/404 Loss 6.545 Prec@(1,5) (0.4%, 2.6%)
08/02 10:25:00 PM | Train: [ 4/210] Step 130/404 Loss 6.537 Prec@(1,5) (0.5%, 2.8%)
08/02 10:25:52 PM | Train: [ 4/210] Step 140/404 Loss 6.536 Prec@(1,5) (0.4%, 2.7%)
08/02 10:26:39 PM | Train: [ 4/210] Step 150/404 Loss 6.537 Prec@(1,5) (0.5%, 2.7%)
08/02 10:27:24 PM | Train: [ 4/210] Step 160/404 Loss 6.543 Prec@(1,5) (0.5%, 2.6%)
08/02 10:28:11 PM | Train: [ 4/210] Step 170/404 Loss 6.540 Prec@(1,5) (0.6%, 2.9%)
08/02 10:28:56 PM | Train: [ 4/210] Step 180/404 Loss 6.536 Prec@(1,5) (0.6%, 2.8%)
08/02 10:29:40 PM | Train: [ 4/210] Step 190/404 Loss 6.528 Prec@(1,5) (0.7%, 3.0%)
08/02 10:30:30 PM | Train: [ 4/210] Step 200/404 Loss 6.518 Prec@(1,5) (0.7%, 3.1%)
08/02 10:31:16 PM | Train: [ 4/210] Step 210/404 Loss 6.521 Prec@(1,5) (0.7%, 3.0%)
08/02 10:32:02 PM | Train: [ 4/210] Step 220/404 Loss 6.520 Prec@(1,5) (0.7%, 3.0%)
08/02 10:32:47 PM | Train: [ 4/210] Step 230/404 Loss 6.519 Prec@(1,5) (0.7%, 3.0%)
08/02 10:33:32 PM | Train: [ 4/210] Step 240/404 Loss 6.514 Prec@(1,5) (0.7%, 3.1%)
08/02 10:34:19 PM | Train: [ 4/210] Step 250/404 Loss 6.511 Prec@(1,5) (0.7%, 3.2%)
08/02 10:35:03 PM | Train: [ 4/210] Step 260/404 Loss 6.506 Prec@(1,5) (0.8%, 3.2%)
08/02 10:35:49 PM | Train: [ 4/210] Step 270/404 Loss 6.503 Prec@(1,5) (0.8%, 3.3%)
08/02 10:36:35 PM | Train: [ 4/210] Step 280/404 Loss 6.499 Prec@(1,5) (0.7%, 3.3%)
08/02 10:37:19 PM | Train: [ 4/210] Step 290/404 Loss 6.498 Prec@(1,5) (0.7%, 3.3%)
08/02 10:38:03 PM | Train: [ 4/210] Step 300/404 Loss 6.499 Prec@(1,5) (0.7%, 3.3%)
08/02 10:38:49 PM | Train: [ 4/210] Step 310/404 Loss 6.499 Prec@(1,5) (0.7%, 3.3%)
08/02 10:39:38 PM | Train: [ 4/210] Step 320/404 Loss 6.492 Prec@(1,5) (0.7%, 3.2%)
08/02 10:40:23 PM | Train: [ 4/210] Step 330/404 Loss 6.488 Prec@(1,5) (0.7%, 3.3%)
08/02 10:41:07 PM | Train: [ 4/210] Step 340/404 Loss 6.491 Prec@(1,5) (0.7%, 3.3%)
08/02 10:41:53 PM | Train: [ 4/210] Step 350/404 Loss 6.488 Prec@(1,5) (0.7%, 3.3%)
08/02 10:42:36 PM | Train: [ 4/210] Step 360/404 Loss 6.483 Prec@(1,5) (0.7%, 3.3%)
08/02 10:43:21 PM | Train: [ 4/210] Step 370/404 Loss 6.478 Prec@(1,5) (0.7%, 3.4%)
08/02 10:44:06 PM | Train: [ 4/210] Step 380/404 Loss 6.474 Prec@(1,5) (0.7%, 3.5%)
08/02 10:44:52 PM | Train: [ 4/210] Step 390/404 Loss 6.474 Prec@(1,5) (0.7%, 3.5%)
08/02 10:45:40 PM | Train: [ 4/210] Step 400/404 Loss 6.470 Prec@(1,5) (0.7%, 3.6%)
08/02 10:46:03 PM | Train: [ 4/210] Final Prec@1 0.7580%
08/02 10:47:50 PM | Valid: [ 4/210] Step 000/023 Loss 7.147 Prec@(1,5) (6.2%, 15.6%)
08/02 10:47:53 PM | Valid: [ 4/210] Step 010/023 Loss 7.371 Prec@(1,5) (1.4%, 5.7%)
08/02 10:47:56 PM | Valid: [ 4/210] Step 020/023 Loss 7.396 Prec@(1,5) (1.2%, 4.8%)
08/02 10:48:00 PM | Valid: [ 4/210] Final Prec@1 1.0870%, Prec@5 4.4837%, Prec@10 8.9674%
08/02 10:48:00 PM | Final best Prec@1 = 1.3587%
08/02 10:48:00 PM | genotype = Genotype(normal=[[('max_pool_3x3', 1), ('sep_conv_3x3', 0)], [('max_pool_3x3', 2), ('sep_conv_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 1)], [('dil_conv_3x3', 1), ('max_pool_3x3', 4)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 2), ('dil_conv_3x3', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('dil_conv_3x3', 3), ('dil_conv_3x3', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1649, 0.0513, 0.0526, 0.2480, 0.1736, 0.1651, 0.1444],
        [0.2812, 0.0729, 0.0807, 0.0658, 0.1554, 0.2186, 0.1254]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2360, 0.0760, 0.0783, 0.2911, 0.0962, 0.1237, 0.0986],
        [0.1628, 0.0686, 0.0741, 0.2153, 0.2028, 0.1779, 0.0985],
        [0.2548, 0.0745, 0.0997, 0.1051, 0.1206, 0.1052, 0.2400]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1987, 0.0752, 0.0790, 0.2192, 0.1108, 0.2159, 0.1012],
        [0.2876, 0.0992, 0.1074, 0.1219, 0.1448, 0.1168, 0.1223],
        [0.4041, 0.0818, 0.0979, 0.0686, 0.0990, 0.0961, 0.1524],
        [0.1625, 0.0939, 0.0949, 0.2605, 0.0936, 0.1878, 0.1067]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2086, 0.0917, 0.0970, 0.1189, 0.2179, 0.1561, 0.1099],
        [0.1484, 0.0787, 0.0834, 0.1018, 0.0920, 0.4164, 0.0793],
        [0.1714, 0.0837, 0.1080, 0.2178, 0.1432, 0.0911, 0.1848],
        [0.1822, 0.0955, 0.0919, 0.2415, 0.1289, 0.1100, 0.1501],
        [0.2637, 0.0941, 0.0924, 0.1111, 0.1770, 0.0986, 0.1632]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1931, 0.0660, 0.1292, 0.0835, 0.0116, 0.0747, 0.4418],
        [0.1017, 0.0650, 0.1515, 0.1369, 0.0105, 0.0932, 0.4411]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1240, 0.0462, 0.0788, 0.0515, 0.0085, 0.0728, 0.6182],
        [0.0621, 0.0327, 0.1117, 0.1008, 0.0071, 0.1426, 0.5430],
        [0.0369, 0.0169, 0.0299, 0.1821, 0.0097, 0.1406, 0.5839]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2373, 0.0492, 0.1766, 0.1054, 0.0062, 0.0519, 0.3733],
        [0.0677, 0.0328, 0.0955, 0.2142, 0.0046, 0.0365, 0.5488],
        [0.0224, 0.0125, 0.0229, 0.0433, 0.0041, 0.0263, 0.8685],
        [0.0471, 0.0225, 0.0262, 0.0731, 0.0071, 0.1441, 0.6797]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0639, 0.0263, 0.0686, 0.0513, 0.0036, 0.0520, 0.7342],
        [0.0370, 0.0251, 0.0674, 0.0464, 0.0029, 0.0510, 0.7702],
        [0.0231, 0.0136, 0.0236, 0.0515, 0.0037, 0.0442, 0.8402],
        [0.0274, 0.0180, 0.0186, 0.0829, 0.0050, 0.1013, 0.7468],
        [0.0343, 0.0232, 0.0206, 0.0684, 0.0051, 0.0764, 0.7720]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
08/02 10:51:56 PM | Train: [ 5/210] Step 000/404 Loss 6.806 Prec@(1,5) (0.0%, 3.1%)
08/02 10:52:43 PM | Train: [ 5/210] Step 010/404 Loss 6.560 Prec@(1,5) (1.7%, 4.3%)
08/02 10:53:30 PM | Train: [ 5/210] Step 020/404 Loss 6.497 Prec@(1,5) (1.2%, 3.7%)
08/02 10:54:16 PM | Train: [ 5/210] Step 030/404 Loss 6.491 Prec@(1,5) (0.9%, 3.7%)
08/02 10:55:01 PM | Train: [ 5/210] Step 040/404 Loss 6.411 Prec@(1,5) (1.0%, 5.0%)
08/02 10:55:49 PM | Train: [ 5/210] Step 050/404 Loss 6.394 Prec@(1,5) (1.1%, 5.0%)
08/02 10:56:34 PM | Train: [ 5/210] Step 060/404 Loss 6.408 Prec@(1,5) (1.0%, 4.6%)
08/02 10:57:20 PM | Train: [ 5/210] Step 070/404 Loss 6.389 Prec@(1,5) (0.8%, 4.4%)
08/02 10:58:04 PM | Train: [ 5/210] Step 080/404 Loss 6.397 Prec@(1,5) (0.8%, 4.1%)
08/02 10:58:49 PM | Train: [ 5/210] Step 090/404 Loss 6.410 Prec@(1,5) (0.7%, 3.9%)
08/02 10:59:39 PM | Train: [ 5/210] Step 100/404 Loss 6.430 Prec@(1,5) (0.7%, 3.8%)
08/02 11:00:26 PM | Train: [ 5/210] Step 110/404 Loss 6.422 Prec@(1,5) (0.6%, 3.9%)
08/02 11:01:12 PM | Train: [ 5/210] Step 120/404 Loss 6.419 Prec@(1,5) (0.6%, 3.8%)
08/02 11:01:56 PM | Train: [ 5/210] Step 130/404 Loss 6.409 Prec@(1,5) (0.7%, 3.9%)
08/02 11:02:41 PM | Train: [ 5/210] Step 140/404 Loss 6.404 Prec@(1,5) (0.8%, 4.0%)
08/02 11:03:26 PM | Train: [ 5/210] Step 150/404 Loss 6.406 Prec@(1,5) (0.7%, 4.0%)
08/02 11:04:10 PM | Train: [ 5/210] Step 160/404 Loss 6.409 Prec@(1,5) (0.7%, 3.8%)
08/02 11:04:55 PM | Train: [ 5/210] Step 170/404 Loss 6.408 Prec@(1,5) (0.8%, 4.1%)
08/02 11:05:41 PM | Train: [ 5/210] Step 180/404 Loss 6.405 Prec@(1,5) (0.8%, 4.0%)
08/02 11:06:26 PM | Train: [ 5/210] Step 190/404 Loss 6.399 Prec@(1,5) (0.9%, 4.3%)
08/02 11:07:11 PM | Train: [ 5/210] Step 200/404 Loss 6.389 Prec@(1,5) (0.9%, 4.4%)
08/02 11:07:55 PM | Train: [ 5/210] Step 210/404 Loss 6.393 Prec@(1,5) (0.9%, 4.3%)
08/02 11:08:40 PM | Train: [ 5/210] Step 220/404 Loss 6.394 Prec@(1,5) (0.8%, 4.1%)
08/02 11:09:28 PM | Train: [ 5/210] Step 230/404 Loss 6.393 Prec@(1,5) (0.8%, 4.1%)
08/02 11:10:15 PM | Train: [ 5/210] Step 240/404 Loss 6.388 Prec@(1,5) (0.8%, 4.1%)
08/02 11:11:00 PM | Train: [ 5/210] Step 250/404 Loss 6.387 Prec@(1,5) (0.9%, 4.2%)
08/02 11:11:46 PM | Train: [ 5/210] Step 260/404 Loss 6.381 Prec@(1,5) (0.9%, 4.3%)
08/02 11:12:29 PM | Train: [ 5/210] Step 270/404 Loss 6.377 Prec@(1,5) (0.9%, 4.3%)
08/02 11:13:16 PM | Train: [ 5/210] Step 280/404 Loss 6.373 Prec@(1,5) (0.9%, 4.3%)
08/02 11:14:03 PM | Train: [ 5/210] Step 290/404 Loss 6.373 Prec@(1,5) (0.9%, 4.3%)
08/02 11:14:49 PM | Train: [ 5/210] Step 300/404 Loss 6.376 Prec@(1,5) (0.9%, 4.3%)
08/02 11:15:39 PM | Train: [ 5/210] Step 310/404 Loss 6.376 Prec@(1,5) (0.9%, 4.2%)
08/02 11:16:23 PM | Train: [ 5/210] Step 320/404 Loss 6.370 Prec@(1,5) (0.9%, 4.2%)
08/02 11:17:09 PM | Train: [ 5/210] Step 330/404 Loss 6.364 Prec@(1,5) (0.9%, 4.3%)
08/02 11:17:54 PM | Train: [ 5/210] Step 340/404 Loss 6.367 Prec@(1,5) (0.9%, 4.2%)
08/02 11:18:39 PM | Train: [ 5/210] Step 350/404 Loss 6.367 Prec@(1,5) (0.9%, 4.2%)
08/02 11:19:23 PM | Train: [ 5/210] Step 360/404 Loss 6.364 Prec@(1,5) (0.9%, 4.2%)
08/02 11:20:09 PM | Train: [ 5/210] Step 370/404 Loss 6.357 Prec@(1,5) (0.9%, 4.3%)
08/02 11:20:55 PM | Train: [ 5/210] Step 380/404 Loss 6.354 Prec@(1,5) (0.9%, 4.4%)
08/02 11:21:42 PM | Train: [ 5/210] Step 390/404 Loss 6.353 Prec@(1,5) (1.0%, 4.4%)
08/02 11:22:28 PM | Train: [ 5/210] Step 400/404 Loss 6.350 Prec@(1,5) (1.0%, 4.5%)
08/02 11:22:50 PM | Train: [ 5/210] Final Prec@1 1.0133%
08/02 11:24:39 PM | Valid: [ 5/210] Step 000/023 Loss 8.520 Prec@(1,5) (6.2%, 9.4%)
08/02 11:24:42 PM | Valid: [ 5/210] Step 010/023 Loss 8.270 Prec@(1,5) (1.4%, 5.7%)
08/02 11:24:45 PM | Valid: [ 5/210] Step 020/023 Loss 8.334 Prec@(1,5) (1.5%, 5.2%)
08/02 11:24:50 PM | Valid: [ 5/210] Final Prec@1 1.3587%, Prec@5 4.8913%, Prec@10 7.8804%
08/02 11:24:50 PM | Final best Prec@1 = 1.3587%
08/02 11:24:50 PM | genotype = Genotype(normal=[[('max_pool_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 2), ('max_pool_3x3', 1)], [('dil_conv_3x3', 1), ('max_pool_3x3', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('dil_conv_3x3', 3), ('dil_conv_3x3', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.2392, 0.0565, 0.0572, 0.2493, 0.1124, 0.1454, 0.1400],
        [0.3383, 0.0702, 0.0776, 0.0524, 0.1051, 0.2548, 0.1016]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2231, 0.0657, 0.0668, 0.3568, 0.0755, 0.1230, 0.0891],
        [0.1938, 0.0716, 0.0795, 0.1701, 0.1454, 0.2277, 0.1119],
        [0.2713, 0.0718, 0.0776, 0.0779, 0.1654, 0.1219, 0.2140]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2609, 0.0817, 0.0845, 0.1846, 0.0749, 0.2419, 0.0715],
        [0.3199, 0.0891, 0.0980, 0.1068, 0.1348, 0.1180, 0.1334],
        [0.4202, 0.0741, 0.0842, 0.0806, 0.0951, 0.0950, 0.1507],
        [0.1770, 0.1007, 0.1000, 0.2179, 0.1519, 0.1390, 0.1135]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2835, 0.0984, 0.1041, 0.1010, 0.2198, 0.0997, 0.0935],
        [0.2073, 0.0854, 0.0939, 0.0816, 0.0810, 0.3692, 0.0817],
        [0.1888, 0.0802, 0.0986, 0.1847, 0.1722, 0.1105, 0.1651],
        [0.1803, 0.0869, 0.0880, 0.2309, 0.1614, 0.1326, 0.1199],
        [0.2842, 0.0810, 0.0875, 0.0699, 0.2645, 0.0952, 0.1178]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1743, 0.0507, 0.1586, 0.0865, 0.0117, 0.0675, 0.4507],
        [0.1432, 0.0805, 0.0893, 0.1560, 0.0105, 0.1010, 0.4195]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1147, 0.0276, 0.0719, 0.0502, 0.0089, 0.0583, 0.6685],
        [0.0859, 0.0296, 0.1327, 0.1478, 0.0087, 0.1387, 0.4567],
        [0.0359, 0.0125, 0.0230, 0.1792, 0.0102, 0.1522, 0.5870]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2002, 0.0298, 0.3153, 0.1091, 0.0058, 0.0431, 0.2967],
        [0.0886, 0.0270, 0.1004, 0.1870, 0.0047, 0.0330, 0.5594],
        [0.0164, 0.0078, 0.0172, 0.0310, 0.0036, 0.0448, 0.8791],
        [0.0638, 0.0219, 0.0311, 0.0741, 0.0069, 0.1320, 0.6702]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0508, 0.0159, 0.0541, 0.0413, 0.0029, 0.0412, 0.7939],
        [0.0349, 0.0194, 0.0583, 0.0453, 0.0027, 0.0434, 0.7960],
        [0.0151, 0.0081, 0.0139, 0.0433, 0.0030, 0.0290, 0.8876],
        [0.0199, 0.0122, 0.0136, 0.0561, 0.0042, 0.2281, 0.6659],
        [0.0435, 0.0235, 0.0236, 0.0709, 0.0053, 0.1264, 0.7067]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
08/02 11:28:48 PM | Train: [ 6/210] Step 000/404 Loss 6.660 Prec@(1,5) (0.0%, 6.2%)
08/02 11:29:35 PM | Train: [ 6/210] Step 010/404 Loss 6.460 Prec@(1,5) (1.4%, 4.5%)
08/02 11:30:22 PM | Train: [ 6/210] Step 020/404 Loss 6.405 Prec@(1,5) (1.0%, 4.0%)
08/02 11:31:09 PM | Train: [ 6/210] Step 030/404 Loss 6.397 Prec@(1,5) (1.0%, 3.9%)
08/02 11:31:55 PM | Train: [ 6/210] Step 040/404 Loss 6.320 Prec@(1,5) (0.9%, 4.9%)
08/02 11:32:38 PM | Train: [ 6/210] Step 050/404 Loss 6.267 Prec@(1,5) (1.3%, 5.8%)
08/02 11:33:21 PM | Train: [ 6/210] Step 060/404 Loss 6.264 Prec@(1,5) (1.3%, 5.4%)
08/02 11:34:06 PM | Train: [ 6/210] Step 070/404 Loss 6.246 Prec@(1,5) (1.2%, 5.5%)
08/02 11:34:52 PM | Train: [ 6/210] Step 080/404 Loss 6.259 Prec@(1,5) (1.1%, 5.2%)
08/02 11:35:39 PM | Train: [ 6/210] Step 090/404 Loss 6.275 Prec@(1,5) (1.1%, 4.8%)
08/02 11:36:26 PM | Train: [ 6/210] Step 100/404 Loss 6.299 Prec@(1,5) (1.1%, 4.6%)
08/02 11:37:09 PM | Train: [ 6/210] Step 110/404 Loss 6.292 Prec@(1,5) (1.2%, 4.7%)
08/02 11:37:54 PM | Train: [ 6/210] Step 120/404 Loss 6.297 Prec@(1,5) (1.1%, 4.6%)
08/02 11:38:38 PM | Train: [ 6/210] Step 130/404 Loss 6.293 Prec@(1,5) (1.1%, 4.6%)
08/02 11:39:24 PM | Train: [ 6/210] Step 140/404 Loss 6.286 Prec@(1,5) (1.2%, 4.7%)
08/02 11:40:11 PM | Train: [ 6/210] Step 150/404 Loss 6.294 Prec@(1,5) (1.1%, 4.6%)
08/02 11:40:58 PM | Train: [ 6/210] Step 160/404 Loss 6.301 Prec@(1,5) (1.1%, 4.5%)
08/02 11:41:44 PM | Train: [ 6/210] Step 170/404 Loss 6.303 Prec@(1,5) (1.2%, 4.6%)
08/02 11:42:27 PM | Train: [ 6/210] Step 180/404 Loss 6.300 Prec@(1,5) (1.2%, 4.7%)
08/02 11:43:13 PM | Train: [ 6/210] Step 190/404 Loss 6.292 Prec@(1,5) (1.3%, 4.9%)
08/02 11:43:57 PM | Train: [ 6/210] Step 200/404 Loss 6.281 Prec@(1,5) (1.4%, 5.1%)
08/02 11:44:42 PM | Train: [ 6/210] Step 210/404 Loss 6.283 Prec@(1,5) (1.3%, 4.9%)
08/02 11:45:29 PM | Train: [ 6/210] Step 220/404 Loss 6.283 Prec@(1,5) (1.3%, 4.9%)
08/02 11:46:13 PM | Train: [ 6/210] Step 230/404 Loss 6.281 Prec@(1,5) (1.3%, 4.8%)
08/02 11:46:58 PM | Train: [ 6/210] Step 240/404 Loss 6.276 Prec@(1,5) (1.3%, 4.8%)
08/02 11:47:43 PM | Train: [ 6/210] Step 250/404 Loss 6.278 Prec@(1,5) (1.3%, 4.9%)
08/02 11:48:32 PM | Train: [ 6/210] Step 260/404 Loss 6.274 Prec@(1,5) (1.4%, 4.9%)
08/02 11:49:16 PM | Train: [ 6/210] Step 270/404 Loss 6.268 Prec@(1,5) (1.4%, 5.0%)
08/02 11:50:02 PM | Train: [ 6/210] Step 280/404 Loss 6.262 Prec@(1,5) (1.4%, 5.0%)
08/02 11:50:53 PM | Train: [ 6/210] Step 290/404 Loss 6.264 Prec@(1,5) (1.3%, 5.0%)
08/02 11:51:39 PM | Train: [ 6/210] Step 300/404 Loss 6.269 Prec@(1,5) (1.3%, 5.0%)
08/02 11:52:24 PM | Train: [ 6/210] Step 310/404 Loss 6.271 Prec@(1,5) (1.3%, 5.0%)
08/02 11:53:10 PM | Train: [ 6/210] Step 320/404 Loss 6.267 Prec@(1,5) (1.3%, 4.9%)
08/02 11:53:55 PM | Train: [ 6/210] Step 330/404 Loss 6.261 Prec@(1,5) (1.3%, 5.1%)
08/02 11:54:40 PM | Train: [ 6/210] Step 340/404 Loss 6.265 Prec@(1,5) (1.3%, 5.0%)
08/02 11:55:26 PM | Train: [ 6/210] Step 350/404 Loss 6.265 Prec@(1,5) (1.3%, 5.1%)
08/02 11:56:12 PM | Train: [ 6/210] Step 360/404 Loss 6.262 Prec@(1,5) (1.3%, 5.1%)
08/02 11:56:56 PM | Train: [ 6/210] Step 370/404 Loss 6.255 Prec@(1,5) (1.3%, 5.2%)
08/02 11:57:39 PM | Train: [ 6/210] Step 380/404 Loss 6.252 Prec@(1,5) (1.3%, 5.2%)
08/02 11:58:26 PM | Train: [ 6/210] Step 390/404 Loss 6.250 Prec@(1,5) (1.4%, 5.3%)
08/02 11:59:14 PM | Train: [ 6/210] Step 400/404 Loss 6.246 Prec@(1,5) (1.4%, 5.5%)
08/02 11:59:33 PM | Train: [ 6/210] Final Prec@1 1.4155%
08/03 12:01:20 AM | Valid: [ 6/210] Step 000/023 Loss 9.031 Prec@(1,5) (3.1%, 3.1%)
08/03 12:01:23 AM | Valid: [ 6/210] Step 010/023 Loss 7.985 Prec@(1,5) (1.7%, 4.5%)
08/03 12:01:26 AM | Valid: [ 6/210] Step 020/023 Loss 8.055 Prec@(1,5) (1.3%, 4.8%)
08/03 12:01:29 AM | Valid: [ 6/210] Final Prec@1 1.2228%, Prec@5 4.4837%, Prec@10 7.2011%
08/03 12:01:29 AM | Final best Prec@1 = 1.3587%
08/03 12:01:29 AM | genotype = Genotype(normal=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 2), ('max_pool_3x3', 1)], [('dil_conv_3x3', 1), ('max_pool_3x3', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 1), ('skip_connect', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('dil_conv_3x3', 2)], [('dil_conv_3x3', 3), ('dil_conv_3x3', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.2810, 0.0538, 0.0531, 0.2496, 0.0859, 0.1599, 0.1168],
        [0.3529, 0.0659, 0.0725, 0.0539, 0.1002, 0.2624, 0.0924]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2050, 0.0586, 0.0589, 0.3985, 0.0736, 0.1279, 0.0774],
        [0.2180, 0.0793, 0.0872, 0.1556, 0.1254, 0.2251, 0.1093],
        [0.2520, 0.0664, 0.0792, 0.0737, 0.2063, 0.1166, 0.2058]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2538, 0.0796, 0.0824, 0.1864, 0.0571, 0.2769, 0.0638],
        [0.3443, 0.0968, 0.1068, 0.0901, 0.1255, 0.1196, 0.1169],
        [0.4476, 0.0578, 0.0816, 0.0698, 0.1006, 0.0885, 0.1541],
        [0.1832, 0.0990, 0.0982, 0.1977, 0.1612, 0.1281, 0.1325]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3042, 0.0975, 0.1029, 0.1021, 0.1945, 0.1032, 0.0954],
        [0.2222, 0.0921, 0.1005, 0.0914, 0.0755, 0.3509, 0.0673],
        [0.2011, 0.0708, 0.0978, 0.1404, 0.1661, 0.1101, 0.2137],
        [0.1971, 0.0892, 0.0914, 0.2155, 0.1509, 0.1540, 0.1019],
        [0.3032, 0.0833, 0.1055, 0.0681, 0.2287, 0.0997, 0.1115]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1368, 0.0348, 0.1807, 0.0951, 0.0121, 0.0799, 0.4606],
        [0.1059, 0.0527, 0.0807, 0.2449, 0.0111, 0.1133, 0.3914]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1060, 0.0199, 0.1005, 0.0711, 0.0097, 0.0653, 0.6275],
        [0.0744, 0.0226, 0.1330, 0.1809, 0.0091, 0.0534, 0.5266],
        [0.0440, 0.0119, 0.0222, 0.1858, 0.0101, 0.1131, 0.6128]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1843, 0.0247, 0.2635, 0.1493, 0.0062, 0.0426, 0.3292],
        [0.0769, 0.0244, 0.0531, 0.1099, 0.0038, 0.0255, 0.7065],
        [0.0202, 0.0081, 0.0270, 0.0425, 0.0044, 0.0678, 0.8301],
        [0.0615, 0.0217, 0.0387, 0.0976, 0.0066, 0.0723, 0.7017]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0506, 0.0131, 0.0446, 0.0422, 0.0029, 0.0489, 0.7976],
        [0.0310, 0.0166, 0.0495, 0.0350, 0.0027, 0.0426, 0.8226],
        [0.0102, 0.0054, 0.0115, 0.0420, 0.0024, 0.0299, 0.8985],
        [0.0186, 0.0119, 0.0132, 0.0996, 0.0054, 0.2686, 0.5827],
        [0.0462, 0.0182, 0.0237, 0.1271, 0.0056, 0.2651, 0.5142]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
08/03 12:05:19 AM | Train: [ 7/210] Step 000/404 Loss 6.512 Prec@(1,5) (0.0%, 3.1%)
08/03 12:06:03 AM | Train: [ 7/210] Step 010/404 Loss 6.356 Prec@(1,5) (1.4%, 5.4%)
08/03 12:06:49 AM | Train: [ 7/210] Step 020/404 Loss 6.322 Prec@(1,5) (0.9%, 4.9%)
08/03 12:07:34 AM | Train: [ 7/210] Step 030/404 Loss 6.322 Prec@(1,5) (0.8%, 5.1%)
08/03 12:08:20 AM | Train: [ 7/210] Step 040/404 Loss 6.253 Prec@(1,5) (1.1%, 5.6%)
08/03 12:09:05 AM | Train: [ 7/210] Step 050/404 Loss 6.193 Prec@(1,5) (1.4%, 6.5%)
08/03 12:09:54 AM | Train: [ 7/210] Step 060/404 Loss 6.140 Prec@(1,5) (1.5%, 7.4%)
08/03 12:10:40 AM | Train: [ 7/210] Step 070/404 Loss 6.120 Prec@(1,5) (1.4%, 7.0%)
08/03 12:11:26 AM | Train: [ 7/210] Step 080/404 Loss 6.141 Prec@(1,5) (1.3%, 6.6%)
08/03 12:12:13 AM | Train: [ 7/210] Step 090/404 Loss 6.165 Prec@(1,5) (1.2%, 6.3%)
08/03 12:13:00 AM | Train: [ 7/210] Step 100/404 Loss 6.191 Prec@(1,5) (1.1%, 5.9%)
08/03 12:13:46 AM | Train: [ 7/210] Step 110/404 Loss 6.173 Prec@(1,5) (1.2%, 6.4%)
08/03 12:14:29 AM | Train: [ 7/210] Step 120/404 Loss 6.183 Prec@(1,5) (1.2%, 6.1%)
08/03 12:15:17 AM | Train: [ 7/210] Step 130/404 Loss 6.187 Prec@(1,5) (1.1%, 6.0%)
08/03 12:16:01 AM | Train: [ 7/210] Step 140/404 Loss 6.180 Prec@(1,5) (1.2%, 6.3%)
08/03 12:16:47 AM | Train: [ 7/210] Step 150/404 Loss 6.191 Prec@(1,5) (1.2%, 6.2%)
08/03 12:17:34 AM | Train: [ 7/210] Step 160/404 Loss 6.202 Prec@(1,5) (1.1%, 5.9%)
08/03 12:18:20 AM | Train: [ 7/210] Step 170/404 Loss 6.209 Prec@(1,5) (1.2%, 6.0%)
08/03 12:19:06 AM | Train: [ 7/210] Step 180/404 Loss 6.208 Prec@(1,5) (1.2%, 6.0%)
08/03 12:19:50 AM | Train: [ 7/210] Step 190/404 Loss 6.199 Prec@(1,5) (1.4%, 6.2%)
08/03 12:20:36 AM | Train: [ 7/210] Step 200/404 Loss 6.191 Prec@(1,5) (1.4%, 6.4%)
08/03 12:21:21 AM | Train: [ 7/210] Step 210/404 Loss 6.196 Prec@(1,5) (1.4%, 6.1%)
08/03 12:22:06 AM | Train: [ 7/210] Step 220/404 Loss 6.194 Prec@(1,5) (1.4%, 6.0%)
08/03 12:22:50 AM | Train: [ 7/210] Step 230/404 Loss 6.187 Prec@(1,5) (1.3%, 6.0%)
08/03 12:23:36 AM | Train: [ 7/210] Step 240/404 Loss 6.179 Prec@(1,5) (1.3%, 6.1%)
08/03 12:24:21 AM | Train: [ 7/210] Step 250/404 Loss 6.180 Prec@(1,5) (1.3%, 6.1%)
08/03 12:25:07 AM | Train: [ 7/210] Step 260/404 Loss 6.178 Prec@(1,5) (1.4%, 6.2%)
08/03 12:25:53 AM | Train: [ 7/210] Step 270/404 Loss 6.173 Prec@(1,5) (1.4%, 6.2%)
08/03 12:26:38 AM | Train: [ 7/210] Step 280/404 Loss 6.168 Prec@(1,5) (1.4%, 6.2%)
08/03 12:27:26 AM | Train: [ 7/210] Step 290/404 Loss 6.168 Prec@(1,5) (1.4%, 6.2%)
08/03 12:28:13 AM | Train: [ 7/210] Step 300/404 Loss 6.172 Prec@(1,5) (1.4%, 6.1%)
08/03 12:29:00 AM | Train: [ 7/210] Step 310/404 Loss 6.173 Prec@(1,5) (1.4%, 6.1%)
08/03 12:29:44 AM | Train: [ 7/210] Step 320/404 Loss 6.170 Prec@(1,5) (1.4%, 6.1%)
08/03 12:30:32 AM | Train: [ 7/210] Step 330/404 Loss 6.166 Prec@(1,5) (1.4%, 6.2%)
08/03 12:31:16 AM | Train: [ 7/210] Step 340/404 Loss 6.172 Prec@(1,5) (1.3%, 6.1%)
08/03 12:32:03 AM | Train: [ 7/210] Step 350/404 Loss 6.175 Prec@(1,5) (1.3%, 6.1%)
08/03 12:32:48 AM | Train: [ 7/210] Step 360/404 Loss 6.174 Prec@(1,5) (1.3%, 6.1%)
08/03 12:33:32 AM | Train: [ 7/210] Step 370/404 Loss 6.167 Prec@(1,5) (1.4%, 6.2%)
08/03 12:34:17 AM | Train: [ 7/210] Step 380/404 Loss 6.165 Prec@(1,5) (1.4%, 6.3%)
08/03 12:35:02 AM | Train: [ 7/210] Step 390/404 Loss 6.163 Prec@(1,5) (1.4%, 6.4%)
08/03 12:35:50 AM | Train: [ 7/210] Step 400/404 Loss 6.154 Prec@(1,5) (1.5%, 6.6%)
08/03 12:36:10 AM | Train: [ 7/210] Final Prec@1 1.5393%
08/03 12:37:52 AM | Valid: [ 7/210] Step 000/023 Loss 7.866 Prec@(1,5) (3.1%, 15.6%)
08/03 12:37:55 AM | Valid: [ 7/210] Step 010/023 Loss 7.467 Prec@(1,5) (2.3%, 8.5%)
08/03 12:37:58 AM | Valid: [ 7/210] Step 020/023 Loss 7.497 Prec@(1,5) (1.8%, 6.8%)
08/03 12:38:02 AM | Valid: [ 7/210] Final Prec@1 1.6304%, Prec@5 6.5217%, Prec@10 9.3750%
08/03 12:38:02 AM | Final best Prec@1 = 1.6304%
08/03 12:38:02 AM | genotype = Genotype(normal=[[('max_pool_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 2), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 1), ('skip_connect', 0)], [('sep_conv_3x3', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 3), ('dil_conv_3x3', 2)], [('dil_conv_3x3', 4), ('sep_conv_3x3', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.2822, 0.0444, 0.0453, 0.2977, 0.0719, 0.1718, 0.0866],
        [0.3523, 0.0553, 0.0614, 0.0398, 0.0791, 0.3038, 0.1083]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.2003, 0.0551, 0.0569, 0.4163, 0.0710, 0.1270, 0.0734],
        [0.2134, 0.0722, 0.0817, 0.1380, 0.1308, 0.2310, 0.1331],
        [0.2640, 0.0626, 0.0775, 0.0607, 0.2080, 0.1269, 0.2003]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.2654, 0.0758, 0.0819, 0.1614, 0.0548, 0.2997, 0.0608],
        [0.3483, 0.0851, 0.0950, 0.0835, 0.1304, 0.1229, 0.1348],
        [0.5292, 0.0494, 0.0666, 0.0562, 0.0863, 0.0804, 0.1319],
        [0.1661, 0.0808, 0.0856, 0.2852, 0.1217, 0.1502, 0.1103]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.3582, 0.0961, 0.1064, 0.0768, 0.1996, 0.0772, 0.0858],
        [0.2351, 0.0869, 0.0963, 0.0784, 0.0826, 0.3487, 0.0720],
        [0.2263, 0.0654, 0.0889, 0.2050, 0.1311, 0.1083, 0.1750],
        [0.2212, 0.0805, 0.0865, 0.2493, 0.1424, 0.1342, 0.0859],
        [0.2838, 0.0699, 0.0878, 0.0632, 0.2618, 0.1482, 0.0853]],
       grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1044, 0.0228, 0.1986, 0.0897, 0.0121, 0.0888, 0.4835],
        [0.1306, 0.0562, 0.1119, 0.2049, 0.0111, 0.1162, 0.3691]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1348, 0.0175, 0.1537, 0.0894, 0.0105, 0.0707, 0.5233],
        [0.0910, 0.0223, 0.2120, 0.2236, 0.0100, 0.0486, 0.3924],
        [0.0447, 0.0122, 0.0224, 0.1346, 0.0094, 0.0959, 0.6808]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1439, 0.0180, 0.1575, 0.1561, 0.0066, 0.0392, 0.4786],
        [0.0816, 0.0219, 0.0415, 0.0763, 0.0034, 0.0205, 0.7547],
        [0.0396, 0.0105, 0.0406, 0.0512, 0.0054, 0.0829, 0.7697],
        [0.0589, 0.0249, 0.0537, 0.1508, 0.0062, 0.0550, 0.6504]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0520, 0.0114, 0.0484, 0.0479, 0.0031, 0.0588, 0.7784],
        [0.0292, 0.0158, 0.0869, 0.0433, 0.0035, 0.1081, 0.7132],
        [0.0107, 0.0062, 0.0144, 0.0638, 0.0030, 0.0318, 0.8701],
        [0.0183, 0.0107, 0.0115, 0.1544, 0.0058, 0.1359, 0.6633],
        [0.0481, 0.0138, 0.0260, 0.0886, 0.0054, 0.5079, 0.3101]],
       grad_fn=<SoftmaxBackward>)
#####################
08/03 12:42:01 AM | Train: [ 8/210] Step 000/404 Loss 6.413 Prec@(1,5) (3.1%, 3.1%)
08/03 12:42:46 AM | Train: [ 8/210] Step 010/404 Loss 6.298 Prec@(1,5) (2.0%, 8.0%)
08/03 12:43:30 AM | Train: [ 8/210] Step 020/404 Loss 6.251 Prec@(1,5) (1.3%, 6.5%)
08/03 12:44:14 AM | Train: [ 8/210] Step 030/404 Loss 6.232 Prec@(1,5) (1.2%, 5.9%)
08/03 12:44:59 AM | Train: [ 8/210] Step 040/404 Loss 6.177 Prec@(1,5) (1.5%, 6.4%)
08/03 12:45:44 AM | Train: [ 8/210] Step 050/404 Loss 6.119 Prec@(1,5) (1.9%, 7.5%)
08/03 12:46:30 AM | Train: [ 8/210] Step 060/404 Loss 6.070 Prec@(1,5) (2.1%, 8.5%)
08/03 12:47:14 AM | Train: [ 8/210] Step 070/404 Loss 6.035 Prec@(1,5) (2.2%, 9.0%)
08/03 12:48:00 AM | Train: [ 8/210] Step 080/404 Loss 6.050 Prec@(1,5) (2.0%, 8.4%)
08/03 12:48:45 AM | Train: [ 8/210] Step 090/404 Loss 6.070 Prec@(1,5) (2.0%, 8.0%)
08/03 12:49:29 AM | Train: [ 8/210] Step 100/404 Loss 6.097 Prec@(1,5) (1.8%, 7.5%)
08/03 12:50:14 AM | Train: [ 8/210] Step 110/404 Loss 6.080 Prec@(1,5) (2.0%, 8.0%)
08/03 12:50:59 AM | Train: [ 8/210] Step 120/404 Loss 6.088 Prec@(1,5) (1.9%, 7.7%)
08/03 12:51:44 AM | Train: [ 8/210] Step 130/404 Loss 6.094 Prec@(1,5) (1.8%, 7.6%)
08/03 12:52:28 AM | Train: [ 8/210] Step 140/404 Loss 6.094 Prec@(1,5) (1.8%, 7.6%)
08/03 12:53:13 AM | Train: [ 8/210] Step 150/404 Loss 6.103 Prec@(1,5) (1.7%, 7.5%)
08/03 12:53:58 AM | Train: [ 8/210] Step 160/404 Loss 6.115 Prec@(1,5) (1.6%, 7.2%)
08/03 12:54:42 AM | Train: [ 8/210] Step 170/404 Loss 6.121 Prec@(1,5) (1.7%, 7.3%)
08/03 12:55:28 AM | Train: [ 8/210] Step 180/404 Loss 6.120 Prec@(1,5) (1.7%, 7.1%)
