07/31 11:23:19 PM | Logger is set 
07/31 11:23:19 PM | Logger without distribution
07/31 11:24:06 PM | Initializing dataset used 46.74059319496155 basic time unit
07/31 11:24:06 PM | The training classes labels length :  751
07/31 11:25:26 PM | batch loading time example is 79.96259999275208
####### ALPHA #######
# Alpha - normal
tensor([[0.1251, 0.1250, 0.1251, 0.1249, 0.1249, 0.1251, 0.1251, 0.1248],
        [0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1251, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1248, 0.1251, 0.1250, 0.1249, 0.1253, 0.1250, 0.1248, 0.1251],
        [0.1251, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1250, 0.1252],
        [0.1248, 0.1250, 0.1251, 0.1252, 0.1248, 0.1251, 0.1248, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1248, 0.1253, 0.1250, 0.1251, 0.1249, 0.1251, 0.1249],
        [0.1251, 0.1249, 0.1250, 0.1251, 0.1250, 0.1247, 0.1252, 0.1250],
        [0.1249, 0.1251, 0.1249, 0.1251, 0.1251, 0.1249, 0.1250, 0.1248],
        [0.1251, 0.1249, 0.1249, 0.1251, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1252, 0.1251, 0.1248, 0.1251, 0.1251, 0.1250, 0.1250, 0.1247],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1249, 0.1252, 0.1250, 0.1251],
        [0.1251, 0.1248, 0.1251, 0.1249, 0.1251, 0.1250, 0.1250, 0.1250],
        [0.1248, 0.1251, 0.1253, 0.1247, 0.1250, 0.1251, 0.1249, 0.1250],
        [0.1252, 0.1250, 0.1249, 0.1250, 0.1249, 0.1251, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1247, 0.1250, 0.1251, 0.1252, 0.1252, 0.1250, 0.1248, 0.1250],
        [0.1250, 0.1250, 0.1252, 0.1249, 0.1249, 0.1250, 0.1252, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1251, 0.1249, 0.1250, 0.1248, 0.1249, 0.1251, 0.1252],
        [0.1250, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1246, 0.1252],
        [0.1250, 0.1249, 0.1251, 0.1251, 0.1249, 0.1249, 0.1253, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1252],
        [0.1249, 0.1249, 0.1249, 0.1250, 0.1250, 0.1251, 0.1251, 0.1249],
        [0.1249, 0.1252, 0.1250, 0.1250, 0.1251, 0.1249, 0.1250, 0.1249],
        [0.1251, 0.1249, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1249],
        [0.1251, 0.1249, 0.1249, 0.1250, 0.1248, 0.1251, 0.1251, 0.1251],
        [0.1249, 0.1249, 0.1252, 0.1250, 0.1251, 0.1248, 0.1252, 0.1250],
        [0.1248, 0.1249, 0.1251, 0.1251, 0.1249, 0.1251, 0.1251, 0.1250],
        [0.1251, 0.1250, 0.1252, 0.1252, 0.1249, 0.1248, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/31 11:35:03 PM | Logger is set 
07/31 11:35:03 PM | Logger is set 
07/31 11:35:03 PM | Logger without distribution
07/31 11:35:03 PM | Logger without distribution
07/31 11:35:27 PM | Initializing dataset used 23.17797541618347 basic time unit
07/31 11:35:27 PM | Initializing dataset used 23.178180932998657 basic time unit
07/31 11:35:27 PM | The training classes labels length :  751
07/31 11:35:27 PM | The training classes labels length :  751
07/31 11:36:39 PM | batch loading time example is 72.58172059059143
07/31 11:36:39 PM | batch loading time example is 72.58304738998413
####### ALPHA #######
# Alpha - normal
####### ALPHA #######
# Alpha - normal
tensor([[0.1251, 0.1250, 0.1251, 0.1249, 0.1249, 0.1251, 0.1251, 0.1248],
        [0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1251, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1250, 0.1251, 0.1249, 0.1249, 0.1251, 0.1251, 0.1248],
        [0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1251, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1248, 0.1251, 0.1250, 0.1249, 0.1253, 0.1250, 0.1248, 0.1251],
        [0.1251, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1250, 0.1252],
        [0.1248, 0.1250, 0.1251, 0.1252, 0.1248, 0.1251, 0.1248, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1248, 0.1251, 0.1250, 0.1249, 0.1253, 0.1250, 0.1248, 0.1251],
        [0.1251, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1250, 0.1252],
        [0.1248, 0.1250, 0.1251, 0.1252, 0.1248, 0.1251, 0.1248, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1248, 0.1253, 0.1250, 0.1251, 0.1249, 0.1251, 0.1249],
        [0.1251, 0.1249, 0.1250, 0.1251, 0.1250, 0.1247, 0.1252, 0.1250],
        [0.1249, 0.1251, 0.1249, 0.1251, 0.1251, 0.1249, 0.1250, 0.1248],
        [0.1251, 0.1249, 0.1249, 0.1251, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1248, 0.1253, 0.1250, 0.1251, 0.1249, 0.1251, 0.1249],
        [0.1251, 0.1249, 0.1250, 0.1251, 0.1250, 0.1247, 0.1252, 0.1250],
        [0.1249, 0.1251, 0.1249, 0.1251, 0.1251, 0.1249, 0.1250, 0.1248],
        [0.1251, 0.1249, 0.1249, 0.1251, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1252, 0.1251, 0.1248, 0.1251, 0.1251, 0.1250, 0.1250, 0.1247],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1249, 0.1252, 0.1250, 0.1251],
        [0.1251, 0.1248, 0.1251, 0.1249, 0.1251, 0.1250, 0.1250, 0.1250],
        [0.1248, 0.1251, 0.1253, 0.1247, 0.1250, 0.1251, 0.1249, 0.1250],
        [0.1252, 0.1250, 0.1249, 0.1250, 0.1249, 0.1251, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1252, 0.1251, 0.1248, 0.1251, 0.1251, 0.1250, 0.1250, 0.1247],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1249, 0.1252, 0.1250, 0.1251],
        [0.1251, 0.1248, 0.1251, 0.1249, 0.1251, 0.1250, 0.1250, 0.1250],
        [0.1248, 0.1251, 0.1253, 0.1247, 0.1250, 0.1251, 0.1249, 0.1250],
        [0.1252, 0.1250, 0.1249, 0.1250, 0.1249, 0.1251, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce

# Alpha - reduce
tensor([[0.1247, 0.1250, 0.1251, 0.1252, 0.1252, 0.1250, 0.1248, 0.1250],
        [0.1250, 0.1250, 0.1252, 0.1249, 0.1249, 0.1250, 0.1252, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1247, 0.1250, 0.1251, 0.1252, 0.1252, 0.1250, 0.1248, 0.1250],
        [0.1250, 0.1250, 0.1252, 0.1249, 0.1249, 0.1250, 0.1252, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1251, 0.1249, 0.1250, 0.1248, 0.1249, 0.1251, 0.1252],
        [0.1250, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1246, 0.1252],
        [0.1250, 0.1249, 0.1251, 0.1251, 0.1249, 0.1249, 0.1253, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1251, 0.1249, 0.1250, 0.1248, 0.1249, 0.1251, 0.1252],
        [0.1250, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1246, 0.1252],
        [0.1250, 0.1249, 0.1251, 0.1251, 0.1249, 0.1249, 0.1253, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1252],
        [0.1249, 0.1249, 0.1249, 0.1250, 0.1250, 0.1251, 0.1251, 0.1249],
        [0.1249, 0.1252, 0.1250, 0.1250, 0.1251, 0.1249, 0.1250, 0.1249],
        [0.1251, 0.1249, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1252],
        [0.1249, 0.1249, 0.1249, 0.1250, 0.1250, 0.1251, 0.1251, 0.1249],
        [0.1249, 0.1252, 0.1250, 0.1250, 0.1251, 0.1249, 0.1250, 0.1249],
        [0.1251, 0.1249, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1249],
        [0.1251, 0.1249, 0.1249, 0.1250, 0.1248, 0.1251, 0.1251, 0.1251],
        [0.1249, 0.1249, 0.1252, 0.1250, 0.1251, 0.1248, 0.1252, 0.1250],
        [0.1248, 0.1249, 0.1251, 0.1251, 0.1249, 0.1251, 0.1251, 0.1250],
        [0.1251, 0.1250, 0.1252, 0.1252, 0.1249, 0.1248, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1249],
        [0.1251, 0.1249, 0.1249, 0.1250, 0.1248, 0.1251, 0.1251, 0.1251],
        [0.1249, 0.1249, 0.1252, 0.1250, 0.1251, 0.1248, 0.1252, 0.1250],
        [0.1248, 0.1249, 0.1251, 0.1251, 0.1249, 0.1251, 0.1251, 0.1250],
        [0.1251, 0.1250, 0.1252, 0.1252, 0.1249, 0.1248, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
#####################
07/31 11:41:24 PM | Logger is set 
07/31 11:41:24 PM | Logger without distribution
07/31 11:41:24 PM | Logger is set 
07/31 11:41:24 PM | Logger without distribution
07/31 11:41:50 PM | Initializing dataset used 25.871607065200806 basic time unit
07/31 11:41:50 PM | Initializing dataset used 25.88790988922119 basic time unit
07/31 11:41:50 PM | The training classes labels length :  751
07/31 11:41:50 PM | The training classes labels length :  751
07/31 11:42:46 PM | batch loading time example is 56.279003858566284
07/31 11:42:46 PM | batch loading time example is 56.30682849884033
####### ALPHA #######
# Alpha - normal
####### ALPHA #######
# Alpha - normal
tensor([[0.1251, 0.1250, 0.1251, 0.1249, 0.1249, 0.1251, 0.1251, 0.1248],
        [0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1251, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1250, 0.1251, 0.1249, 0.1249, 0.1251, 0.1251, 0.1248],
        [0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1251, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1248, 0.1251, 0.1250, 0.1249, 0.1253, 0.1250, 0.1248, 0.1251],
        [0.1251, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1250, 0.1252],
        [0.1248, 0.1250, 0.1251, 0.1252, 0.1248, 0.1251, 0.1248, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1248, 0.1251, 0.1250, 0.1249, 0.1253, 0.1250, 0.1248, 0.1251],
        [0.1251, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1250, 0.1252],
        [0.1248, 0.1250, 0.1251, 0.1252, 0.1248, 0.1251, 0.1248, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1248, 0.1253, 0.1250, 0.1251, 0.1249, 0.1251, 0.1249],
        [0.1251, 0.1249, 0.1250, 0.1251, 0.1250, 0.1247, 0.1252, 0.1250],
        [0.1249, 0.1251, 0.1249, 0.1251, 0.1251, 0.1249, 0.1250, 0.1248],
        [0.1251, 0.1249, 0.1249, 0.1251, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1248, 0.1253, 0.1250, 0.1251, 0.1249, 0.1251, 0.1249],
        [0.1251, 0.1249, 0.1250, 0.1251, 0.1250, 0.1247, 0.1252, 0.1250],
        [0.1249, 0.1251, 0.1249, 0.1251, 0.1251, 0.1249, 0.1250, 0.1248],
        [0.1251, 0.1249, 0.1249, 0.1251, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1252, 0.1251, 0.1248, 0.1251, 0.1251, 0.1250, 0.1250, 0.1247],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1249, 0.1252, 0.1250, 0.1251],
        [0.1251, 0.1248, 0.1251, 0.1249, 0.1251, 0.1250, 0.1250, 0.1250],
        [0.1248, 0.1251, 0.1253, 0.1247, 0.1250, 0.1251, 0.1249, 0.1250],
        [0.1252, 0.1250, 0.1249, 0.1250, 0.1249, 0.1251, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1252, 0.1251, 0.1248, 0.1251, 0.1251, 0.1250, 0.1250, 0.1247],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1249, 0.1252, 0.1250, 0.1251],
        [0.1251, 0.1248, 0.1251, 0.1249, 0.1251, 0.1250, 0.1250, 0.1250],
        [0.1248, 0.1251, 0.1253, 0.1247, 0.1250, 0.1251, 0.1249, 0.1250],
        [0.1252, 0.1250, 0.1249, 0.1250, 0.1249, 0.1251, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce

# Alpha - reduce
tensor([[0.1247, 0.1250, 0.1251, 0.1252, 0.1252, 0.1250, 0.1248, 0.1250],
        [0.1250, 0.1250, 0.1252, 0.1249, 0.1249, 0.1250, 0.1252, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1247, 0.1250, 0.1251, 0.1252, 0.1252, 0.1250, 0.1248, 0.1250],
        [0.1250, 0.1250, 0.1252, 0.1249, 0.1249, 0.1250, 0.1252, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1251, 0.1249, 0.1250, 0.1248, 0.1249, 0.1251, 0.1252],
        [0.1250, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1246, 0.1252],
        [0.1250, 0.1249, 0.1251, 0.1251, 0.1249, 0.1249, 0.1253, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1251, 0.1249, 0.1250, 0.1248, 0.1249, 0.1251, 0.1252],
        [0.1250, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1246, 0.1252],
        [0.1250, 0.1249, 0.1251, 0.1251, 0.1249, 0.1249, 0.1253, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1252],
        [0.1249, 0.1249, 0.1249, 0.1250, 0.1250, 0.1251, 0.1251, 0.1249],
        [0.1249, 0.1252, 0.1250, 0.1250, 0.1251, 0.1249, 0.1250, 0.1249],
        [0.1251, 0.1249, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1252],
        [0.1249, 0.1249, 0.1249, 0.1250, 0.1250, 0.1251, 0.1251, 0.1249],
        [0.1249, 0.1252, 0.1250, 0.1250, 0.1251, 0.1249, 0.1250, 0.1249],
        [0.1251, 0.1249, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1249],
        [0.1251, 0.1249, 0.1249, 0.1250, 0.1248, 0.1251, 0.1251, 0.1251],
        [0.1249, 0.1249, 0.1252, 0.1250, 0.1251, 0.1248, 0.1252, 0.1250],
        [0.1248, 0.1249, 0.1251, 0.1251, 0.1249, 0.1251, 0.1251, 0.1250],
        [0.1251, 0.1250, 0.1252, 0.1252, 0.1249, 0.1248, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1249],
        [0.1251, 0.1249, 0.1249, 0.1250, 0.1248, 0.1251, 0.1251, 0.1251],
        [0.1249, 0.1249, 0.1252, 0.1250, 0.1251, 0.1248, 0.1252, 0.1250],
        [0.1248, 0.1249, 0.1251, 0.1251, 0.1249, 0.1251, 0.1251, 0.1250],
        [0.1251, 0.1250, 0.1252, 0.1252, 0.1249, 0.1248, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
#####################
07/31 11:48:30 PM | Logger is set 
07/31 11:48:30 PM | Logger without distribution
07/31 11:48:30 PM | Logger is set 
07/31 11:48:30 PM | Logger without distribution
07/31 11:48:56 PM | Initializing dataset used 25.446383714675903 basic time unit
07/31 11:48:56 PM | The training classes labels length :  751
07/31 11:48:56 PM | Initializing dataset used 25.462564706802368 basic time unit
07/31 11:48:56 PM | The training classes labels length :  751
07/31 11:49:56 PM | batch loading time example is 60.35570025444031
07/31 11:49:56 PM | batch loading time example is 60.35316205024719
####### ALPHA #######
# Alpha - normal
####### ALPHA #######
# Alpha - normal
tensor([[0.1251, 0.1250, 0.1251, 0.1249, 0.1249, 0.1251, 0.1251, 0.1248],
        [0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1251, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1250, 0.1251, 0.1249, 0.1249, 0.1251, 0.1251, 0.1248],
        [0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1251, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1248, 0.1251, 0.1250, 0.1249, 0.1253, 0.1250, 0.1248, 0.1251],
        [0.1251, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1250, 0.1252],
        [0.1248, 0.1250, 0.1251, 0.1252, 0.1248, 0.1251, 0.1248, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1248, 0.1251, 0.1250, 0.1249, 0.1253, 0.1250, 0.1248, 0.1251],
        [0.1251, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1250, 0.1252],
        [0.1248, 0.1250, 0.1251, 0.1252, 0.1248, 0.1251, 0.1248, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1248, 0.1253, 0.1250, 0.1251, 0.1249, 0.1251, 0.1249],
        [0.1251, 0.1249, 0.1250, 0.1251, 0.1250, 0.1247, 0.1252, 0.1250],
        [0.1249, 0.1251, 0.1249, 0.1251, 0.1251, 0.1249, 0.1250, 0.1248],
        [0.1251, 0.1249, 0.1249, 0.1251, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1248, 0.1253, 0.1250, 0.1251, 0.1249, 0.1251, 0.1249],
        [0.1251, 0.1249, 0.1250, 0.1251, 0.1250, 0.1247, 0.1252, 0.1250],
        [0.1249, 0.1251, 0.1249, 0.1251, 0.1251, 0.1249, 0.1250, 0.1248],
        [0.1251, 0.1249, 0.1249, 0.1251, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1252, 0.1251, 0.1248, 0.1251, 0.1251, 0.1250, 0.1250, 0.1247],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1249, 0.1252, 0.1250, 0.1251],
        [0.1251, 0.1248, 0.1251, 0.1249, 0.1251, 0.1250, 0.1250, 0.1250],
        [0.1248, 0.1251, 0.1253, 0.1247, 0.1250, 0.1251, 0.1249, 0.1250],
        [0.1252, 0.1250, 0.1249, 0.1250, 0.1249, 0.1251, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1252, 0.1251, 0.1248, 0.1251, 0.1251, 0.1250, 0.1250, 0.1247],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1249, 0.1252, 0.1250, 0.1251],
        [0.1251, 0.1248, 0.1251, 0.1249, 0.1251, 0.1250, 0.1250, 0.1250],
        [0.1248, 0.1251, 0.1253, 0.1247, 0.1250, 0.1251, 0.1249, 0.1250],
        [0.1252, 0.1250, 0.1249, 0.1250, 0.1249, 0.1251, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1247, 0.1250, 0.1251, 0.1252, 0.1252, 0.1250, 0.1248, 0.1250],
        [0.1250, 0.1250, 0.1252, 0.1249, 0.1249, 0.1250, 0.1252, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1250, 0.1251, 0.1249, 0.1250, 0.1248, 0.1249, 0.1251, 0.1252],
        [0.1250, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1246, 0.1252],
        [0.1250, 0.1249, 0.1251, 0.1251, 0.1249, 0.1249, 0.1253, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1247, 0.1250, 0.1251, 0.1252, 0.1252, 0.1250, 0.1248, 0.1250],
        [0.1250, 0.1250, 0.1252, 0.1249, 0.1249, 0.1250, 0.1252, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1252],
        [0.1249, 0.1249, 0.1249, 0.1250, 0.1250, 0.1251, 0.1251, 0.1249],
        [0.1249, 0.1252, 0.1250, 0.1250, 0.1251, 0.1249, 0.1250, 0.1249],
        [0.1251, 0.1249, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1251, 0.1249, 0.1250, 0.1248, 0.1249, 0.1251, 0.1252],
        [0.1250, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1246, 0.1252],
        [0.1250, 0.1249, 0.1251, 0.1251, 0.1249, 0.1249, 0.1253, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1249],
        [0.1251, 0.1249, 0.1249, 0.1250, 0.1248, 0.1251, 0.1251, 0.1251],
        [0.1249, 0.1249, 0.1252, 0.1250, 0.1251, 0.1248, 0.1252, 0.1250],
        [0.1248, 0.1249, 0.1251, 0.1251, 0.1249, 0.1251, 0.1251, 0.1250],
        [0.1251, 0.1250, 0.1252, 0.1252, 0.1249, 0.1248, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1252],
        [0.1249, 0.1249, 0.1249, 0.1250, 0.1250, 0.1251, 0.1251, 0.1249],
        [0.1249, 0.1252, 0.1250, 0.1250, 0.1251, 0.1249, 0.1250, 0.1249],
        [0.1251, 0.1249, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
tensor([[0.1250, 0.1250, 0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1249],
        [0.1251, 0.1249, 0.1249, 0.1250, 0.1248, 0.1251, 0.1251, 0.1251],
        [0.1249, 0.1249, 0.1252, 0.1250, 0.1251, 0.1248, 0.1252, 0.1250],
        [0.1248, 0.1249, 0.1251, 0.1251, 0.1249, 0.1251, 0.1251, 0.1250],
        [0.1251, 0.1250, 0.1252, 0.1252, 0.1249, 0.1248, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
08/01 10:21:26 AM | Logger is set 
08/01 10:21:26 AM | Logger without distribution
08/01 10:21:26 AM | Logger is set 
08/01 10:21:26 AM | Logger without distribution
08/01 10:23:37 AM | Initializing dataset used 130.76018261909485 basic time unit
08/01 10:23:37 AM | Initializing dataset used 130.7912368774414 basic time unit
08/01 10:23:37 AM | The training classes labels length :  751
08/01 10:23:37 AM | The training classes labels length :  751
08/01 10:25:59 AM | batch loading time example is 142.4091033935547
08/01 10:25:59 AM | batch loading time example is 142.40975522994995
####### ALPHA #######
# Alpha - normal
####### ALPHA #######
# Alpha - normal
tensor([[0.1251, 0.1250, 0.1251, 0.1249, 0.1249, 0.1251, 0.1251, 0.1248],
        [0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1251, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1250, 0.1251, 0.1249, 0.1249, 0.1251, 0.1251, 0.1248],
        [0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1251, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1248, 0.1251, 0.1250, 0.1249, 0.1253, 0.1250, 0.1248, 0.1251],
        [0.1251, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1250, 0.1252],
        [0.1248, 0.1250, 0.1251, 0.1252, 0.1248, 0.1251, 0.1248, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1248, 0.1251, 0.1250, 0.1249, 0.1253, 0.1250, 0.1248, 0.1251],
        [0.1251, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1250, 0.1252],
        [0.1248, 0.1250, 0.1251, 0.1252, 0.1248, 0.1251, 0.1248, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1248, 0.1253, 0.1250, 0.1251, 0.1249, 0.1251, 0.1249],
        [0.1251, 0.1249, 0.1250, 0.1251, 0.1250, 0.1247, 0.1252, 0.1250],
        [0.1249, 0.1251, 0.1249, 0.1251, 0.1251, 0.1249, 0.1250, 0.1248],
        [0.1251, 0.1249, 0.1249, 0.1251, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1248, 0.1253, 0.1250, 0.1251, 0.1249, 0.1251, 0.1249],
        [0.1251, 0.1249, 0.1250, 0.1251, 0.1250, 0.1247, 0.1252, 0.1250],
        [0.1249, 0.1251, 0.1249, 0.1251, 0.1251, 0.1249, 0.1250, 0.1248],
        [0.1251, 0.1249, 0.1249, 0.1251, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1252, 0.1251, 0.1248, 0.1251, 0.1251, 0.1250, 0.1250, 0.1247],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1249, 0.1252, 0.1250, 0.1251],
        [0.1251, 0.1248, 0.1251, 0.1249, 0.1251, 0.1250, 0.1250, 0.1250],
        [0.1248, 0.1251, 0.1253, 0.1247, 0.1250, 0.1251, 0.1249, 0.1250],
        [0.1252, 0.1250, 0.1249, 0.1250, 0.1249, 0.1251, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1252, 0.1251, 0.1248, 0.1251, 0.1251, 0.1250, 0.1250, 0.1247],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1249, 0.1252, 0.1250, 0.1251],
        [0.1251, 0.1248, 0.1251, 0.1249, 0.1251, 0.1250, 0.1250, 0.1250],
        [0.1248, 0.1251, 0.1253, 0.1247, 0.1250, 0.1251, 0.1249, 0.1250],
        [0.1252, 0.1250, 0.1249, 0.1250, 0.1249, 0.1251, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce

# Alpha - reduce
tensor([[0.1247, 0.1250, 0.1251, 0.1252, 0.1252, 0.1250, 0.1248, 0.1250],
        [0.1250, 0.1250, 0.1252, 0.1249, 0.1249, 0.1250, 0.1252, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1247, 0.1250, 0.1251, 0.1252, 0.1252, 0.1250, 0.1248, 0.1250],
        [0.1250, 0.1250, 0.1252, 0.1249, 0.1249, 0.1250, 0.1252, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1251, 0.1249, 0.1250, 0.1248, 0.1249, 0.1251, 0.1252],
        [0.1250, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1246, 0.1252],
        [0.1250, 0.1249, 0.1251, 0.1251, 0.1249, 0.1249, 0.1253, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1251, 0.1249, 0.1250, 0.1248, 0.1249, 0.1251, 0.1252],
        [0.1250, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1246, 0.1252],
        [0.1250, 0.1249, 0.1251, 0.1251, 0.1249, 0.1249, 0.1253, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1252],
        [0.1249, 0.1249, 0.1249, 0.1250, 0.1250, 0.1251, 0.1251, 0.1249],
        [0.1249, 0.1252, 0.1250, 0.1250, 0.1251, 0.1249, 0.1250, 0.1249],
        [0.1251, 0.1249, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1252],
        [0.1249, 0.1249, 0.1249, 0.1250, 0.1250, 0.1251, 0.1251, 0.1249],
        [0.1249, 0.1252, 0.1250, 0.1250, 0.1251, 0.1249, 0.1250, 0.1249],
        [0.1251, 0.1249, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1249],
        [0.1251, 0.1249, 0.1249, 0.1250, 0.1248, 0.1251, 0.1251, 0.1251],
        [0.1249, 0.1249, 0.1252, 0.1250, 0.1251, 0.1248, 0.1252, 0.1250],
        [0.1248, 0.1249, 0.1251, 0.1251, 0.1249, 0.1251, 0.1251, 0.1250],
        [0.1251, 0.1250, 0.1252, 0.1252, 0.1249, 0.1248, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1249],
        [0.1251, 0.1249, 0.1249, 0.1250, 0.1248, 0.1251, 0.1251, 0.1251],
        [0.1249, 0.1249, 0.1252, 0.1250, 0.1251, 0.1248, 0.1252, 0.1250],
        [0.1248, 0.1249, 0.1251, 0.1251, 0.1249, 0.1251, 0.1251, 0.1250],
        [0.1251, 0.1250, 0.1252, 0.1252, 0.1249, 0.1248, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
#####################
08/01 10:29:51 AM | Train: [ 1/200] Step 000/6467 Loss 6.567 Prec@(1,5) (0.0%, 0.0%)
08/01 10:30:10 AM | Train: [ 1/200] Step 010/6467 Loss 6.623 Prec@(1,5) (4.5%, 4.5%)
08/01 10:30:31 AM | Train: [ 1/200] Step 020/6467 Loss 6.695 Prec@(1,5) (2.4%, 7.1%)
08/01 10:30:49 AM | Train: [ 1/200] Step 030/6467 Loss 6.700 Prec@(1,5) (1.6%, 4.8%)
08/01 10:31:06 AM | Train: [ 1/200] Step 040/6467 Loss 6.731 Prec@(1,5) (1.2%, 3.7%)
08/01 10:31:24 AM | Train: [ 1/200] Step 050/6467 Loss 6.725 Prec@(1,5) (1.0%, 2.9%)
08/01 10:31:41 AM | Train: [ 1/200] Step 060/6467 Loss 6.676 Prec@(1,5) (0.8%, 2.5%)
08/01 10:31:59 AM | Train: [ 1/200] Step 070/6467 Loss 6.708 Prec@(1,5) (0.7%, 2.1%)
08/01 10:32:18 AM | Train: [ 1/200] Step 080/6467 Loss 6.710 Prec@(1,5) (0.6%, 1.9%)
08/01 10:32:36 AM | Train: [ 1/200] Step 090/6467 Loss 6.715 Prec@(1,5) (0.5%, 1.6%)
08/01 10:32:53 AM | Train: [ 1/200] Step 100/6467 Loss 6.710 Prec@(1,5) (0.5%, 1.5%)
08/01 10:35:44 AM | Logger is set 
08/01 10:35:44 AM | Logger is set 
08/01 10:35:44 AM | Logger without distribution
08/01 10:35:44 AM | Logger without distribution
08/01 10:36:39 AM | Initializing dataset used 55.01461863517761 basic time unit
08/01 10:36:39 AM | The training classes labels length :  751
08/01 10:36:44 AM | Initializing dataset used 60.16706943511963 basic time unit
08/01 10:36:44 AM | The training classes labels length :  751
08/01 10:38:38 AM | batch loading time example is 118.65790629386902
08/01 10:38:38 AM | batch loading time example is 113.50479340553284
####### ALPHA #######
# Alpha - normal
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1248, 0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251],
        [0.1249, 0.1250, 0.1247, 0.1250, 0.1252, 0.1251, 0.1250, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1248, 0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251],
        [0.1249, 0.1250, 0.1247, 0.1250, 0.1252, 0.1251, 0.1250, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1251],
        [0.1251, 0.1250, 0.1248, 0.1249, 0.1251, 0.1252, 0.1251, 0.1249],
        [0.1252, 0.1250, 0.1251, 0.1251, 0.1250, 0.1246, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1251],
        [0.1251, 0.1250, 0.1248, 0.1249, 0.1251, 0.1252, 0.1251, 0.1249],
        [0.1252, 0.1250, 0.1251, 0.1251, 0.1250, 0.1246, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1252, 0.1249, 0.1250, 0.1249, 0.1251, 0.1250, 0.1249, 0.1249],
        [0.1248, 0.1249, 0.1251, 0.1252, 0.1250, 0.1248, 0.1250, 0.1252],
        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1251, 0.1250, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1250, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1252, 0.1249, 0.1250, 0.1249, 0.1251, 0.1250, 0.1249, 0.1249],
        [0.1248, 0.1249, 0.1251, 0.1252, 0.1250, 0.1248, 0.1250, 0.1252],
        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1251, 0.1250, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1250, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1248, 0.1250, 0.1248, 0.1251, 0.1251, 0.1249, 0.1252],
        [0.1250, 0.1251, 0.1251, 0.1250, 0.1251, 0.1249, 0.1251, 0.1248],
        [0.1248, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251, 0.1252, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1249, 0.1250, 0.1250, 0.1251, 0.1250],
        [0.1251, 0.1251, 0.1250, 0.1249, 0.1250, 0.1252, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1248, 0.1250, 0.1248, 0.1251, 0.1251, 0.1249, 0.1252],
        [0.1250, 0.1251, 0.1251, 0.1250, 0.1251, 0.1249, 0.1251, 0.1248],
        [0.1248, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251, 0.1252, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1249, 0.1250, 0.1250, 0.1251, 0.1250],
        [0.1251, 0.1251, 0.1250, 0.1249, 0.1250, 0.1252, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce

# Alpha - reduce
tensor([[0.1252, 0.1252, 0.1248, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249],
        [0.1251, 0.1251, 0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1252, 0.1252, 0.1248, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249],
        [0.1251, 0.1251, 0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1251, 0.1249, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250],
        [0.1251, 0.1251, 0.1249, 0.1252, 0.1250, 0.1250, 0.1248, 0.1249],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1248, 0.1251, 0.1251, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1251, 0.1249, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250],
        [0.1251, 0.1251, 0.1249, 0.1252, 0.1250, 0.1250, 0.1248, 0.1249],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1248, 0.1251, 0.1251, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1251, 0.1249, 0.1250, 0.1249, 0.1250, 0.1251, 0.1250],
        [0.1251, 0.1249, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],
        [0.1251, 0.1249, 0.1250, 0.1252, 0.1251, 0.1247, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1251, 0.1248, 0.1250, 0.1251, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1251, 0.1249, 0.1250, 0.1249, 0.1250, 0.1251, 0.1250],
        [0.1251, 0.1249, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],
        [0.1251, 0.1249, 0.1250, 0.1252, 0.1251, 0.1247, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1251, 0.1248, 0.1250, 0.1251, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1251, 0.1249, 0.1251, 0.1248, 0.1251, 0.1252, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1248, 0.1251, 0.1252, 0.1250],
        [0.1251, 0.1251, 0.1250, 0.1252, 0.1250, 0.1248, 0.1250, 0.1248],
        [0.1253, 0.1251, 0.1250, 0.1249, 0.1250, 0.1249, 0.1248, 0.1249],
        [0.1251, 0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1251, 0.1249, 0.1251, 0.1248, 0.1251, 0.1252, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1248, 0.1251, 0.1252, 0.1250],
        [0.1251, 0.1251, 0.1250, 0.1252, 0.1250, 0.1248, 0.1250, 0.1248],
        [0.1253, 0.1251, 0.1250, 0.1249, 0.1250, 0.1249, 0.1248, 0.1249],
        [0.1251, 0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
#####################
08/01 10:45:55 AM | Logger is set 
08/01 10:45:55 AM | Logger without distribution
08/01 10:46:40 AM | Initializing dataset used 44.43269371986389 basic time unit
08/01 10:46:40 AM | The training classes labels length :  751
08/01 10:48:56 AM | batch loading time example is 135.94510436058044
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1248, 0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251],
        [0.1249, 0.1250, 0.1247, 0.1250, 0.1252, 0.1251, 0.1250, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1251],
        [0.1251, 0.1250, 0.1248, 0.1249, 0.1251, 0.1252, 0.1251, 0.1249],
        [0.1252, 0.1250, 0.1251, 0.1251, 0.1250, 0.1246, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1252, 0.1249, 0.1250, 0.1249, 0.1251, 0.1250, 0.1249, 0.1249],
        [0.1248, 0.1249, 0.1251, 0.1252, 0.1250, 0.1248, 0.1250, 0.1252],
        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1251, 0.1250, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1250, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1248, 0.1250, 0.1248, 0.1251, 0.1251, 0.1249, 0.1252],
        [0.1250, 0.1251, 0.1251, 0.1250, 0.1251, 0.1249, 0.1251, 0.1248],
        [0.1248, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251, 0.1252, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1249, 0.1250, 0.1250, 0.1251, 0.1250],
        [0.1251, 0.1251, 0.1250, 0.1249, 0.1250, 0.1252, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1252, 0.1252, 0.1248, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249],
        [0.1251, 0.1251, 0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1251, 0.1249, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250],
        [0.1251, 0.1251, 0.1249, 0.1252, 0.1250, 0.1250, 0.1248, 0.1249],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1248, 0.1251, 0.1251, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1251, 0.1249, 0.1250, 0.1249, 0.1250, 0.1251, 0.1250],
        [0.1251, 0.1249, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],
        [0.1251, 0.1249, 0.1250, 0.1252, 0.1251, 0.1247, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1251, 0.1248, 0.1250, 0.1251, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1251, 0.1249, 0.1251, 0.1248, 0.1251, 0.1252, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1248, 0.1251, 0.1252, 0.1250],
        [0.1251, 0.1251, 0.1250, 0.1252, 0.1250, 0.1248, 0.1250, 0.1248],
        [0.1253, 0.1251, 0.1250, 0.1249, 0.1250, 0.1249, 0.1248, 0.1249],
        [0.1251, 0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
08/01 10:53:22 AM | Train: [ 1/200] Step 000/6092 Loss 6.589 Prec@(1,5) (0.0%, 0.0%)
08/01 10:53:39 AM | Train: [ 1/200] Step 010/6092 Loss 6.646 Prec@(1,5) (0.0%, 0.0%)
08/01 10:53:55 AM | Train: [ 1/200] Step 020/6092 Loss 6.616 Prec@(1,5) (0.0%, 2.4%)
08/01 10:54:11 AM | Train: [ 1/200] Step 030/6092 Loss 6.835 Prec@(1,5) (0.0%, 1.6%)
08/01 10:54:28 AM | Train: [ 1/200] Step 040/6092 Loss 6.833 Prec@(1,5) (0.0%, 1.2%)
08/01 10:54:44 AM | Train: [ 1/200] Step 050/6092 Loss 6.806 Prec@(1,5) (0.0%, 1.0%)
08/01 10:55:00 AM | Train: [ 1/200] Step 060/6092 Loss 6.787 Prec@(1,5) (0.0%, 1.6%)
08/01 10:55:19 AM | Train: [ 1/200] Step 070/6092 Loss 6.798 Prec@(1,5) (0.0%, 1.4%)
08/01 10:55:35 AM | Train: [ 1/200] Step 080/6092 Loss 6.798 Prec@(1,5) (0.0%, 1.2%)
08/01 10:55:51 AM | Train: [ 1/200] Step 090/6092 Loss 6.793 Prec@(1,5) (0.0%, 1.1%)
08/01 10:56:08 AM | Train: [ 1/200] Step 100/6092 Loss 6.797 Prec@(1,5) (0.0%, 1.0%)
08/01 10:56:25 AM | Train: [ 1/200] Step 110/6092 Loss 6.791 Prec@(1,5) (0.0%, 0.9%)
08/01 10:56:42 AM | Train: [ 1/200] Step 120/6092 Loss 6.766 Prec@(1,5) (0.4%, 1.7%)
08/01 10:56:59 AM | Train: [ 1/200] Step 130/6092 Loss 6.783 Prec@(1,5) (0.4%, 1.5%)
08/01 10:57:15 AM | Train: [ 1/200] Step 140/6092 Loss 6.777 Prec@(1,5) (0.4%, 1.4%)
08/01 10:57:32 AM | Train: [ 1/200] Step 150/6092 Loss 6.782 Prec@(1,5) (0.3%, 1.3%)
08/01 10:57:49 AM | Train: [ 1/200] Step 160/6092 Loss 6.777 Prec@(1,5) (0.3%, 1.2%)
08/01 10:58:05 AM | Train: [ 1/200] Step 170/6092 Loss 6.776 Prec@(1,5) (0.3%, 1.2%)
08/01 10:58:22 AM | Train: [ 1/200] Step 180/6092 Loss 6.767 Prec@(1,5) (0.3%, 1.1%)
08/01 10:58:40 AM | Train: [ 1/200] Step 190/6092 Loss 6.754 Prec@(1,5) (0.3%, 1.3%)
08/01 10:58:56 AM | Train: [ 1/200] Step 200/6092 Loss 6.752 Prec@(1,5) (0.2%, 1.2%)
08/01 10:59:13 AM | Train: [ 1/200] Step 210/6092 Loss 6.756 Prec@(1,5) (0.2%, 1.2%)
08/01 10:59:29 AM | Train: [ 1/200] Step 220/6092 Loss 6.753 Prec@(1,5) (0.2%, 1.1%)
08/01 10:59:46 AM | Train: [ 1/200] Step 230/6092 Loss 6.752 Prec@(1,5) (0.2%, 1.1%)
08/01 11:00:02 AM | Train: [ 1/200] Step 240/6092 Loss 6.749 Prec@(1,5) (0.2%, 1.0%)
08/01 11:00:20 AM | Train: [ 1/200] Step 250/6092 Loss 6.744 Prec@(1,5) (0.2%, 1.0%)
08/01 11:00:36 AM | Train: [ 1/200] Step 260/6092 Loss 6.748 Prec@(1,5) (0.2%, 1.0%)
08/01 11:00:53 AM | Train: [ 1/200] Step 270/6092 Loss 6.744 Prec@(1,5) (0.2%, 0.9%)
08/01 11:01:09 AM | Train: [ 1/200] Step 280/6092 Loss 6.745 Prec@(1,5) (0.2%, 0.9%)
08/01 11:01:26 AM | Train: [ 1/200] Step 290/6092 Loss 6.739 Prec@(1,5) (0.2%, 0.9%)
08/01 11:01:42 AM | Train: [ 1/200] Step 300/6092 Loss 6.734 Prec@(1,5) (0.2%, 0.8%)
08/01 11:01:58 AM | Train: [ 1/200] Step 310/6092 Loss 6.727 Prec@(1,5) (0.2%, 0.8%)
08/01 11:02:15 AM | Train: [ 1/200] Step 320/6092 Loss 6.725 Prec@(1,5) (0.2%, 0.8%)
08/01 11:02:31 AM | Train: [ 1/200] Step 330/6092 Loss 6.717 Prec@(1,5) (0.3%, 0.9%)
08/01 11:02:48 AM | Train: [ 1/200] Step 340/6092 Loss 6.715 Prec@(1,5) (0.3%, 0.9%)
08/01 11:03:04 AM | Train: [ 1/200] Step 350/6092 Loss 6.711 Prec@(1,5) (0.3%, 0.9%)
08/01 11:03:21 AM | Train: [ 1/200] Step 360/6092 Loss 6.709 Prec@(1,5) (0.3%, 0.8%)
08/01 11:03:37 AM | Train: [ 1/200] Step 370/6092 Loss 6.701 Prec@(1,5) (0.3%, 0.8%)
08/01 11:03:49 AM | Train: [ 1/200] Final Prec@1 0.2660%
08/01 11:05:34 AM | Valid: [ 1/200] Step 000/375 Loss 6.037 Prec@(1,5) (0.0%, 0.0%)
08/01 11:05:36 AM | Valid: [ 1/200] Step 010/375 Loss 6.370 Prec@(1,5) (4.5%, 4.5%)
08/01 11:05:36 AM | Valid: [ 1/200] Step 020/375 Loss 6.621 Prec@(1,5) (2.4%, 2.4%)
08/01 11:05:37 AM | Valid: [ 1/200] Step 030/375 Loss 6.844 Prec@(1,5) (1.6%, 1.6%)
08/01 11:05:38 AM | Valid: [ 1/200] Step 040/375 Loss 6.846 Prec@(1,5) (1.2%, 1.2%)
08/01 11:05:39 AM | Valid: [ 1/200] Step 050/375 Loss 6.842 Prec@(1,5) (1.0%, 1.0%)
08/01 11:05:40 AM | Valid: [ 1/200] Step 060/375 Loss 6.814 Prec@(1,5) (0.8%, 1.6%)
08/01 11:05:41 AM | Valid: [ 1/200] Step 070/375 Loss 6.847 Prec@(1,5) (0.7%, 1.4%)
08/01 11:05:42 AM | Valid: [ 1/200] Step 080/375 Loss 6.818 Prec@(1,5) (0.6%, 1.2%)
08/01 11:05:42 AM | Valid: [ 1/200] Step 090/375 Loss 6.800 Prec@(1,5) (0.5%, 1.1%)
08/01 11:05:43 AM | Valid: [ 1/200] Step 100/375 Loss 6.802 Prec@(1,5) (0.5%, 1.0%)
08/01 11:05:44 AM | Valid: [ 1/200] Step 110/375 Loss 6.837 Prec@(1,5) (0.5%, 0.9%)
08/01 11:05:45 AM | Valid: [ 1/200] Step 120/375 Loss 6.831 Prec@(1,5) (0.4%, 0.8%)
08/01 11:05:46 AM | Valid: [ 1/200] Step 130/375 Loss 6.832 Prec@(1,5) (0.4%, 0.8%)
08/01 11:05:47 AM | Valid: [ 1/200] Step 140/375 Loss 6.827 Prec@(1,5) (0.4%, 0.7%)
08/01 11:05:47 AM | Valid: [ 1/200] Step 150/375 Loss 6.869 Prec@(1,5) (0.3%, 0.7%)
08/01 11:05:48 AM | Valid: [ 1/200] Step 160/375 Loss 6.864 Prec@(1,5) (0.3%, 0.6%)
08/01 11:05:50 AM | Valid: [ 1/200] Step 170/375 Loss 6.891 Prec@(1,5) (0.3%, 0.6%)
08/01 11:05:50 AM | Valid: [ 1/200] Step 180/375 Loss 6.861 Prec@(1,5) (0.3%, 1.1%)
08/01 11:05:51 AM | Valid: [ 1/200] Step 190/375 Loss 6.867 Prec@(1,5) (0.3%, 1.0%)
08/01 11:05:52 AM | Valid: [ 1/200] Step 200/375 Loss 6.850 Prec@(1,5) (0.2%, 1.2%)
08/01 11:05:53 AM | Valid: [ 1/200] Step 210/375 Loss 6.856 Prec@(1,5) (0.2%, 1.2%)
08/01 11:05:54 AM | Valid: [ 1/200] Step 220/375 Loss 6.853 Prec@(1,5) (0.2%, 1.1%)
08/01 11:05:55 AM | Valid: [ 1/200] Step 230/375 Loss 6.849 Prec@(1,5) (0.2%, 1.1%)
08/01 11:05:56 AM | Valid: [ 1/200] Step 240/375 Loss 6.836 Prec@(1,5) (0.2%, 1.0%)
08/01 11:05:56 AM | Valid: [ 1/200] Step 250/375 Loss 6.840 Prec@(1,5) (0.2%, 1.0%)
08/01 11:05:57 AM | Valid: [ 1/200] Step 260/375 Loss 6.833 Prec@(1,5) (0.2%, 1.1%)
08/01 11:05:58 AM | Valid: [ 1/200] Step 270/375 Loss 6.827 Prec@(1,5) (0.2%, 1.1%)
08/01 11:05:59 AM | Valid: [ 1/200] Step 280/375 Loss 6.820 Prec@(1,5) (0.4%, 1.2%)
08/01 11:06:00 AM | Valid: [ 1/200] Step 290/375 Loss 6.826 Prec@(1,5) (0.3%, 1.2%)
08/01 11:06:01 AM | Valid: [ 1/200] Step 300/375 Loss 6.826 Prec@(1,5) (0.3%, 1.2%)
08/01 11:06:01 AM | Valid: [ 1/200] Step 310/375 Loss 6.828 Prec@(1,5) (0.3%, 1.1%)
08/01 11:06:02 AM | Valid: [ 1/200] Step 320/375 Loss 6.836 Prec@(1,5) (0.3%, 1.1%)
08/01 11:06:03 AM | Valid: [ 1/200] Step 330/375 Loss 6.841 Prec@(1,5) (0.3%, 1.1%)
08/01 11:06:04 AM | Valid: [ 1/200] Step 340/375 Loss 6.838 Prec@(1,5) (0.3%, 1.0%)
08/01 11:06:05 AM | Valid: [ 1/200] Step 350/375 Loss 6.834 Prec@(1,5) (0.3%, 1.1%)
08/01 11:06:06 AM | Valid: [ 1/200] Step 360/375 Loss 6.835 Prec@(1,5) (0.3%, 1.1%)
08/01 11:06:07 AM | Valid: [ 1/200] Step 370/375 Loss 6.835 Prec@(1,5) (0.3%, 1.1%)
08/01 11:06:09 AM | Valid: [ 1/200] Final Prec@1 0.2667%, Prec@5 1.0667%, Prec@10 2.2667%
08/01 11:06:09 AM | Final best Prec@1 = 0.2667%
####### ALPHA #######
# Alpha - normal
tensor([[0.0616, 0.0661, 0.0651, 0.1569, 0.3754, 0.0812, 0.1012, 0.0925],
        [0.0735, 0.0696, 0.0698, 0.2563, 0.1821, 0.1101, 0.1056, 0.1329]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1033, 0.1034, 0.1048, 0.1229, 0.1487, 0.1037, 0.0924, 0.2207],
        [0.0837, 0.0890, 0.0896, 0.1350, 0.2189, 0.0982, 0.0677, 0.2178],
        [0.1188, 0.1321, 0.1042, 0.0829, 0.2159, 0.1142, 0.1552, 0.0768]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0820, 0.0844, 0.0842, 0.1133, 0.1588, 0.2246, 0.0948, 0.1578],
        [0.1100, 0.1144, 0.1151, 0.1486, 0.1712, 0.0935, 0.1442, 0.1031],
        [0.0883, 0.0968, 0.0896, 0.1516, 0.1288, 0.1615, 0.1833, 0.1002],
        [0.0716, 0.0775, 0.0745, 0.1253, 0.2150, 0.1911, 0.1531, 0.0920]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1100, 0.1125, 0.1139, 0.1460, 0.1566, 0.1159, 0.1125, 0.1326],
        [0.1422, 0.1505, 0.1527, 0.1432, 0.0834, 0.0965, 0.1027, 0.1287],
        [0.1197, 0.1362, 0.1352, 0.1524, 0.1302, 0.0924, 0.1260, 0.1079],
        [0.0944, 0.1005, 0.1031, 0.1957, 0.1008, 0.1768, 0.0773, 0.1514],
        [0.0698, 0.0697, 0.0857, 0.2989, 0.1188, 0.0825, 0.1966, 0.0781]],
       grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1118, 0.1050, 0.0661, 0.1388, 0.0565, 0.0632, 0.0710, 0.3876],
        [0.0341, 0.0329, 0.0601, 0.0919, 0.0713, 0.0388, 0.0385, 0.6323]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0656, 0.0628, 0.0814, 0.0581, 0.0723, 0.0456, 0.0548, 0.5595],
        [0.0485, 0.0470, 0.1020, 0.0319, 0.0346, 0.0781, 0.0356, 0.6225],
        [0.0216, 0.0220, 0.0303, 0.0814, 0.0319, 0.0604, 0.0341, 0.7185]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0475, 0.0470, 0.0917, 0.0592, 0.0889, 0.0778, 0.0400, 0.5478],
        [0.0396, 0.0418, 0.0329, 0.1006, 0.0765, 0.0523, 0.0493, 0.6070],
        [0.0228, 0.0212, 0.0331, 0.0318, 0.0590, 0.0341, 0.0220, 0.7761],
        [0.0177, 0.0168, 0.0231, 0.1264, 0.0694, 0.0399, 0.0403, 0.6663]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0444, 0.0424, 0.0650, 0.0482, 0.0630, 0.0719, 0.0651, 0.6001],
        [0.0641, 0.0638, 0.0581, 0.0599, 0.0351, 0.0479, 0.0309, 0.6403],
        [0.0263, 0.0262, 0.0419, 0.0536, 0.0245, 0.0318, 0.0484, 0.7473],
        [0.0159, 0.0148, 0.0248, 0.0371, 0.0272, 0.0368, 0.0516, 0.7917],
        [0.0256, 0.0256, 0.0332, 0.1172, 0.1151, 0.0722, 0.0485, 0.5627]],
       grad_fn=<SoftmaxBackward>)
#####################
08/01 11:10:01 AM | Train: [ 2/200] Step 000/6092 Loss 6.947 Prec@(1,5) (0.0%, 0.0%)
08/01 11:10:19 AM | Train: [ 2/200] Step 010/6092 Loss 6.462 Prec@(1,5) (0.0%, 0.0%)
08/01 11:10:36 AM | Train: [ 2/200] Step 020/6092 Loss 6.663 Prec@(1,5) (0.0%, 0.0%)
08/01 11:10:52 AM | Train: [ 2/200] Step 030/6092 Loss 6.590 Prec@(1,5) (0.0%, 1.6%)
08/01 11:11:09 AM | Train: [ 2/200] Step 040/6092 Loss 6.563 Prec@(1,5) (0.0%, 2.4%)
08/01 11:11:26 AM | Train: [ 2/200] Step 050/6092 Loss 6.582 Prec@(1,5) (0.0%, 2.0%)
08/01 11:11:43 AM | Train: [ 2/200] Step 060/6092 Loss 6.548 Prec@(1,5) (0.0%, 2.5%)
08/01 11:11:59 AM | Train: [ 2/200] Step 070/6092 Loss 6.571 Prec@(1,5) (0.0%, 2.1%)
08/01 11:12:15 AM | Train: [ 2/200] Step 080/6092 Loss 6.576 Prec@(1,5) (0.0%, 1.9%)
08/01 11:12:31 AM | Train: [ 2/200] Step 090/6092 Loss 6.565 Prec@(1,5) (0.0%, 2.2%)
08/01 11:12:48 AM | Train: [ 2/200] Step 100/6092 Loss 6.568 Prec@(1,5) (0.5%, 2.5%)
08/01 11:13:05 AM | Train: [ 2/200] Step 110/6092 Loss 6.571 Prec@(1,5) (0.5%, 2.3%)
08/01 11:13:22 AM | Train: [ 2/200] Step 120/6092 Loss 6.555 Prec@(1,5) (0.4%, 2.1%)
08/01 11:13:39 AM | Train: [ 2/200] Step 130/6092 Loss 6.556 Prec@(1,5) (0.4%, 2.3%)
08/01 11:13:56 AM | Train: [ 2/200] Step 140/6092 Loss 6.549 Prec@(1,5) (0.4%, 2.1%)
08/01 11:14:12 AM | Train: [ 2/200] Step 150/6092 Loss 6.543 Prec@(1,5) (0.7%, 2.6%)
08/01 11:14:29 AM | Train: [ 2/200] Step 160/6092 Loss 6.543 Prec@(1,5) (0.6%, 2.5%)
08/01 11:14:45 AM | Train: [ 2/200] Step 170/6092 Loss 6.548 Prec@(1,5) (0.6%, 2.3%)
08/01 11:15:02 AM | Train: [ 2/200] Step 180/6092 Loss 6.552 Prec@(1,5) (0.8%, 2.5%)
08/01 11:15:20 AM | Train: [ 2/200] Step 190/6092 Loss 6.545 Prec@(1,5) (0.8%, 2.4%)
08/01 11:15:37 AM | Train: [ 2/200] Step 200/6092 Loss 6.534 Prec@(1,5) (0.7%, 2.5%)
08/01 11:15:54 AM | Train: [ 2/200] Step 210/6092 Loss 6.540 Prec@(1,5) (0.7%, 2.4%)
08/01 11:16:10 AM | Train: [ 2/200] Step 220/6092 Loss 6.538 Prec@(1,5) (0.7%, 2.3%)
08/01 11:16:27 AM | Train: [ 2/200] Step 230/6092 Loss 6.541 Prec@(1,5) (0.6%, 2.2%)
08/01 11:16:43 AM | Train: [ 2/200] Step 240/6092 Loss 6.544 Prec@(1,5) (0.6%, 2.3%)
08/01 11:17:00 AM | Train: [ 2/200] Step 250/6092 Loss 6.541 Prec@(1,5) (0.6%, 2.4%)
08/01 11:17:17 AM | Train: [ 2/200] Step 260/6092 Loss 6.544 Prec@(1,5) (0.6%, 2.3%)
08/01 11:17:33 AM | Train: [ 2/200] Step 270/6092 Loss 6.540 Prec@(1,5) (0.6%, 2.2%)
08/01 11:17:50 AM | Train: [ 2/200] Step 280/6092 Loss 6.528 Prec@(1,5) (0.5%, 2.3%)
08/01 11:18:06 AM | Train: [ 2/200] Step 290/6092 Loss 6.529 Prec@(1,5) (0.5%, 2.2%)
08/01 11:18:23 AM | Train: [ 2/200] Step 300/6092 Loss 6.532 Prec@(1,5) (0.5%, 2.2%)
08/01 11:18:39 AM | Train: [ 2/200] Step 310/6092 Loss 6.530 Prec@(1,5) (0.6%, 2.3%)
08/01 11:18:55 AM | Train: [ 2/200] Step 320/6092 Loss 6.531 Prec@(1,5) (0.6%, 2.2%)
08/01 11:19:12 AM | Train: [ 2/200] Step 330/6092 Loss 6.530 Prec@(1,5) (0.6%, 2.1%)
08/01 11:19:28 AM | Train: [ 2/200] Step 340/6092 Loss 6.531 Prec@(1,5) (0.6%, 2.3%)
08/01 11:19:45 AM | Train: [ 2/200] Step 350/6092 Loss 6.524 Prec@(1,5) (0.6%, 2.3%)
08/01 11:20:02 AM | Train: [ 2/200] Step 360/6092 Loss 6.526 Prec@(1,5) (0.6%, 2.2%)
08/01 11:20:20 AM | Train: [ 2/200] Step 370/6092 Loss 6.528 Prec@(1,5) (0.5%, 2.3%)
08/01 11:20:29 AM | Train: [ 2/200] Final Prec@1 0.5319%
08/01 11:22:04 AM | Valid: [ 2/200] Step 000/375 Loss 6.567 Prec@(1,5) (0.0%, 0.0%)
08/01 11:22:05 AM | Valid: [ 2/200] Step 010/375 Loss 6.630 Prec@(1,5) (0.0%, 4.5%)
08/01 11:22:06 AM | Valid: [ 2/200] Step 020/375 Loss 6.764 Prec@(1,5) (0.0%, 2.4%)
08/01 11:22:07 AM | Valid: [ 2/200] Step 030/375 Loss 6.862 Prec@(1,5) (0.0%, 1.6%)
08/01 11:22:08 AM | Valid: [ 2/200] Step 040/375 Loss 6.882 Prec@(1,5) (0.0%, 1.2%)
08/01 11:22:09 AM | Valid: [ 2/200] Step 050/375 Loss 6.861 Prec@(1,5) (0.0%, 1.0%)
08/01 11:22:09 AM | Valid: [ 2/200] Step 060/375 Loss 6.844 Prec@(1,5) (0.8%, 1.6%)
08/01 11:22:10 AM | Valid: [ 2/200] Step 070/375 Loss 6.869 Prec@(1,5) (0.7%, 1.4%)
08/01 11:22:11 AM | Valid: [ 2/200] Step 080/375 Loss 6.863 Prec@(1,5) (0.6%, 1.2%)
08/01 11:22:12 AM | Valid: [ 2/200] Step 090/375 Loss 6.859 Prec@(1,5) (0.5%, 1.1%)
08/01 11:22:13 AM | Valid: [ 2/200] Step 100/375 Loss 6.854 Prec@(1,5) (0.5%, 1.0%)
08/01 11:22:14 AM | Valid: [ 2/200] Step 110/375 Loss 6.841 Prec@(1,5) (0.5%, 1.4%)
08/01 11:22:15 AM | Valid: [ 2/200] Step 120/375 Loss 6.841 Prec@(1,5) (0.4%, 1.2%)
08/01 11:22:15 AM | Valid: [ 2/200] Step 130/375 Loss 6.842 Prec@(1,5) (0.4%, 1.1%)
08/01 11:22:16 AM | Valid: [ 2/200] Step 140/375 Loss 6.848 Prec@(1,5) (0.4%, 1.8%)
08/01 11:22:17 AM | Valid: [ 2/200] Step 150/375 Loss 6.886 Prec@(1,5) (0.3%, 1.7%)
08/01 11:22:18 AM | Valid: [ 2/200] Step 160/375 Loss 6.865 Prec@(1,5) (0.6%, 1.9%)
08/01 11:22:19 AM | Valid: [ 2/200] Step 170/375 Loss 6.870 Prec@(1,5) (0.6%, 1.8%)
08/01 11:22:20 AM | Valid: [ 2/200] Step 180/375 Loss 6.859 Prec@(1,5) (0.6%, 1.9%)
08/01 11:22:20 AM | Valid: [ 2/200] Step 190/375 Loss 6.857 Prec@(1,5) (0.5%, 1.8%)
08/01 11:22:21 AM | Valid: [ 2/200] Step 200/375 Loss 6.831 Prec@(1,5) (0.5%, 2.0%)
08/01 11:22:22 AM | Valid: [ 2/200] Step 210/375 Loss 6.841 Prec@(1,5) (0.5%, 1.9%)
08/01 11:22:23 AM | Valid: [ 2/200] Step 220/375 Loss 6.828 Prec@(1,5) (0.5%, 2.0%)
08/01 11:22:24 AM | Valid: [ 2/200] Step 230/375 Loss 6.829 Prec@(1,5) (0.4%, 1.9%)
08/01 11:22:25 AM | Valid: [ 2/200] Step 240/375 Loss 6.816 Prec@(1,5) (0.4%, 1.9%)
08/01 11:22:26 AM | Valid: [ 2/200] Step 250/375 Loss 6.825 Prec@(1,5) (0.4%, 1.8%)
08/01 11:22:26 AM | Valid: [ 2/200] Step 260/375 Loss 6.802 Prec@(1,5) (0.4%, 2.1%)
08/01 11:22:27 AM | Valid: [ 2/200] Step 270/375 Loss 6.798 Prec@(1,5) (0.6%, 2.2%)
08/01 11:22:28 AM | Valid: [ 2/200] Step 280/375 Loss 6.807 Prec@(1,5) (0.5%, 2.1%)
08/01 11:22:29 AM | Valid: [ 2/200] Step 290/375 Loss 6.812 Prec@(1,5) (0.5%, 2.2%)
08/01 11:22:30 AM | Valid: [ 2/200] Step 300/375 Loss 6.813 Prec@(1,5) (0.5%, 2.2%)
08/01 11:22:31 AM | Valid: [ 2/200] Step 310/375 Loss 6.817 Prec@(1,5) (0.5%, 2.1%)
08/01 11:22:31 AM | Valid: [ 2/200] Step 320/375 Loss 6.823 Prec@(1,5) (0.5%, 2.0%)
08/01 11:22:32 AM | Valid: [ 2/200] Step 330/375 Loss 6.816 Prec@(1,5) (0.5%, 2.0%)
08/01 11:22:33 AM | Valid: [ 2/200] Step 340/375 Loss 6.815 Prec@(1,5) (0.4%, 1.9%)
08/01 11:22:34 AM | Valid: [ 2/200] Step 350/375 Loss 6.824 Prec@(1,5) (0.4%, 1.9%)
08/01 11:22:35 AM | Valid: [ 2/200] Step 360/375 Loss 6.827 Prec@(1,5) (0.4%, 1.8%)
08/01 11:22:36 AM | Valid: [ 2/200] Step 370/375 Loss 6.831 Prec@(1,5) (0.4%, 1.8%)
08/01 11:22:36 AM | Valid: [ 2/200] Final Prec@1 0.4000%, Prec@5 1.7333%, Prec@10 2.9333%
08/01 11:22:37 AM | Final best Prec@1 = 0.4000%
####### ALPHA #######
# Alpha - normal
tensor([[0.0680, 0.0696, 0.0701, 0.1619, 0.3162, 0.1097, 0.0879, 0.1166],
        [0.0769, 0.0701, 0.0715, 0.2430, 0.1830, 0.1269, 0.1086, 0.1199]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1178, 0.1252, 0.1429, 0.1191, 0.0987, 0.0949, 0.1764],
        [0.1008, 0.1010, 0.1053, 0.1324, 0.1551, 0.1160, 0.0783, 0.2112],
        [0.0957, 0.0988, 0.0971, 0.0826, 0.2103, 0.1337, 0.2072, 0.0746]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0896, 0.0908, 0.0929, 0.1375, 0.1542, 0.1929, 0.1030, 0.1392],
        [0.1151, 0.1155, 0.1195, 0.1426, 0.2004, 0.0891, 0.1150, 0.1028],
        [0.0713, 0.0765, 0.0793, 0.1448, 0.1491, 0.2350, 0.1523, 0.0916],
        [0.0858, 0.0903, 0.0885, 0.0917, 0.2220, 0.1963, 0.1161, 0.1092]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1151, 0.1190, 0.1254, 0.1484, 0.1306, 0.1290, 0.0964, 0.1361],
        [0.1402, 0.1452, 0.1529, 0.1653, 0.0800, 0.0929, 0.0967, 0.1267],
        [0.0940, 0.1013, 0.1194, 0.2087, 0.1199, 0.1120, 0.1054, 0.1392],
        [0.0830, 0.0873, 0.0936, 0.2269, 0.0933, 0.1791, 0.0817, 0.1551],
        [0.0627, 0.0635, 0.0773, 0.3173, 0.0907, 0.0825, 0.2224, 0.0837]],
       grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.0771, 0.0673, 0.0385, 0.0822, 0.0310, 0.0297, 0.0584, 0.6157],
        [0.0327, 0.0289, 0.0314, 0.0418, 0.0328, 0.0213, 0.0256, 0.7855]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0336, 0.0310, 0.0424, 0.0235, 0.0288, 0.0189, 0.0244, 0.7974],
        [0.0518, 0.0458, 0.0736, 0.0213, 0.0253, 0.0423, 0.0298, 0.7102],
        [0.0111, 0.0112, 0.0177, 0.0454, 0.0145, 0.0280, 0.0142, 0.8579]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0292, 0.0272, 0.0289, 0.0255, 0.0369, 0.0227, 0.0183, 0.8114],
        [0.0299, 0.0284, 0.0183, 0.0667, 0.0352, 0.0278, 0.0313, 0.7624],
        [0.0144, 0.0125, 0.0217, 0.0121, 0.0252, 0.0144, 0.0108, 0.8890],
        [0.0103, 0.0094, 0.0161, 0.0770, 0.0300, 0.0229, 0.0233, 0.8111]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0433, 0.0387, 0.0350, 0.0227, 0.0344, 0.0422, 0.0333, 0.7504],
        [0.0471, 0.0404, 0.0276, 0.0209, 0.0151, 0.0255, 0.0103, 0.8130],
        [0.0127, 0.0115, 0.0218, 0.0194, 0.0079, 0.0090, 0.0165, 0.9012],
        [0.0081, 0.0072, 0.0140, 0.0118, 0.0095, 0.0126, 0.0186, 0.9181],
        [0.0100, 0.0095, 0.0160, 0.0311, 0.0360, 0.0194, 0.0121, 0.8659]],
       grad_fn=<SoftmaxBackward>)
#####################
08/01 11:26:09 AM | Train: [ 3/200] Step 000/6092 Loss 6.848 Prec@(1,5) (0.0%, 0.0%)
08/01 11:26:25 AM | Train: [ 3/200] Step 010/6092 Loss 6.457 Prec@(1,5) (0.0%, 0.0%)
08/01 11:26:42 AM | Train: [ 3/200] Step 020/6092 Loss 6.708 Prec@(1,5) (0.0%, 0.0%)
08/01 11:26:58 AM | Train: [ 3/200] Step 030/6092 Loss 6.586 Prec@(1,5) (0.0%, 3.2%)
08/01 11:27:15 AM | Train: [ 3/200] Step 040/6092 Loss 6.638 Prec@(1,5) (0.0%, 2.4%)
08/01 11:27:31 AM | Train: [ 3/200] Step 050/6092 Loss 6.597 Prec@(1,5) (1.0%, 3.9%)
08/01 11:27:48 AM | Train: [ 3/200] Step 060/6092 Loss 6.609 Prec@(1,5) (0.8%, 4.1%)
08/01 11:28:04 AM | Train: [ 3/200] Step 070/6092 Loss 6.580 Prec@(1,5) (0.7%, 3.5%)
08/01 11:28:21 AM | Train: [ 3/200] Step 080/6092 Loss 6.604 Prec@(1,5) (0.6%, 3.7%)
08/01 11:28:38 AM | Train: [ 3/200] Step 090/6092 Loss 6.583 Prec@(1,5) (0.5%, 3.8%)
08/01 11:28:54 AM | Train: [ 3/200] Step 100/6092 Loss 6.562 Prec@(1,5) (0.5%, 4.0%)
08/01 11:29:11 AM | Train: [ 3/200] Step 110/6092 Loss 6.555 Prec@(1,5) (0.5%, 4.1%)
08/01 11:29:29 AM | Train: [ 3/200] Step 120/6092 Loss 6.547 Prec@(1,5) (0.4%, 3.7%)
08/01 11:29:45 AM | Train: [ 3/200] Step 130/6092 Loss 6.562 Prec@(1,5) (0.4%, 3.4%)
08/01 11:30:03 AM | Train: [ 3/200] Step 140/6092 Loss 6.556 Prec@(1,5) (0.4%, 3.5%)
08/01 11:30:21 AM | Train: [ 3/200] Step 150/6092 Loss 6.531 Prec@(1,5) (0.3%, 4.0%)
08/01 11:30:38 AM | Train: [ 3/200] Step 160/6092 Loss 6.524 Prec@(1,5) (0.3%, 4.0%)
08/01 11:30:55 AM | Train: [ 3/200] Step 170/6092 Loss 6.516 Prec@(1,5) (0.3%, 4.1%)
08/01 11:31:13 AM | Train: [ 3/200] Step 180/6092 Loss 6.511 Prec@(1,5) (0.6%, 4.1%)
08/01 11:31:29 AM | Train: [ 3/200] Step 190/6092 Loss 6.488 Prec@(1,5) (0.5%, 4.7%)
08/01 11:31:46 AM | Train: [ 3/200] Step 200/6092 Loss 6.486 Prec@(1,5) (0.5%, 4.5%)
08/01 11:32:02 AM | Train: [ 3/200] Step 210/6092 Loss 6.475 Prec@(1,5) (0.7%, 4.5%)
08/01 11:32:19 AM | Train: [ 3/200] Step 220/6092 Loss 6.477 Prec@(1,5) (0.7%, 4.5%)
08/01 11:32:35 AM | Train: [ 3/200] Step 230/6092 Loss 6.476 Prec@(1,5) (0.6%, 4.3%)
08/01 11:32:53 AM | Train: [ 3/200] Step 240/6092 Loss 6.486 Prec@(1,5) (0.6%, 4.1%)
08/01 11:33:09 AM | Train: [ 3/200] Step 250/6092 Loss 6.491 Prec@(1,5) (0.6%, 4.0%)
08/01 11:33:25 AM | Train: [ 3/200] Step 260/6092 Loss 6.489 Prec@(1,5) (0.6%, 3.8%)
08/01 11:33:42 AM | Train: [ 3/200] Step 270/6092 Loss 6.484 Prec@(1,5) (0.7%, 3.9%)
08/01 11:33:59 AM | Train: [ 3/200] Step 280/6092 Loss 6.474 Prec@(1,5) (0.7%, 4.3%)
08/01 11:34:15 AM | Train: [ 3/200] Step 290/6092 Loss 6.479 Prec@(1,5) (0.7%, 4.1%)
08/01 11:34:32 AM | Train: [ 3/200] Step 300/6092 Loss 6.476 Prec@(1,5) (0.7%, 4.3%)
08/01 11:34:48 AM | Train: [ 3/200] Step 310/6092 Loss 6.473 Prec@(1,5) (0.6%, 4.3%)
08/01 11:35:07 AM | Train: [ 3/200] Step 320/6092 Loss 6.475 Prec@(1,5) (0.6%, 4.4%)
08/01 11:35:24 AM | Train: [ 3/200] Step 330/6092 Loss 6.472 Prec@(1,5) (0.9%, 4.5%)
08/01 11:38:19 AM | Logger is set 
08/01 11:38:19 AM | Logger without distribution
08/01 11:39:40 AM | Initializing dataset used 80.55252122879028 basic time unit
08/01 11:39:40 AM | The training classes labels length :  751
08/01 11:43:55 AM | Logger is set 
08/01 11:43:55 AM | Logger without distribution
08/01 11:45:24 AM | Initializing dataset used 88.48949527740479 basic time unit
08/01 11:45:24 AM | The training classes labels length :  751
08/01 11:47:39 AM | batch loading time example is 134.92111587524414
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1248, 0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251],
        [0.1249, 0.1250, 0.1247, 0.1250, 0.1252, 0.1251, 0.1250, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1251],
        [0.1251, 0.1250, 0.1248, 0.1249, 0.1251, 0.1252, 0.1251, 0.1249],
        [0.1252, 0.1250, 0.1251, 0.1251, 0.1250, 0.1246, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1252, 0.1249, 0.1250, 0.1249, 0.1251, 0.1250, 0.1249, 0.1249],
        [0.1248, 0.1249, 0.1251, 0.1252, 0.1250, 0.1248, 0.1250, 0.1252],
        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1251, 0.1250, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1250, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1248, 0.1250, 0.1248, 0.1251, 0.1251, 0.1249, 0.1252],
        [0.1250, 0.1251, 0.1251, 0.1250, 0.1251, 0.1249, 0.1251, 0.1248],
        [0.1248, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251, 0.1252, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1249, 0.1250, 0.1250, 0.1251, 0.1250],
        [0.1251, 0.1251, 0.1250, 0.1249, 0.1250, 0.1252, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1252, 0.1252, 0.1248, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249],
        [0.1251, 0.1251, 0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1251, 0.1249, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250],
        [0.1251, 0.1251, 0.1249, 0.1252, 0.1250, 0.1250, 0.1248, 0.1249],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1248, 0.1251, 0.1251, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1251, 0.1249, 0.1250, 0.1249, 0.1250, 0.1251, 0.1250],
        [0.1251, 0.1249, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],
        [0.1251, 0.1249, 0.1250, 0.1252, 0.1251, 0.1247, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1251, 0.1248, 0.1250, 0.1251, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1251, 0.1249, 0.1251, 0.1248, 0.1251, 0.1252, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1248, 0.1251, 0.1252, 0.1250],
        [0.1251, 0.1251, 0.1250, 0.1252, 0.1250, 0.1248, 0.1250, 0.1248],
        [0.1253, 0.1251, 0.1250, 0.1249, 0.1250, 0.1249, 0.1248, 0.1249],
        [0.1251, 0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
08/01 11:52:20 AM | Train: [ 1/210] Step 000/6092 Loss 6.589 Prec@(1,5) (0.0%, 0.0%)
08/01 11:52:36 AM | Train: [ 1/210] Step 010/6092 Loss 6.646 Prec@(1,5) (0.0%, 0.0%)
08/01 11:52:53 AM | Train: [ 1/210] Step 020/6092 Loss 6.630 Prec@(1,5) (0.0%, 2.4%)
08/01 11:53:09 AM | Train: [ 1/210] Step 030/6092 Loss 6.812 Prec@(1,5) (0.0%, 1.6%)
08/01 11:53:25 AM | Train: [ 1/210] Step 040/6092 Loss 6.814 Prec@(1,5) (0.0%, 1.2%)
08/01 11:53:42 AM | Train: [ 1/210] Step 050/6092 Loss 6.780 Prec@(1,5) (0.0%, 1.0%)
08/01 11:53:58 AM | Train: [ 1/210] Step 060/6092 Loss 6.767 Prec@(1,5) (0.0%, 0.8%)
08/01 11:54:14 AM | Train: [ 1/210] Step 070/6092 Loss 6.769 Prec@(1,5) (0.0%, 0.7%)
08/01 11:54:31 AM | Train: [ 1/210] Step 080/6092 Loss 6.771 Prec@(1,5) (0.0%, 0.6%)
08/01 11:54:47 AM | Train: [ 1/210] Step 090/6092 Loss 6.762 Prec@(1,5) (0.0%, 0.5%)
08/01 11:55:05 AM | Train: [ 1/210] Step 100/6092 Loss 6.766 Prec@(1,5) (0.0%, 0.5%)
08/01 11:55:24 AM | Train: [ 1/210] Step 110/6092 Loss 6.744 Prec@(1,5) (0.5%, 0.9%)
08/01 11:55:40 AM | Train: [ 1/210] Step 120/6092 Loss 6.735 Prec@(1,5) (0.8%, 1.2%)
08/01 11:55:58 AM | Train: [ 1/210] Step 130/6092 Loss 6.737 Prec@(1,5) (0.8%, 1.1%)
08/01 11:56:15 AM | Train: [ 1/210] Step 140/6092 Loss 6.724 Prec@(1,5) (0.7%, 1.4%)
08/01 11:56:32 AM | Train: [ 1/210] Step 150/6092 Loss 6.734 Prec@(1,5) (0.7%, 1.3%)
08/01 11:56:49 AM | Train: [ 1/210] Step 160/6092 Loss 6.732 Prec@(1,5) (0.6%, 1.6%)
08/01 11:57:07 AM | Train: [ 1/210] Step 170/6092 Loss 6.730 Prec@(1,5) (0.6%, 1.5%)
08/01 11:57:24 AM | Train: [ 1/210] Step 180/6092 Loss 6.722 Prec@(1,5) (0.6%, 1.7%)
08/01 11:57:42 AM | Train: [ 1/210] Step 190/6092 Loss 6.710 Prec@(1,5) (0.5%, 1.8%)
08/01 11:57:59 AM | Train: [ 1/210] Step 200/6092 Loss 6.705 Prec@(1,5) (0.5%, 1.7%)
08/01 11:58:16 AM | Train: [ 1/210] Step 210/6092 Loss 6.710 Prec@(1,5) (0.5%, 1.7%)
08/01 11:58:34 AM | Train: [ 1/210] Step 220/6092 Loss 6.710 Prec@(1,5) (0.5%, 1.6%)
08/01 11:58:51 AM | Train: [ 1/210] Step 230/6092 Loss 6.709 Prec@(1,5) (0.6%, 1.7%)
08/01 11:59:09 AM | Train: [ 1/210] Step 240/6092 Loss 6.708 Prec@(1,5) (0.6%, 1.7%)
08/01 11:59:26 AM | Train: [ 1/210] Step 250/6092 Loss 6.705 Prec@(1,5) (0.6%, 1.6%)
08/01 11:59:44 AM | Train: [ 1/210] Step 260/6092 Loss 6.710 Prec@(1,5) (0.6%, 1.5%)
08/01 12:00:01 PM | Train: [ 1/210] Step 270/6092 Loss 6.707 Prec@(1,5) (0.6%, 1.5%)
08/01 12:00:19 PM | Train: [ 1/210] Step 280/6092 Loss 6.709 Prec@(1,5) (0.5%, 1.4%)
08/01 12:00:38 PM | Train: [ 1/210] Step 290/6092 Loss 6.707 Prec@(1,5) (0.5%, 1.4%)
08/01 12:00:55 PM | Train: [ 1/210] Step 300/6092 Loss 6.705 Prec@(1,5) (0.5%, 1.3%)
08/01 12:01:12 PM | Train: [ 1/210] Step 310/6092 Loss 6.696 Prec@(1,5) (0.5%, 1.3%)
08/01 12:01:30 PM | Train: [ 1/210] Step 320/6092 Loss 6.694 Prec@(1,5) (0.5%, 1.2%)
08/01 12:01:47 PM | Train: [ 1/210] Step 330/6092 Loss 6.688 Prec@(1,5) (0.6%, 1.4%)
08/01 12:02:05 PM | Train: [ 1/210] Step 340/6092 Loss 6.688 Prec@(1,5) (0.6%, 1.3%)
08/01 12:02:22 PM | Train: [ 1/210] Step 350/6092 Loss 6.684 Prec@(1,5) (0.6%, 1.3%)
08/01 12:02:40 PM | Train: [ 1/210] Step 360/6092 Loss 6.683 Prec@(1,5) (0.6%, 1.2%)
08/01 12:02:57 PM | Train: [ 1/210] Step 370/6092 Loss 6.677 Prec@(1,5) (0.5%, 1.2%)
08/01 12:03:11 PM | Train: [ 1/210] Final Prec@1 0.6649%
08/01 12:05:24 PM | Valid: [ 1/210] Step 000/375 Loss 5.956 Prec@(1,5) (0.0%, 0.0%)
08/01 12:05:25 PM | Valid: [ 1/210] Step 010/375 Loss 6.438 Prec@(1,5) (4.5%, 4.5%)
08/01 12:05:26 PM | Valid: [ 1/210] Step 020/375 Loss 6.665 Prec@(1,5) (2.4%, 2.4%)
08/01 12:05:27 PM | Valid: [ 1/210] Step 030/375 Loss 6.788 Prec@(1,5) (1.6%, 1.6%)
08/01 12:05:28 PM | Valid: [ 1/210] Step 040/375 Loss 6.807 Prec@(1,5) (1.2%, 1.2%)
08/01 12:05:28 PM | Valid: [ 1/210] Step 050/375 Loss 6.790 Prec@(1,5) (1.0%, 1.0%)
08/01 12:05:29 PM | Valid: [ 1/210] Step 060/375 Loss 6.753 Prec@(1,5) (0.8%, 1.6%)
08/01 12:05:30 PM | Valid: [ 1/210] Step 070/375 Loss 6.745 Prec@(1,5) (0.7%, 1.4%)
08/01 12:05:31 PM | Valid: [ 1/210] Step 080/375 Loss 6.750 Prec@(1,5) (0.6%, 1.2%)
08/01 12:05:32 PM | Valid: [ 1/210] Step 090/375 Loss 6.738 Prec@(1,5) (0.5%, 1.1%)
08/01 12:05:33 PM | Valid: [ 1/210] Step 100/375 Loss 6.749 Prec@(1,5) (0.5%, 1.5%)
08/01 12:05:33 PM | Valid: [ 1/210] Step 110/375 Loss 6.755 Prec@(1,5) (0.5%, 1.4%)
08/01 12:05:34 PM | Valid: [ 1/210] Step 120/375 Loss 6.761 Prec@(1,5) (0.4%, 1.2%)
08/01 12:05:35 PM | Valid: [ 1/210] Step 130/375 Loss 6.770 Prec@(1,5) (0.4%, 1.1%)
08/01 12:05:36 PM | Valid: [ 1/210] Step 140/375 Loss 6.761 Prec@(1,5) (0.4%, 1.4%)
08/01 12:05:37 PM | Valid: [ 1/210] Step 150/375 Loss 6.775 Prec@(1,5) (0.3%, 1.7%)
08/01 12:05:38 PM | Valid: [ 1/210] Step 160/375 Loss 6.770 Prec@(1,5) (0.3%, 1.6%)
08/01 12:05:39 PM | Valid: [ 1/210] Step 170/375 Loss 6.781 Prec@(1,5) (0.3%, 1.5%)
08/01 12:05:39 PM | Valid: [ 1/210] Step 180/375 Loss 6.747 Prec@(1,5) (0.3%, 1.7%)
08/01 12:05:40 PM | Valid: [ 1/210] Step 190/375 Loss 6.760 Prec@(1,5) (0.3%, 1.6%)
08/01 12:05:41 PM | Valid: [ 1/210] Step 200/375 Loss 6.749 Prec@(1,5) (0.2%, 1.7%)
08/01 12:05:42 PM | Valid: [ 1/210] Step 210/375 Loss 6.754 Prec@(1,5) (0.2%, 1.7%)
08/01 12:05:43 PM | Valid: [ 1/210] Step 220/375 Loss 6.747 Prec@(1,5) (0.2%, 1.6%)
08/01 12:05:44 PM | Valid: [ 1/210] Step 230/375 Loss 6.743 Prec@(1,5) (0.2%, 1.5%)
08/01 12:05:45 PM | Valid: [ 1/210] Step 240/375 Loss 6.740 Prec@(1,5) (0.2%, 1.5%)
08/01 12:05:45 PM | Valid: [ 1/210] Step 250/375 Loss 6.746 Prec@(1,5) (0.2%, 1.4%)
08/01 12:05:46 PM | Valid: [ 1/210] Step 260/375 Loss 6.732 Prec@(1,5) (0.2%, 1.5%)
08/01 12:05:47 PM | Valid: [ 1/210] Step 270/375 Loss 6.732 Prec@(1,5) (0.2%, 1.5%)
08/01 12:05:48 PM | Valid: [ 1/210] Step 280/375 Loss 6.726 Prec@(1,5) (0.2%, 1.8%)
08/01 12:05:49 PM | Valid: [ 1/210] Step 290/375 Loss 6.735 Prec@(1,5) (0.2%, 1.7%)
08/01 12:05:50 PM | Valid: [ 1/210] Step 300/375 Loss 6.736 Prec@(1,5) (0.2%, 1.7%)
08/01 12:05:50 PM | Valid: [ 1/210] Step 310/375 Loss 6.740 Prec@(1,5) (0.2%, 1.6%)
08/01 12:05:51 PM | Valid: [ 1/210] Step 320/375 Loss 6.746 Prec@(1,5) (0.2%, 1.6%)
08/01 12:05:52 PM | Valid: [ 1/210] Step 330/375 Loss 6.748 Prec@(1,5) (0.2%, 1.5%)
08/01 12:05:53 PM | Valid: [ 1/210] Step 340/375 Loss 6.746 Prec@(1,5) (0.1%, 1.5%)
08/01 12:05:54 PM | Valid: [ 1/210] Step 350/375 Loss 6.751 Prec@(1,5) (0.1%, 1.4%)
08/01 12:05:55 PM | Valid: [ 1/210] Step 360/375 Loss 6.749 Prec@(1,5) (0.1%, 1.4%)
08/01 12:05:55 PM | Valid: [ 1/210] Step 370/375 Loss 6.752 Prec@(1,5) (0.1%, 1.3%)
08/01 12:05:57 PM | Valid: [ 1/210] Final Prec@1 0.1333%, Prec@5 1.3333%, Prec@10 2.5333%
08/01 12:05:58 PM | Final best Prec@1 = 0.1333%
####### ALPHA #######
# Alpha - normal
tensor([[0.1181, 0.1085, 0.1116, 0.0728, 0.1282, 0.2251, 0.0629, 0.1729],
        [0.1914, 0.1640, 0.1705, 0.1187, 0.1255, 0.0579, 0.0719, 0.1001]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1391, 0.1185, 0.1247, 0.1009, 0.0778, 0.0994, 0.1405, 0.1990],
        [0.2030, 0.1852, 0.1964, 0.1236, 0.0610, 0.0546, 0.0759, 0.1003],
        [0.1068, 0.0972, 0.0958, 0.1879, 0.2125, 0.0861, 0.0971, 0.1166]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1158, 0.1097, 0.1129, 0.0901, 0.1030, 0.1915, 0.1371, 0.1399],
        [0.1833, 0.1743, 0.1813, 0.0900, 0.1023, 0.0814, 0.1016, 0.0857],
        [0.1454, 0.1212, 0.1164, 0.2230, 0.0883, 0.0834, 0.1268, 0.0955],
        [0.1606, 0.1377, 0.1190, 0.0717, 0.1813, 0.0831, 0.0921, 0.1545]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1153, 0.1030, 0.1067, 0.1191, 0.1223, 0.2380, 0.0820, 0.1137],
        [0.1941, 0.1659, 0.1718, 0.0763, 0.0891, 0.1425, 0.0798, 0.0806],
        [0.1275, 0.1115, 0.1069, 0.1165, 0.1134, 0.2204, 0.0865, 0.1173],
        [0.1297, 0.1156, 0.0967, 0.1156, 0.2123, 0.1314, 0.0628, 0.1359],
        [0.1341, 0.1246, 0.0911, 0.1881, 0.0728, 0.1815, 0.0998, 0.1080]],
       grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.0519, 0.0490, 0.0590, 0.0833, 0.0660, 0.0462, 0.0731, 0.5715],
        [0.0498, 0.0453, 0.0634, 0.0470, 0.1676, 0.0403, 0.0505, 0.5360]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0518, 0.0495, 0.0779, 0.0692, 0.0620, 0.0444, 0.0521, 0.5932],
        [0.0395, 0.0377, 0.0650, 0.0466, 0.0792, 0.0983, 0.0554, 0.5784],
        [0.0191, 0.0189, 0.0247, 0.0822, 0.0493, 0.0574, 0.0441, 0.7042]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0639, 0.0646, 0.0641, 0.0714, 0.0474, 0.0860, 0.0481, 0.5544],
        [0.0329, 0.0344, 0.0509, 0.0336, 0.0371, 0.0264, 0.0325, 0.7522],
        [0.0195, 0.0196, 0.0304, 0.1120, 0.0844, 0.0688, 0.0345, 0.6308],
        [0.0138, 0.0135, 0.0204, 0.0847, 0.0669, 0.0474, 0.0339, 0.7196]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0809, 0.0725, 0.0616, 0.0677, 0.0997, 0.0605, 0.0517, 0.5053],
        [0.0317, 0.0311, 0.0357, 0.0330, 0.0499, 0.0394, 0.0353, 0.7439],
        [0.0174, 0.0174, 0.0246, 0.0347, 0.0513, 0.0476, 0.0248, 0.7821],
        [0.0170, 0.0152, 0.0246, 0.0473, 0.0973, 0.0466, 0.0924, 0.6598],
        [0.0244, 0.0242, 0.0299, 0.0407, 0.0542, 0.0750, 0.0280, 0.7236]],
       grad_fn=<SoftmaxBackward>)
#####################
08/01 12:11:02 PM | Train: [ 2/210] Step 000/6092 Loss 6.918 Prec@(1,5) (0.0%, 0.0%)
08/01 12:11:18 PM | Train: [ 2/210] Step 010/6092 Loss 6.515 Prec@(1,5) (0.0%, 0.0%)
08/01 12:11:34 PM | Train: [ 2/210] Step 020/6092 Loss 6.673 Prec@(1,5) (0.0%, 2.4%)
08/01 12:11:51 PM | Train: [ 2/210] Step 030/6092 Loss 6.611 Prec@(1,5) (0.0%, 1.6%)
08/01 12:12:07 PM | Train: [ 2/210] Step 040/6092 Loss 6.576 Prec@(1,5) (0.0%, 2.4%)
08/01 12:12:24 PM | Train: [ 2/210] Step 050/6092 Loss 6.598 Prec@(1,5) (0.0%, 2.0%)
08/01 12:12:40 PM | Train: [ 2/210] Step 060/6092 Loss 6.561 Prec@(1,5) (0.0%, 2.5%)
08/01 12:12:57 PM | Train: [ 2/210] Step 070/6092 Loss 6.587 Prec@(1,5) (0.0%, 2.1%)
08/01 12:13:14 PM | Train: [ 2/210] Step 080/6092 Loss 6.595 Prec@(1,5) (0.0%, 1.9%)
08/01 12:13:31 PM | Train: [ 2/210] Step 090/6092 Loss 6.581 Prec@(1,5) (0.0%, 2.2%)
08/01 12:13:48 PM | Train: [ 2/210] Step 100/6092 Loss 6.577 Prec@(1,5) (0.5%, 2.5%)
08/01 12:14:05 PM | Train: [ 2/210] Step 110/6092 Loss 6.582 Prec@(1,5) (0.5%, 2.3%)
08/01 12:14:22 PM | Train: [ 2/210] Step 120/6092 Loss 6.563 Prec@(1,5) (0.4%, 2.5%)
08/01 12:14:40 PM | Train: [ 2/210] Step 130/6092 Loss 6.550 Prec@(1,5) (0.4%, 2.7%)
08/01 12:14:57 PM | Train: [ 2/210] Step 140/6092 Loss 6.547 Prec@(1,5) (0.4%, 2.5%)
08/01 12:15:14 PM | Train: [ 2/210] Step 150/6092 Loss 6.539 Prec@(1,5) (0.7%, 3.0%)
08/01 12:15:33 PM | Train: [ 2/210] Step 160/6092 Loss 6.543 Prec@(1,5) (0.6%, 2.8%)
08/01 12:15:50 PM | Train: [ 2/210] Step 170/6092 Loss 6.543 Prec@(1,5) (0.6%, 2.6%)
08/01 12:16:08 PM | Train: [ 2/210] Step 180/6092 Loss 6.546 Prec@(1,5) (0.8%, 2.8%)
08/01 12:16:25 PM | Train: [ 2/210] Step 190/6092 Loss 6.541 Prec@(1,5) (0.8%, 2.6%)
08/01 12:16:42 PM | Train: [ 2/210] Step 200/6092 Loss 6.527 Prec@(1,5) (0.7%, 2.7%)
08/01 12:16:59 PM | Train: [ 2/210] Step 210/6092 Loss 6.531 Prec@(1,5) (0.7%, 2.6%)
08/01 12:17:16 PM | Train: [ 2/210] Step 220/6092 Loss 6.534 Prec@(1,5) (0.7%, 2.5%)
08/01 12:17:34 PM | Train: [ 2/210] Step 230/6092 Loss 6.536 Prec@(1,5) (0.6%, 2.4%)
08/01 12:17:51 PM | Train: [ 2/210] Step 240/6092 Loss 6.538 Prec@(1,5) (0.6%, 2.3%)
08/01 12:18:09 PM | Train: [ 2/210] Step 250/6092 Loss 6.534 Prec@(1,5) (0.8%, 2.4%)
08/01 12:18:26 PM | Train: [ 2/210] Step 260/6092 Loss 6.537 Prec@(1,5) (0.8%, 2.3%)
08/01 12:18:44 PM | Train: [ 2/210] Step 270/6092 Loss 6.533 Prec@(1,5) (0.7%, 2.2%)
08/01 12:19:01 PM | Train: [ 2/210] Step 280/6092 Loss 6.520 Prec@(1,5) (0.7%, 2.3%)
08/01 12:19:19 PM | Train: [ 2/210] Step 290/6092 Loss 6.521 Prec@(1,5) (0.7%, 2.2%)
08/01 12:19:36 PM | Train: [ 2/210] Step 300/6092 Loss 6.523 Prec@(1,5) (0.7%, 2.3%)
08/01 12:19:53 PM | Train: [ 2/210] Step 310/6092 Loss 6.521 Prec@(1,5) (0.8%, 2.4%)
08/01 12:20:12 PM | Train: [ 2/210] Step 320/6092 Loss 6.524 Prec@(1,5) (0.8%, 2.3%)
08/01 12:20:30 PM | Train: [ 2/210] Step 330/6092 Loss 6.524 Prec@(1,5) (0.8%, 2.3%)
08/01 12:20:48 PM | Train: [ 2/210] Step 340/6092 Loss 6.526 Prec@(1,5) (0.7%, 2.3%)
08/01 12:21:05 PM | Train: [ 2/210] Step 350/6092 Loss 6.519 Prec@(1,5) (0.7%, 2.3%)
08/01 12:21:23 PM | Train: [ 2/210] Step 360/6092 Loss 6.521 Prec@(1,5) (0.7%, 2.2%)
08/01 12:21:40 PM | Train: [ 2/210] Step 370/6092 Loss 6.522 Prec@(1,5) (0.7%, 2.2%)
08/01 12:21:58 PM | Train: [ 2/210] Step 380/6092 Loss 6.520 Prec@(1,5) (0.7%, 2.1%)
08/01 12:22:15 PM | Train: [ 2/210] Step 390/6092 Loss 6.515 Prec@(1,5) (0.6%, 2.2%)
08/01 12:22:33 PM | Train: [ 2/210] Step 400/6092 Loss 6.507 Prec@(1,5) (0.6%, 2.5%)
08/01 12:22:50 PM | Train: [ 2/210] Step 410/6092 Loss 6.510 Prec@(1,5) (0.6%, 2.4%)
08/01 12:23:08 PM | Train: [ 2/210] Step 420/6092 Loss 6.498 Prec@(1,5) (0.8%, 2.6%)
08/01 12:23:25 PM | Train: [ 2/210] Step 430/6092 Loss 6.501 Prec@(1,5) (0.8%, 2.6%)
08/01 12:23:42 PM | Train: [ 2/210] Step 440/6092 Loss 6.491 Prec@(1,5) (0.9%, 2.8%)
08/01 12:23:59 PM | Train: [ 2/210] Step 450/6092 Loss 6.492 Prec@(1,5) (0.9%, 2.8%)
08/01 12:24:17 PM | Train: [ 2/210] Step 460/6092 Loss 6.483 Prec@(1,5) (0.9%, 2.8%)
08/01 12:24:34 PM | Train: [ 2/210] Step 470/6092 Loss 6.490 Prec@(1,5) (0.8%, 2.8%)
08/01 12:24:52 PM | Train: [ 2/210] Step 480/6092 Loss 6.495 Prec@(1,5) (0.8%, 2.7%)
08/01 12:25:11 PM | Train: [ 2/210] Step 490/6092 Loss 6.494 Prec@(1,5) (0.8%, 2.7%)
08/01 12:25:29 PM | Train: [ 2/210] Step 500/6092 Loss 6.494 Prec@(1,5) (0.8%, 2.7%)
08/01 12:25:47 PM | Train: [ 2/210] Step 510/6092 Loss 6.495 Prec@(1,5) (0.8%, 2.7%)
08/01 12:26:04 PM | Train: [ 2/210] Step 520/6092 Loss 6.494 Prec@(1,5) (0.8%, 2.7%)
08/01 12:26:23 PM | Train: [ 2/210] Step 530/6092 Loss 6.499 Prec@(1,5) (0.8%, 2.6%)
08/01 12:26:40 PM | Train: [ 2/210] Step 540/6092 Loss 6.500 Prec@(1,5) (0.7%, 2.6%)
08/01 12:26:57 PM | Train: [ 2/210] Step 550/6092 Loss 6.498 Prec@(1,5) (0.7%, 2.6%)
08/01 12:27:14 PM | Train: [ 2/210] Step 560/6092 Loss 6.500 Prec@(1,5) (0.7%, 2.6%)
08/01 12:27:32 PM | Train: [ 2/210] Step 570/6092 Loss 6.502 Prec@(1,5) (0.7%, 2.6%)
08/01 12:27:49 PM | Train: [ 2/210] Step 580/6092 Loss 6.504 Prec@(1,5) (0.8%, 2.7%)
08/01 12:28:07 PM | Train: [ 2/210] Step 590/6092 Loss 6.502 Prec@(1,5) (0.8%, 2.6%)
08/01 12:28:24 PM | Train: [ 2/210] Step 600/6092 Loss 6.507 Prec@(1,5) (0.7%, 2.6%)
08/01 12:28:42 PM | Train: [ 2/210] Step 610/6092 Loss 6.506 Prec@(1,5) (0.7%, 2.6%)
08/01 12:28:59 PM | Train: [ 2/210] Step 620/6092 Loss 6.512 Prec@(1,5) (0.7%, 2.6%)
08/01 12:29:16 PM | Train: [ 2/210] Step 630/6092 Loss 6.513 Prec@(1,5) (0.7%, 2.6%)
08/01 12:29:34 PM | Train: [ 2/210] Step 640/6092 Loss 6.514 Prec@(1,5) (0.7%, 2.6%)
08/01 12:29:52 PM | Train: [ 2/210] Step 650/6092 Loss 6.519 Prec@(1,5) (0.7%, 2.5%)
08/01 12:30:11 PM | Train: [ 2/210] Step 660/6092 Loss 6.519 Prec@(1,5) (0.8%, 2.7%)
08/01 12:30:28 PM | Train: [ 2/210] Step 670/6092 Loss 6.519 Prec@(1,5) (0.7%, 2.7%)
08/01 12:30:45 PM | Train: [ 2/210] Step 680/6092 Loss 6.521 Prec@(1,5) (0.7%, 2.6%)
08/01 12:31:03 PM | Train: [ 2/210] Step 690/6092 Loss 6.521 Prec@(1,5) (0.7%, 2.6%)
08/01 12:31:20 PM | Train: [ 2/210] Step 700/6092 Loss 6.520 Prec@(1,5) (0.8%, 2.6%)
08/01 12:31:38 PM | Train: [ 2/210] Step 710/6092 Loss 6.521 Prec@(1,5) (0.8%, 2.6%)
08/01 12:31:55 PM | Train: [ 2/210] Step 720/6092 Loss 6.524 Prec@(1,5) (0.8%, 2.6%)
08/01 12:32:13 PM | Train: [ 2/210] Step 730/6092 Loss 6.525 Prec@(1,5) (0.8%, 2.6%)
08/01 12:32:30 PM | Train: [ 2/210] Step 740/6092 Loss 6.526 Prec@(1,5) (0.8%, 2.6%)
08/01 12:32:47 PM | Train: [ 2/210] Step 750/6092 Loss 6.525 Prec@(1,5) (0.8%, 2.5%)
08/01 12:33:04 PM | Train: [ 2/210] Step 760/6092 Loss 6.525 Prec@(1,5) (0.8%, 2.5%)
08/01 12:33:22 PM | Train: [ 2/210] Step 770/6092 Loss 6.521 Prec@(1,5) (0.8%, 2.5%)
08/01 12:33:40 PM | Train: [ 2/210] Step 780/6092 Loss 6.526 Prec@(1,5) (0.8%, 2.5%)
08/01 12:33:57 PM | Train: [ 2/210] Step 790/6092 Loss 6.525 Prec@(1,5) (0.8%, 2.5%)
08/01 12:34:15 PM | Train: [ 2/210] Step 800/6092 Loss 6.527 Prec@(1,5) (0.8%, 2.4%)
08/01 12:34:32 PM | Train: [ 2/210] Step 810/6092 Loss 6.528 Prec@(1,5) (0.8%, 2.4%)
08/01 12:34:50 PM | Train: [ 2/210] Step 820/6092 Loss 6.527 Prec@(1,5) (0.8%, 2.4%)
08/01 12:35:09 PM | Train: [ 2/210] Step 830/6092 Loss 6.524 Prec@(1,5) (0.8%, 2.5%)
08/01 12:35:26 PM | Train: [ 2/210] Step 840/6092 Loss 6.524 Prec@(1,5) (0.8%, 2.4%)
08/01 12:35:44 PM | Train: [ 2/210] Step 850/6092 Loss 6.519 Prec@(1,5) (0.8%, 2.6%)
08/01 12:36:01 PM | Train: [ 2/210] Step 860/6092 Loss 6.517 Prec@(1,5) (0.8%, 2.6%)
08/01 12:36:19 PM | Train: [ 2/210] Step 870/6092 Loss 6.517 Prec@(1,5) (0.8%, 2.6%)
08/01 12:36:36 PM | Train: [ 2/210] Step 880/6092 Loss 6.516 Prec@(1,5) (0.8%, 2.6%)
08/01 12:36:54 PM | Train: [ 2/210] Step 890/6092 Loss 6.515 Prec@(1,5) (0.8%, 2.6%)
08/01 12:37:11 PM | Train: [ 2/210] Step 900/6092 Loss 6.510 Prec@(1,5) (0.9%, 2.7%)
08/01 12:37:29 PM | Train: [ 2/210] Step 910/6092 Loss 6.506 Prec@(1,5) (0.9%, 2.7%)
08/01 12:37:46 PM | Train: [ 2/210] Step 920/6092 Loss 6.504 Prec@(1,5) (0.9%, 2.8%)
08/01 12:38:04 PM | Train: [ 2/210] Step 930/6092 Loss 6.503 Prec@(1,5) (1.0%, 2.8%)
08/01 12:38:21 PM | Train: [ 2/210] Step 940/6092 Loss 6.504 Prec@(1,5) (1.0%, 2.9%)
08/01 12:38:38 PM | Train: [ 2/210] Step 950/6092 Loss 6.507 Prec@(1,5) (0.9%, 2.8%)
08/01 12:38:56 PM | Train: [ 2/210] Step 960/6092 Loss 6.509 Prec@(1,5) (0.9%, 2.8%)
08/01 12:39:15 PM | Train: [ 2/210] Step 970/6092 Loss 6.505 Prec@(1,5) (1.0%, 2.9%)
08/01 12:39:32 PM | Train: [ 2/210] Step 980/6092 Loss 6.501 Prec@(1,5) (1.0%, 3.0%)
08/01 12:39:49 PM | Train: [ 2/210] Step 990/6092 Loss 6.499 Prec@(1,5) (1.0%, 3.0%)
08/01 12:40:09 PM | Train: [ 2/210] Step 1000/6092 Loss 6.499 Prec@(1,5) (0.9%, 3.1%)
08/01 12:40:26 PM | Train: [ 2/210] Step 1010/6092 Loss 6.497 Prec@(1,5) (0.9%, 3.1%)
08/01 12:40:44 PM | Train: [ 2/210] Step 1020/6092 Loss 6.499 Prec@(1,5) (0.9%, 3.1%)
08/01 12:41:01 PM | Train: [ 2/210] Step 1030/6092 Loss 6.499 Prec@(1,5) (0.9%, 3.1%)
08/01 12:41:19 PM | Train: [ 2/210] Step 1040/6092 Loss 6.494 Prec@(1,5) (0.9%, 3.2%)
08/01 12:41:36 PM | Train: [ 2/210] Step 1050/6092 Loss 6.495 Prec@(1,5) (0.9%, 3.1%)
08/01 12:41:54 PM | Train: [ 2/210] Step 1060/6092 Loss 6.495 Prec@(1,5) (0.9%, 3.1%)
08/01 12:42:11 PM | Train: [ 2/210] Step 1070/6092 Loss 6.495 Prec@(1,5) (0.9%, 3.1%)
08/01 12:42:29 PM | Train: [ 2/210] Step 1080/6092 Loss 6.494 Prec@(1,5) (0.9%, 3.1%)
08/01 12:42:46 PM | Train: [ 2/210] Step 1090/6092 Loss 6.493 Prec@(1,5) (0.9%, 3.1%)
08/01 12:43:03 PM | Train: [ 2/210] Step 1100/6092 Loss 6.494 Prec@(1,5) (0.9%, 3.0%)
08/01 12:43:20 PM | Train: [ 2/210] Step 1110/6092 Loss 6.495 Prec@(1,5) (0.9%, 3.0%)
08/01 12:43:37 PM | Train: [ 2/210] Step 1120/6092 Loss 6.495 Prec@(1,5) (0.8%, 3.0%)
08/01 12:43:55 PM | Train: [ 2/210] Step 1130/6092 Loss 6.496 Prec@(1,5) (0.8%, 3.0%)
08/01 12:44:12 PM | Train: [ 2/210] Step 1140/6092 Loss 6.494 Prec@(1,5) (0.9%, 3.0%)
08/01 12:44:30 PM | Train: [ 2/210] Step 1150/6092 Loss 6.494 Prec@(1,5) (1.0%, 3.0%)
08/01 12:44:47 PM | Train: [ 2/210] Step 1160/6092 Loss 6.491 Prec@(1,5) (0.9%, 3.1%)
08/01 12:45:06 PM | Train: [ 2/210] Step 1170/6092 Loss 6.490 Prec@(1,5) (1.0%, 3.1%)
08/01 12:45:23 PM | Train: [ 2/210] Step 1180/6092 Loss 6.486 Prec@(1,5) (1.0%, 3.0%)
08/01 12:45:40 PM | Train: [ 2/210] Step 1190/6092 Loss 6.487 Prec@(1,5) (1.0%, 3.0%)
08/01 12:45:58 PM | Train: [ 2/210] Step 1200/6092 Loss 6.484 Prec@(1,5) (1.0%, 3.1%)
08/01 12:46:15 PM | Train: [ 2/210] Step 1210/6092 Loss 6.484 Prec@(1,5) (0.9%, 3.1%)
08/01 12:46:33 PM | Train: [ 2/210] Step 1220/6092 Loss 6.485 Prec@(1,5) (0.9%, 3.0%)
08/01 12:46:51 PM | Train: [ 2/210] Step 1230/6092 Loss 6.483 Prec@(1,5) (0.9%, 3.0%)
08/01 12:47:08 PM | Train: [ 2/210] Step 1240/6092 Loss 6.482 Prec@(1,5) (0.9%, 3.1%)
08/01 12:47:26 PM | Train: [ 2/210] Step 1250/6092 Loss 6.482 Prec@(1,5) (1.0%, 3.2%)
08/01 12:47:43 PM | Train: [ 2/210] Step 1260/6092 Loss 6.478 Prec@(1,5) (1.0%, 3.2%)
08/01 12:48:00 PM | Train: [ 2/210] Step 1270/6092 Loss 6.474 Prec@(1,5) (0.9%, 3.2%)
08/01 12:48:18 PM | Train: [ 2/210] Step 1280/6092 Loss 6.473 Prec@(1,5) (0.9%, 3.2%)
08/01 12:48:35 PM | Train: [ 2/210] Step 1290/6092 Loss 6.472 Prec@(1,5) (0.9%, 3.2%)
08/01 12:48:53 PM | Train: [ 2/210] Step 1300/6092 Loss 6.469 Prec@(1,5) (0.9%, 3.2%)
08/01 12:49:10 PM | Train: [ 2/210] Step 1310/6092 Loss 6.471 Prec@(1,5) (0.9%, 3.2%)
08/01 12:49:28 PM | Train: [ 2/210] Step 1320/6092 Loss 6.471 Prec@(1,5) (0.9%, 3.2%)
08/01 12:49:45 PM | Train: [ 2/210] Step 1330/6092 Loss 6.470 Prec@(1,5) (0.9%, 3.2%)
08/01 12:50:03 PM | Train: [ 2/210] Step 1340/6092 Loss 6.470 Prec@(1,5) (0.9%, 3.2%)
08/01 12:50:22 PM | Train: [ 2/210] Step 1350/6092 Loss 6.469 Prec@(1,5) (0.9%, 3.1%)
08/01 12:50:39 PM | Train: [ 2/210] Step 1360/6092 Loss 6.467 Prec@(1,5) (0.9%, 3.1%)
08/01 12:50:56 PM | Train: [ 2/210] Step 1370/6092 Loss 6.467 Prec@(1,5) (0.9%, 3.1%)
08/01 12:51:14 PM | Train: [ 2/210] Step 1380/6092 Loss 6.469 Prec@(1,5) (0.9%, 3.1%)
08/01 12:51:31 PM | Train: [ 2/210] Step 1390/6092 Loss 6.469 Prec@(1,5) (0.9%, 3.1%)
08/01 12:51:49 PM | Train: [ 2/210] Step 1400/6092 Loss 6.471 Prec@(1,5) (0.9%, 3.1%)
08/01 12:52:06 PM | Train: [ 2/210] Step 1410/6092 Loss 6.469 Prec@(1,5) (0.9%, 3.1%)
08/01 12:52:24 PM | Train: [ 2/210] Step 1420/6092 Loss 6.467 Prec@(1,5) (0.9%, 3.1%)
08/01 12:52:41 PM | Train: [ 2/210] Step 1430/6092 Loss 6.469 Prec@(1,5) (0.9%, 3.1%)
08/01 12:52:59 PM | Train: [ 2/210] Step 1440/6092 Loss 6.469 Prec@(1,5) (0.9%, 3.1%)
08/01 12:53:16 PM | Train: [ 2/210] Step 1450/6092 Loss 6.467 Prec@(1,5) (0.9%, 3.1%)
08/01 12:53:34 PM | Train: [ 2/210] Step 1460/6092 Loss 6.468 Prec@(1,5) (0.9%, 3.1%)
08/01 12:53:51 PM | Train: [ 2/210] Step 1470/6092 Loss 6.466 Prec@(1,5) (0.8%, 3.1%)
08/01 12:54:09 PM | Train: [ 2/210] Step 1480/6092 Loss 6.467 Prec@(1,5) (0.8%, 3.1%)
08/01 12:54:27 PM | Train: [ 2/210] Step 1490/6092 Loss 6.462 Prec@(1,5) (0.8%, 3.2%)
08/01 12:54:45 PM | Train: [ 2/210] Step 1500/6092 Loss 6.463 Prec@(1,5) (0.8%, 3.1%)
08/01 12:55:02 PM | Train: [ 2/210] Step 1510/6092 Loss 6.461 Prec@(1,5) (0.8%, 3.1%)
08/01 12:55:20 PM | Train: [ 2/210] Step 1520/6092 Loss 6.460 Prec@(1,5) (0.8%, 3.1%)
08/01 12:55:37 PM | Train: [ 2/210] Step 1530/6092 Loss 6.458 Prec@(1,5) (0.8%, 3.1%)
08/01 12:55:57 PM | Train: [ 2/210] Step 1540/6092 Loss 6.458 Prec@(1,5) (0.8%, 3.2%)
08/01 12:56:15 PM | Train: [ 2/210] Step 1550/6092 Loss 6.458 Prec@(1,5) (0.9%, 3.2%)
08/01 12:56:33 PM | Train: [ 2/210] Step 1560/6092 Loss 6.458 Prec@(1,5) (0.9%, 3.2%)
08/01 12:56:50 PM | Train: [ 2/210] Step 1570/6092 Loss 6.456 Prec@(1,5) (0.9%, 3.2%)
08/01 12:57:08 PM | Train: [ 2/210] Step 1580/6092 Loss 6.454 Prec@(1,5) (0.9%, 3.2%)
08/01 12:57:25 PM | Train: [ 2/210] Step 1590/6092 Loss 6.451 Prec@(1,5) (0.9%, 3.3%)
08/01 12:57:42 PM | Train: [ 2/210] Step 1600/6092 Loss 6.450 Prec@(1,5) (0.9%, 3.3%)
08/01 12:58:00 PM | Train: [ 2/210] Step 1610/6092 Loss 6.451 Prec@(1,5) (0.9%, 3.3%)
08/01 12:58:17 PM | Train: [ 2/210] Step 1620/6092 Loss 6.451 Prec@(1,5) (1.0%, 3.3%)
08/01 12:58:35 PM | Train: [ 2/210] Step 1630/6092 Loss 6.451 Prec@(1,5) (1.0%, 3.3%)
08/01 12:58:52 PM | Train: [ 2/210] Step 1640/6092 Loss 6.451 Prec@(1,5) (1.0%, 3.3%)
08/01 12:59:10 PM | Train: [ 2/210] Step 1650/6092 Loss 6.451 Prec@(1,5) (1.0%, 3.3%)
08/01 12:59:27 PM | Train: [ 2/210] Step 1660/6092 Loss 6.450 Prec@(1,5) (1.0%, 3.3%)
08/01 12:59:45 PM | Train: [ 2/210] Step 1670/6092 Loss 6.448 Prec@(1,5) (1.0%, 3.3%)
08/01 01:00:02 PM | Train: [ 2/210] Step 1680/6092 Loss 6.446 Prec@(1,5) (1.0%, 3.3%)
08/01 01:00:21 PM | Train: [ 2/210] Step 1690/6092 Loss 6.446 Prec@(1,5) (1.0%, 3.3%)
08/01 01:00:38 PM | Train: [ 2/210] Step 1700/6092 Loss 6.444 Prec@(1,5) (1.0%, 3.4%)
08/01 01:00:56 PM | Train: [ 2/210] Step 1710/6092 Loss 6.443 Prec@(1,5) (1.0%, 3.4%)
08/01 01:01:15 PM | Train: [ 2/210] Step 1720/6092 Loss 6.441 Prec@(1,5) (1.0%, 3.3%)
08/01 01:01:33 PM | Train: [ 2/210] Step 1730/6092 Loss 6.442 Prec@(1,5) (1.0%, 3.4%)
08/01 01:01:50 PM | Train: [ 2/210] Step 1740/6092 Loss 6.441 Prec@(1,5) (1.0%, 3.4%)
08/01 01:02:08 PM | Train: [ 2/210] Step 1750/6092 Loss 6.440 Prec@(1,5) (1.0%, 3.4%)
08/01 01:02:25 PM | Train: [ 2/210] Step 1760/6092 Loss 6.439 Prec@(1,5) (1.0%, 3.4%)
08/01 01:02:43 PM | Train: [ 2/210] Step 1770/6092 Loss 6.440 Prec@(1,5) (1.0%, 3.4%)
08/01 01:03:00 PM | Train: [ 2/210] Step 1780/6092 Loss 6.438 Prec@(1,5) (1.0%, 3.4%)
08/01 01:03:18 PM | Train: [ 2/210] Step 1790/6092 Loss 6.437 Prec@(1,5) (1.0%, 3.4%)
08/01 01:03:35 PM | Train: [ 2/210] Step 1800/6092 Loss 6.434 Prec@(1,5) (1.0%, 3.4%)
08/01 01:03:53 PM | Train: [ 2/210] Step 1810/6092 Loss 6.433 Prec@(1,5) (1.0%, 3.4%)
08/01 01:04:10 PM | Train: [ 2/210] Step 1820/6092 Loss 6.434 Prec@(1,5) (1.0%, 3.4%)
08/01 01:04:27 PM | Train: [ 2/210] Step 1830/6092 Loss 6.435 Prec@(1,5) (1.0%, 3.4%)
08/01 01:04:45 PM | Train: [ 2/210] Step 1840/6092 Loss 6.434 Prec@(1,5) (1.0%, 3.4%)
08/01 01:05:02 PM | Train: [ 2/210] Step 1850/6092 Loss 6.432 Prec@(1,5) (0.9%, 3.4%)
08/01 01:05:20 PM | Train: [ 2/210] Step 1860/6092 Loss 6.431 Prec@(1,5) (0.9%, 3.4%)
08/01 01:05:38 PM | Train: [ 2/210] Step 1870/6092 Loss 6.429 Prec@(1,5) (0.9%, 3.4%)
08/01 01:05:55 PM | Train: [ 2/210] Step 1880/6092 Loss 6.429 Prec@(1,5) (0.9%, 3.4%)
08/01 01:06:13 PM | Train: [ 2/210] Step 1890/6092 Loss 6.425 Prec@(1,5) (0.9%, 3.4%)
08/01 01:06:30 PM | Train: [ 2/210] Step 1900/6092 Loss 6.426 Prec@(1,5) (0.9%, 3.4%)
08/01 01:06:48 PM | Train: [ 2/210] Step 1910/6092 Loss 6.426 Prec@(1,5) (0.9%, 3.4%)
08/01 01:07:05 PM | Train: [ 2/210] Step 1920/6092 Loss 6.426 Prec@(1,5) (0.9%, 3.4%)
08/01 01:07:23 PM | Train: [ 2/210] Step 1930/6092 Loss 6.426 Prec@(1,5) (0.9%, 3.4%)
08/01 01:07:41 PM | Train: [ 2/210] Step 1940/6092 Loss 6.425 Prec@(1,5) (0.9%, 3.4%)
08/01 01:07:58 PM | Train: [ 2/210] Step 1950/6092 Loss 6.424 Prec@(1,5) (0.9%, 3.5%)
08/01 01:08:16 PM | Train: [ 2/210] Step 1960/6092 Loss 6.423 Prec@(1,5) (0.9%, 3.5%)
08/01 01:08:34 PM | Train: [ 2/210] Step 1970/6092 Loss 6.421 Prec@(1,5) (0.9%, 3.5%)
08/01 01:08:52 PM | Train: [ 2/210] Step 1980/6092 Loss 6.419 Prec@(1,5) (0.9%, 3.5%)
08/01 01:09:09 PM | Train: [ 2/210] Step 1990/6092 Loss 6.418 Prec@(1,5) (0.9%, 3.5%)
08/01 01:09:26 PM | Train: [ 2/210] Step 2000/6092 Loss 6.415 Prec@(1,5) (0.9%, 3.6%)
08/01 01:09:44 PM | Train: [ 2/210] Step 2010/6092 Loss 6.415 Prec@(1,5) (0.9%, 3.6%)
08/01 01:10:02 PM | Train: [ 2/210] Step 2020/6092 Loss 6.415 Prec@(1,5) (0.9%, 3.6%)
08/01 01:10:19 PM | Train: [ 2/210] Step 2030/6092 Loss 6.413 Prec@(1,5) (0.9%, 3.6%)
08/01 01:10:36 PM | Train: [ 2/210] Step 2040/6092 Loss 6.414 Prec@(1,5) (0.9%, 3.6%)
08/01 01:10:56 PM | Train: [ 2/210] Step 2050/6092 Loss 6.412 Prec@(1,5) (0.9%, 3.7%)
08/01 01:11:14 PM | Train: [ 2/210] Step 2060/6092 Loss 6.411 Prec@(1,5) (0.9%, 3.7%)
08/01 01:11:31 PM | Train: [ 2/210] Step 2070/6092 Loss 6.410 Prec@(1,5) (0.9%, 3.7%)
08/01 01:11:49 PM | Train: [ 2/210] Step 2080/6092 Loss 6.409 Prec@(1,5) (0.9%, 3.7%)
08/01 01:12:07 PM | Train: [ 2/210] Step 2090/6092 Loss 6.409 Prec@(1,5) (0.9%, 3.7%)
08/01 01:12:24 PM | Train: [ 2/210] Step 2100/6092 Loss 6.410 Prec@(1,5) (0.9%, 3.7%)
08/01 01:12:42 PM | Train: [ 2/210] Step 2110/6092 Loss 6.408 Prec@(1,5) (0.9%, 3.8%)
08/01 01:12:59 PM | Train: [ 2/210] Step 2120/6092 Loss 6.409 Prec@(1,5) (0.9%, 3.7%)
08/01 01:13:17 PM | Train: [ 2/210] Step 2130/6092 Loss 6.411 Prec@(1,5) (0.9%, 3.8%)
08/01 01:13:33 PM | Train: [ 2/210] Step 2140/6092 Loss 6.409 Prec@(1,5) (0.9%, 3.8%)
08/01 01:13:51 PM | Train: [ 2/210] Step 2150/6092 Loss 6.408 Prec@(1,5) (1.0%, 3.8%)
08/01 01:14:09 PM | Train: [ 2/210] Step 2160/6092 Loss 6.408 Prec@(1,5) (0.9%, 3.8%)
08/01 01:14:26 PM | Train: [ 2/210] Step 2170/6092 Loss 6.407 Prec@(1,5) (0.9%, 3.8%)
08/01 01:14:44 PM | Train: [ 2/210] Step 2180/6092 Loss 6.406 Prec@(1,5) (0.9%, 3.8%)
08/01 01:15:02 PM | Train: [ 2/210] Step 2190/6092 Loss 6.405 Prec@(1,5) (1.0%, 3.8%)
08/01 01:15:19 PM | Train: [ 2/210] Step 2200/6092 Loss 6.402 Prec@(1,5) (1.0%, 3.8%)
08/01 01:15:37 PM | Train: [ 2/210] Step 2210/6092 Loss 6.402 Prec@(1,5) (0.9%, 3.8%)
08/01 01:15:54 PM | Train: [ 2/210] Step 2220/6092 Loss 6.404 Prec@(1,5) (0.9%, 3.8%)
08/01 01:16:11 PM | Train: [ 2/210] Step 2230/6092 Loss 6.402 Prec@(1,5) (1.0%, 3.9%)
08/01 01:16:29 PM | Train: [ 2/210] Step 2240/6092 Loss 6.403 Prec@(1,5) (1.0%, 3.9%)
08/01 01:16:46 PM | Train: [ 2/210] Step 2250/6092 Loss 6.402 Prec@(1,5) (1.0%, 3.9%)
08/01 01:17:03 PM | Train: [ 2/210] Step 2260/6092 Loss 6.402 Prec@(1,5) (1.0%, 3.9%)
08/01 01:17:22 PM | Train: [ 2/210] Step 2270/6092 Loss 6.403 Prec@(1,5) (0.9%, 3.9%)
08/01 01:17:40 PM | Train: [ 2/210] Step 2280/6092 Loss 6.402 Prec@(1,5) (1.0%, 3.9%)
08/01 01:17:58 PM | Train: [ 2/210] Step 2290/6092 Loss 6.403 Prec@(1,5) (1.0%, 3.9%)
08/01 01:18:15 PM | Train: [ 2/210] Step 2300/6092 Loss 6.404 Prec@(1,5) (1.0%, 3.9%)
08/01 01:18:34 PM | Train: [ 2/210] Step 2310/6092 Loss 6.403 Prec@(1,5) (1.0%, 3.9%)
08/01 01:18:51 PM | Train: [ 2/210] Step 2320/6092 Loss 6.403 Prec@(1,5) (0.9%, 3.9%)
08/01 01:19:09 PM | Train: [ 2/210] Step 2330/6092 Loss 6.404 Prec@(1,5) (0.9%, 3.9%)
08/01 01:19:26 PM | Train: [ 2/210] Step 2340/6092 Loss 6.403 Prec@(1,5) (1.0%, 3.9%)
08/01 01:19:43 PM | Train: [ 2/210] Step 2350/6092 Loss 6.403 Prec@(1,5) (1.0%, 3.9%)
08/01 01:20:01 PM | Train: [ 2/210] Step 2360/6092 Loss 6.404 Prec@(1,5) (1.0%, 3.9%)
08/01 01:20:19 PM | Train: [ 2/210] Step 2370/6092 Loss 6.402 Prec@(1,5) (1.0%, 3.9%)
08/01 01:20:39 PM | Train: [ 2/210] Step 2380/6092 Loss 6.402 Prec@(1,5) (1.0%, 3.9%)
08/01 01:20:56 PM | Train: [ 2/210] Step 2390/6092 Loss 6.401 Prec@(1,5) (1.0%, 3.9%)
08/01 01:21:13 PM | Train: [ 2/210] Step 2400/6092 Loss 6.401 Prec@(1,5) (1.0%, 3.9%)
08/01 01:21:31 PM | Train: [ 2/210] Step 2410/6092 Loss 6.402 Prec@(1,5) (1.0%, 3.9%)
08/01 01:21:49 PM | Train: [ 2/210] Step 2420/6092 Loss 6.401 Prec@(1,5) (1.0%, 3.8%)
08/01 01:22:06 PM | Train: [ 2/210] Step 2430/6092 Loss 6.400 Prec@(1,5) (0.9%, 3.9%)
08/01 01:22:24 PM | Train: [ 2/210] Step 2440/6092 Loss 6.398 Prec@(1,5) (0.9%, 3.9%)
08/01 01:22:41 PM | Train: [ 2/210] Step 2450/6092 Loss 6.397 Prec@(1,5) (1.0%, 3.9%)
08/01 01:22:59 PM | Train: [ 2/210] Step 2460/6092 Loss 6.396 Prec@(1,5) (1.0%, 3.9%)
08/01 01:23:16 PM | Train: [ 2/210] Step 2470/6092 Loss 6.394 Prec@(1,5) (1.0%, 4.0%)
08/01 01:23:34 PM | Train: [ 2/210] Step 2480/6092 Loss 6.394 Prec@(1,5) (1.0%, 4.0%)
08/01 01:23:51 PM | Train: [ 2/210] Step 2490/6092 Loss 6.394 Prec@(1,5) (1.0%, 4.0%)
08/01 01:24:09 PM | Train: [ 2/210] Step 2500/6092 Loss 6.392 Prec@(1,5) (1.0%, 4.0%)
08/01 01:24:27 PM | Train: [ 2/210] Step 2510/6092 Loss 6.391 Prec@(1,5) (1.0%, 4.0%)
08/01 01:24:44 PM | Train: [ 2/210] Step 2520/6092 Loss 6.391 Prec@(1,5) (1.0%, 4.0%)
08/01 01:25:02 PM | Train: [ 2/210] Step 2530/6092 Loss 6.391 Prec@(1,5) (1.0%, 4.0%)
08/01 01:25:21 PM | Train: [ 2/210] Step 2540/6092 Loss 6.391 Prec@(1,5) (1.0%, 4.0%)
08/01 01:25:38 PM | Train: [ 2/210] Step 2550/6092 Loss 6.389 Prec@(1,5) (1.0%, 4.0%)
08/01 01:25:56 PM | Train: [ 2/210] Step 2560/6092 Loss 6.387 Prec@(1,5) (1.0%, 4.0%)
08/01 01:26:13 PM | Train: [ 2/210] Step 2570/6092 Loss 6.386 Prec@(1,5) (1.0%, 4.0%)
08/01 01:26:31 PM | Train: [ 2/210] Step 2580/6092 Loss 6.385 Prec@(1,5) (1.0%, 4.0%)
08/01 01:26:48 PM | Train: [ 2/210] Step 2590/6092 Loss 6.384 Prec@(1,5) (1.0%, 3.9%)
08/01 01:27:06 PM | Train: [ 2/210] Step 2600/6092 Loss 6.382 Prec@(1,5) (1.0%, 3.9%)
08/01 01:27:23 PM | Train: [ 2/210] Step 2610/6092 Loss 6.382 Prec@(1,5) (1.0%, 3.9%)
08/01 01:27:41 PM | Train: [ 2/210] Step 2620/6092 Loss 6.381 Prec@(1,5) (1.0%, 3.9%)
08/01 01:27:59 PM | Train: [ 2/210] Step 2630/6092 Loss 6.379 Prec@(1,5) (1.0%, 4.0%)
08/01 01:28:16 PM | Train: [ 2/210] Step 2640/6092 Loss 6.379 Prec@(1,5) (1.0%, 4.0%)
08/01 01:28:33 PM | Train: [ 2/210] Step 2650/6092 Loss 6.379 Prec@(1,5) (1.0%, 4.0%)
08/01 01:28:51 PM | Train: [ 2/210] Step 2660/6092 Loss 6.377 Prec@(1,5) (1.0%, 4.0%)
08/01 01:29:08 PM | Train: [ 2/210] Step 2670/6092 Loss 6.378 Prec@(1,5) (1.0%, 4.0%)
08/01 01:29:26 PM | Train: [ 2/210] Step 2680/6092 Loss 6.378 Prec@(1,5) (1.0%, 4.0%)
08/01 01:29:43 PM | Train: [ 2/210] Step 2690/6092 Loss 6.377 Prec@(1,5) (1.0%, 4.0%)
08/01 01:30:00 PM | Train: [ 2/210] Step 2700/6092 Loss 6.377 Prec@(1,5) (1.0%, 4.0%)
08/01 01:30:20 PM | Train: [ 2/210] Step 2710/6092 Loss 6.377 Prec@(1,5) (1.0%, 4.0%)
08/01 01:30:37 PM | Train: [ 2/210] Step 2720/6092 Loss 6.376 Prec@(1,5) (1.0%, 4.0%)
08/01 01:30:55 PM | Train: [ 2/210] Step 2730/6092 Loss 6.376 Prec@(1,5) (1.0%, 4.0%)
08/01 01:31:12 PM | Train: [ 2/210] Step 2740/6092 Loss 6.377 Prec@(1,5) (1.0%, 4.0%)
08/01 01:31:30 PM | Train: [ 2/210] Step 2750/6092 Loss 6.376 Prec@(1,5) (1.0%, 4.1%)
08/01 01:31:47 PM | Train: [ 2/210] Step 2760/6092 Loss 6.374 Prec@(1,5) (1.0%, 4.1%)
08/01 01:32:04 PM | Train: [ 2/210] Step 2770/6092 Loss 6.374 Prec@(1,5) (1.0%, 4.1%)
08/01 01:32:22 PM | Train: [ 2/210] Step 2780/6092 Loss 6.374 Prec@(1,5) (1.0%, 4.1%)
08/01 01:32:39 PM | Train: [ 2/210] Step 2790/6092 Loss 6.373 Prec@(1,5) (1.0%, 4.1%)
08/01 01:32:56 PM | Train: [ 2/210] Step 2800/6092 Loss 6.373 Prec@(1,5) (1.0%, 4.1%)
08/01 01:33:14 PM | Train: [ 2/210] Step 2810/6092 Loss 6.372 Prec@(1,5) (1.0%, 4.1%)
08/01 01:33:32 PM | Train: [ 2/210] Step 2820/6092 Loss 6.371 Prec@(1,5) (1.1%, 4.2%)
08/01 01:33:49 PM | Train: [ 2/210] Step 2830/6092 Loss 6.370 Prec@(1,5) (1.1%, 4.2%)
08/01 01:34:06 PM | Train: [ 2/210] Step 2840/6092 Loss 6.370 Prec@(1,5) (1.1%, 4.2%)
08/01 01:34:23 PM | Train: [ 2/210] Step 2850/6092 Loss 6.370 Prec@(1,5) (1.1%, 4.2%)
08/01 01:34:40 PM | Train: [ 2/210] Step 2860/6092 Loss 6.371 Prec@(1,5) (1.1%, 4.2%)
08/01 01:34:58 PM | Train: [ 2/210] Step 2870/6092 Loss 6.369 Prec@(1,5) (1.1%, 4.2%)
08/01 01:35:17 PM | Train: [ 2/210] Step 2880/6092 Loss 6.368 Prec@(1,5) (1.1%, 4.2%)
08/01 01:35:34 PM | Train: [ 2/210] Step 2890/6092 Loss 6.367 Prec@(1,5) (1.1%, 4.2%)
08/01 01:35:51 PM | Train: [ 2/210] Step 2900/6092 Loss 6.366 Prec@(1,5) (1.1%, 4.2%)
08/01 01:36:09 PM | Train: [ 2/210] Step 2910/6092 Loss 6.366 Prec@(1,5) (1.1%, 4.2%)
08/01 01:36:26 PM | Train: [ 2/210] Step 2920/6092 Loss 6.366 Prec@(1,5) (1.1%, 4.2%)
08/01 01:36:43 PM | Train: [ 2/210] Step 2930/6092 Loss 6.365 Prec@(1,5) (1.1%, 4.2%)
08/01 01:37:01 PM | Train: [ 2/210] Step 2940/6092 Loss 6.363 Prec@(1,5) (1.1%, 4.3%)
08/01 01:37:18 PM | Train: [ 2/210] Step 2950/6092 Loss 6.364 Prec@(1,5) (1.1%, 4.3%)
08/01 01:37:35 PM | Train: [ 2/210] Step 2960/6092 Loss 6.363 Prec@(1,5) (1.0%, 4.3%)
08/01 01:37:53 PM | Train: [ 2/210] Step 2970/6092 Loss 6.363 Prec@(1,5) (1.0%, 4.2%)
08/01 01:38:10 PM | Train: [ 2/210] Step 2980/6092 Loss 6.363 Prec@(1,5) (1.0%, 4.2%)
08/01 01:38:27 PM | Train: [ 2/210] Step 2990/6092 Loss 6.363 Prec@(1,5) (1.1%, 4.3%)
08/01 01:38:44 PM | Train: [ 2/210] Step 3000/6092 Loss 6.363 Prec@(1,5) (1.0%, 4.2%)
08/01 01:39:02 PM | Train: [ 2/210] Step 3010/6092 Loss 6.363 Prec@(1,5) (1.0%, 4.2%)
08/01 01:39:19 PM | Train: [ 2/210] Step 3020/6092 Loss 6.362 Prec@(1,5) (1.1%, 4.3%)
08/01 01:39:36 PM | Train: [ 2/210] Step 3030/6092 Loss 6.361 Prec@(1,5) (1.1%, 4.3%)
08/01 01:39:54 PM | Train: [ 2/210] Step 3040/6092 Loss 6.361 Prec@(1,5) (1.1%, 4.3%)
08/01 01:40:13 PM | Train: [ 2/210] Step 3050/6092 Loss 6.360 Prec@(1,5) (1.1%, 4.3%)
08/01 01:40:30 PM | Train: [ 2/210] Step 3060/6092 Loss 6.359 Prec@(1,5) (1.1%, 4.4%)
08/01 01:40:48 PM | Train: [ 2/210] Step 3070/6092 Loss 6.360 Prec@(1,5) (1.1%, 4.4%)
08/01 01:41:05 PM | Train: [ 2/210] Step 3080/6092 Loss 6.358 Prec@(1,5) (1.1%, 4.4%)
08/01 01:41:23 PM | Train: [ 2/210] Step 3090/6092 Loss 6.358 Prec@(1,5) (1.1%, 4.4%)
08/01 01:41:40 PM | Train: [ 2/210] Step 3100/6092 Loss 6.359 Prec@(1,5) (1.1%, 4.4%)
08/01 01:41:57 PM | Train: [ 2/210] Step 3110/6092 Loss 6.359 Prec@(1,5) (1.1%, 4.4%)
08/01 01:42:15 PM | Train: [ 2/210] Step 3120/6092 Loss 6.359 Prec@(1,5) (1.1%, 4.4%)
08/01 01:42:32 PM | Train: [ 2/210] Step 3130/6092 Loss 6.358 Prec@(1,5) (1.1%, 4.5%)
08/01 01:42:50 PM | Train: [ 2/210] Step 3140/6092 Loss 6.356 Prec@(1,5) (1.1%, 4.5%)
08/01 01:43:07 PM | Train: [ 2/210] Step 3150/6092 Loss 6.356 Prec@(1,5) (1.1%, 4.5%)
08/01 01:43:25 PM | Train: [ 2/210] Step 3160/6092 Loss 6.356 Prec@(1,5) (1.1%, 4.5%)
08/01 01:43:42 PM | Train: [ 2/210] Step 3170/6092 Loss 6.354 Prec@(1,5) (1.1%, 4.4%)
08/01 01:43:59 PM | Train: [ 2/210] Step 3180/6092 Loss 6.352 Prec@(1,5) (1.1%, 4.5%)
08/01 01:44:17 PM | Train: [ 2/210] Step 3190/6092 Loss 6.352 Prec@(1,5) (1.1%, 4.5%)
08/01 01:44:34 PM | Train: [ 2/210] Step 3200/6092 Loss 6.352 Prec@(1,5) (1.1%, 4.5%)
08/01 01:44:51 PM | Train: [ 2/210] Step 3210/6092 Loss 6.353 Prec@(1,5) (1.1%, 4.5%)
08/01 01:45:08 PM | Train: [ 2/210] Step 3220/6092 Loss 6.352 Prec@(1,5) (1.1%, 4.5%)
08/01 01:45:28 PM | Train: [ 2/210] Step 3230/6092 Loss 6.351 Prec@(1,5) (1.1%, 4.5%)
08/01 01:45:46 PM | Train: [ 2/210] Step 3240/6092 Loss 6.351 Prec@(1,5) (1.1%, 4.5%)
08/01 01:46:03 PM | Train: [ 2/210] Step 3250/6092 Loss 6.351 Prec@(1,5) (1.1%, 4.5%)
08/01 01:46:20 PM | Train: [ 2/210] Step 3260/6092 Loss 6.350 Prec@(1,5) (1.1%, 4.5%)
08/01 01:46:37 PM | Train: [ 2/210] Step 3270/6092 Loss 6.349 Prec@(1,5) (1.1%, 4.5%)
08/01 01:46:54 PM | Train: [ 2/210] Step 3280/6092 Loss 6.349 Prec@(1,5) (1.1%, 4.5%)
08/01 01:47:12 PM | Train: [ 2/210] Step 3290/6092 Loss 6.349 Prec@(1,5) (1.2%, 4.5%)
08/01 01:47:29 PM | Train: [ 2/210] Step 3300/6092 Loss 6.349 Prec@(1,5) (1.2%, 4.5%)
08/01 01:47:46 PM | Train: [ 2/210] Step 3310/6092 Loss 6.349 Prec@(1,5) (1.2%, 4.6%)
08/01 01:48:04 PM | Train: [ 2/210] Step 3320/6092 Loss 6.347 Prec@(1,5) (1.2%, 4.6%)
08/01 01:48:21 PM | Train: [ 2/210] Step 3330/6092 Loss 6.346 Prec@(1,5) (1.2%, 4.6%)
08/01 01:48:39 PM | Train: [ 2/210] Step 3340/6092 Loss 6.345 Prec@(1,5) (1.2%, 4.6%)
08/01 01:48:56 PM | Train: [ 2/210] Step 3350/6092 Loss 6.344 Prec@(1,5) (1.2%, 4.6%)
08/01 01:49:14 PM | Train: [ 2/210] Step 3360/6092 Loss 6.341 Prec@(1,5) (1.2%, 4.7%)
08/01 01:49:31 PM | Train: [ 2/210] Step 3370/6092 Loss 6.340 Prec@(1,5) (1.2%, 4.7%)
08/01 01:49:48 PM | Train: [ 2/210] Step 3380/6092 Loss 6.338 Prec@(1,5) (1.2%, 4.7%)
08/01 01:50:06 PM | Train: [ 2/210] Step 3390/6092 Loss 6.338 Prec@(1,5) (1.2%, 4.7%)
08/01 01:50:25 PM | Train: [ 2/210] Step 3400/6092 Loss 6.338 Prec@(1,5) (1.2%, 4.7%)
08/01 01:50:42 PM | Train: [ 2/210] Step 3410/6092 Loss 6.336 Prec@(1,5) (1.2%, 4.7%)
08/01 01:50:59 PM | Train: [ 2/210] Step 3420/6092 Loss 6.335 Prec@(1,5) (1.2%, 4.7%)
08/01 01:51:17 PM | Train: [ 2/210] Step 3430/6092 Loss 6.335 Prec@(1,5) (1.2%, 4.7%)
08/01 01:51:34 PM | Train: [ 2/210] Step 3440/6092 Loss 6.335 Prec@(1,5) (1.2%, 4.7%)
08/01 01:51:52 PM | Train: [ 2/210] Step 3450/6092 Loss 6.334 Prec@(1,5) (1.2%, 4.7%)
08/01 01:52:09 PM | Train: [ 2/210] Step 3460/6092 Loss 6.335 Prec@(1,5) (1.2%, 4.7%)
08/01 01:52:26 PM | Train: [ 2/210] Step 3470/6092 Loss 6.334 Prec@(1,5) (1.2%, 4.7%)
08/01 01:52:44 PM | Train: [ 2/210] Step 3480/6092 Loss 6.334 Prec@(1,5) (1.2%, 4.7%)
08/01 01:53:01 PM | Train: [ 2/210] Step 3490/6092 Loss 6.333 Prec@(1,5) (1.2%, 4.8%)
08/01 01:53:18 PM | Train: [ 2/210] Step 3500/6092 Loss 6.332 Prec@(1,5) (1.2%, 4.8%)
08/01 01:53:36 PM | Train: [ 2/210] Step 3510/6092 Loss 6.330 Prec@(1,5) (1.3%, 4.8%)
08/01 01:53:54 PM | Train: [ 2/210] Step 3520/6092 Loss 6.331 Prec@(1,5) (1.2%, 4.8%)
08/01 01:54:11 PM | Train: [ 2/210] Step 3530/6092 Loss 6.332 Prec@(1,5) (1.2%, 4.8%)
08/01 01:54:29 PM | Train: [ 2/210] Step 3540/6092 Loss 6.332 Prec@(1,5) (1.2%, 4.8%)
08/01 01:54:46 PM | Train: [ 2/210] Step 3550/6092 Loss 6.331 Prec@(1,5) (1.3%, 4.8%)
08/01 01:55:03 PM | Train: [ 2/210] Step 3560/6092 Loss 6.329 Prec@(1,5) (1.2%, 4.8%)
08/01 01:55:22 PM | Train: [ 2/210] Step 3570/6092 Loss 6.328 Prec@(1,5) (1.2%, 4.8%)
08/01 01:55:40 PM | Train: [ 2/210] Step 3580/6092 Loss 6.329 Prec@(1,5) (1.2%, 4.8%)
08/01 01:55:57 PM | Train: [ 2/210] Step 3590/6092 Loss 6.327 Prec@(1,5) (1.3%, 4.8%)
08/01 01:56:14 PM | Train: [ 2/210] Step 3600/6092 Loss 6.327 Prec@(1,5) (1.3%, 4.8%)
08/01 01:56:32 PM | Train: [ 2/210] Step 3610/6092 Loss 6.327 Prec@(1,5) (1.3%, 4.8%)
08/01 01:56:49 PM | Train: [ 2/210] Step 3620/6092 Loss 6.325 Prec@(1,5) (1.3%, 4.9%)
08/01 01:57:07 PM | Train: [ 2/210] Step 3630/6092 Loss 6.326 Prec@(1,5) (1.3%, 4.9%)
08/01 01:57:24 PM | Train: [ 2/210] Step 3640/6092 Loss 6.326 Prec@(1,5) (1.3%, 4.8%)
08/01 01:57:42 PM | Train: [ 2/210] Step 3650/6092 Loss 6.325 Prec@(1,5) (1.3%, 4.8%)
08/01 01:57:59 PM | Train: [ 2/210] Step 3660/6092 Loss 6.325 Prec@(1,5) (1.3%, 4.9%)
08/01 01:58:17 PM | Train: [ 2/210] Step 3670/6092 Loss 6.325 Prec@(1,5) (1.3%, 4.8%)
08/01 01:58:34 PM | Train: [ 2/210] Step 3680/6092 Loss 6.324 Prec@(1,5) (1.3%, 4.8%)
08/01 01:58:51 PM | Train: [ 2/210] Step 3690/6092 Loss 6.324 Prec@(1,5) (1.3%, 4.8%)
08/01 01:59:09 PM | Train: [ 2/210] Step 3700/6092 Loss 6.324 Prec@(1,5) (1.3%, 4.8%)
08/01 01:59:26 PM | Train: [ 2/210] Step 3710/6092 Loss 6.324 Prec@(1,5) (1.3%, 4.8%)
08/01 01:59:43 PM | Train: [ 2/210] Step 3720/6092 Loss 6.323 Prec@(1,5) (1.3%, 4.8%)
08/01 02:00:01 PM | Train: [ 2/210] Step 3730/6092 Loss 6.324 Prec@(1,5) (1.3%, 4.8%)
08/01 02:00:18 PM | Train: [ 2/210] Step 3740/6092 Loss 6.323 Prec@(1,5) (1.3%, 4.9%)
08/01 02:00:37 PM | Train: [ 2/210] Step 3750/6092 Loss 6.324 Prec@(1,5) (1.3%, 4.8%)
08/01 02:00:55 PM | Train: [ 2/210] Step 3760/6092 Loss 6.324 Prec@(1,5) (1.3%, 4.8%)
08/01 02:01:13 PM | Train: [ 2/210] Step 3770/6092 Loss 6.323 Prec@(1,5) (1.3%, 4.9%)
08/01 02:01:32 PM | Train: [ 2/210] Step 3780/6092 Loss 6.322 Prec@(1,5) (1.3%, 4.9%)
08/01 02:01:49 PM | Train: [ 2/210] Step 3790/6092 Loss 6.322 Prec@(1,5) (1.3%, 4.9%)
08/01 02:02:06 PM | Train: [ 2/210] Step 3800/6092 Loss 6.322 Prec@(1,5) (1.3%, 4.9%)
08/01 02:02:23 PM | Train: [ 2/210] Step 3810/6092 Loss 6.322 Prec@(1,5) (1.3%, 4.9%)
08/01 02:02:41 PM | Train: [ 2/210] Step 3820/6092 Loss 6.322 Prec@(1,5) (1.3%, 4.9%)
08/01 02:02:58 PM | Train: [ 2/210] Step 3830/6092 Loss 6.321 Prec@(1,5) (1.3%, 4.9%)
08/01 02:03:15 PM | Train: [ 2/210] Step 3840/6092 Loss 6.320 Prec@(1,5) (1.3%, 4.9%)
08/01 02:03:32 PM | Train: [ 2/210] Step 3850/6092 Loss 6.320 Prec@(1,5) (1.3%, 4.9%)
08/01 02:03:50 PM | Train: [ 2/210] Step 3860/6092 Loss 6.320 Prec@(1,5) (1.3%, 4.9%)
08/01 02:04:07 PM | Train: [ 2/210] Step 3870/6092 Loss 6.319 Prec@(1,5) (1.3%, 4.9%)
08/01 02:04:25 PM | Train: [ 2/210] Step 3880/6092 Loss 6.318 Prec@(1,5) (1.3%, 4.9%)
08/01 02:04:43 PM | Train: [ 2/210] Step 3890/6092 Loss 6.316 Prec@(1,5) (1.3%, 4.9%)
08/01 02:05:00 PM | Train: [ 2/210] Step 3900/6092 Loss 6.316 Prec@(1,5) (1.3%, 4.9%)
08/01 02:05:18 PM | Train: [ 2/210] Step 3910/6092 Loss 6.317 Prec@(1,5) (1.3%, 4.9%)
08/01 02:05:36 PM | Train: [ 2/210] Step 3920/6092 Loss 6.316 Prec@(1,5) (1.3%, 4.9%)
08/01 02:05:54 PM | Train: [ 2/210] Step 3930/6092 Loss 6.315 Prec@(1,5) (1.3%, 5.0%)
08/01 02:06:11 PM | Train: [ 2/210] Step 3940/6092 Loss 6.314 Prec@(1,5) (1.3%, 5.0%)
08/01 02:06:29 PM | Train: [ 2/210] Step 3950/6092 Loss 6.313 Prec@(1,5) (1.3%, 5.0%)
08/01 02:06:47 PM | Train: [ 2/210] Step 3960/6092 Loss 6.313 Prec@(1,5) (1.3%, 5.0%)
08/01 02:07:05 PM | Train: [ 2/210] Step 3970/6092 Loss 6.312 Prec@(1,5) (1.3%, 5.0%)
08/01 02:07:23 PM | Train: [ 2/210] Step 3980/6092 Loss 6.311 Prec@(1,5) (1.3%, 5.0%)
08/01 02:07:40 PM | Train: [ 2/210] Step 3990/6092 Loss 6.310 Prec@(1,5) (1.3%, 5.0%)
08/01 02:07:58 PM | Train: [ 2/210] Step 4000/6092 Loss 6.309 Prec@(1,5) (1.3%, 5.0%)
08/01 02:08:16 PM | Train: [ 2/210] Step 4010/6092 Loss 6.308 Prec@(1,5) (1.3%, 5.0%)
08/01 02:08:34 PM | Train: [ 2/210] Step 4020/6092 Loss 6.307 Prec@(1,5) (1.3%, 5.1%)
08/01 02:08:52 PM | Train: [ 2/210] Step 4030/6092 Loss 6.307 Prec@(1,5) (1.3%, 5.1%)
08/01 02:09:09 PM | Train: [ 2/210] Step 4040/6092 Loss 6.306 Prec@(1,5) (1.3%, 5.1%)
08/01 02:09:27 PM | Train: [ 2/210] Step 4050/6092 Loss 6.306 Prec@(1,5) (1.3%, 5.1%)
08/01 02:09:46 PM | Train: [ 2/210] Step 4060/6092 Loss 6.305 Prec@(1,5) (1.3%, 5.1%)
08/01 02:10:05 PM | Train: [ 2/210] Step 4070/6092 Loss 6.304 Prec@(1,5) (1.3%, 5.1%)
08/01 02:10:23 PM | Train: [ 2/210] Step 4080/6092 Loss 6.303 Prec@(1,5) (1.3%, 5.1%)
08/01 02:10:40 PM | Train: [ 2/210] Step 4090/6092 Loss 6.303 Prec@(1,5) (1.3%, 5.1%)
08/01 02:10:59 PM | Train: [ 2/210] Step 4100/6092 Loss 6.302 Prec@(1,5) (1.3%, 5.1%)
08/01 02:11:16 PM | Train: [ 2/210] Step 4110/6092 Loss 6.302 Prec@(1,5) (1.3%, 5.1%)
08/01 02:11:35 PM | Train: [ 2/210] Step 4120/6092 Loss 6.302 Prec@(1,5) (1.3%, 5.1%)
08/01 02:11:52 PM | Train: [ 2/210] Step 4130/6092 Loss 6.302 Prec@(1,5) (1.3%, 5.1%)
08/01 02:12:10 PM | Train: [ 2/210] Step 4140/6092 Loss 6.303 Prec@(1,5) (1.3%, 5.1%)
08/01 02:12:28 PM | Train: [ 2/210] Step 4150/6092 Loss 6.303 Prec@(1,5) (1.3%, 5.1%)
08/01 02:12:46 PM | Train: [ 2/210] Step 4160/6092 Loss 6.303 Prec@(1,5) (1.3%, 5.1%)
08/01 02:13:04 PM | Train: [ 2/210] Step 4170/6092 Loss 6.303 Prec@(1,5) (1.3%, 5.1%)
08/01 02:13:21 PM | Train: [ 2/210] Step 4180/6092 Loss 6.301 Prec@(1,5) (1.4%, 5.1%)
08/01 02:13:40 PM | Train: [ 2/210] Step 4190/6092 Loss 6.300 Prec@(1,5) (1.3%, 5.1%)
08/01 02:13:58 PM | Train: [ 2/210] Step 4200/6092 Loss 6.299 Prec@(1,5) (1.4%, 5.1%)
08/01 02:14:15 PM | Train: [ 2/210] Step 4210/6092 Loss 6.299 Prec@(1,5) (1.4%, 5.1%)
08/01 02:14:33 PM | Train: [ 2/210] Step 4220/6092 Loss 6.299 Prec@(1,5) (1.4%, 5.1%)
08/01 02:14:50 PM | Train: [ 2/210] Step 4230/6092 Loss 6.299 Prec@(1,5) (1.3%, 5.1%)
08/01 02:15:09 PM | Train: [ 2/210] Step 4240/6092 Loss 6.300 Prec@(1,5) (1.3%, 5.1%)
08/01 02:15:27 PM | Train: [ 2/210] Step 4250/6092 Loss 6.299 Prec@(1,5) (1.3%, 5.1%)
08/01 02:15:45 PM | Train: [ 2/210] Step 4260/6092 Loss 6.298 Prec@(1,5) (1.3%, 5.1%)
08/01 02:16:03 PM | Train: [ 2/210] Step 4270/6092 Loss 6.297 Prec@(1,5) (1.4%, 5.1%)
08/01 02:16:21 PM | Train: [ 2/210] Step 4280/6092 Loss 6.296 Prec@(1,5) (1.4%, 5.2%)
08/01 02:16:38 PM | Train: [ 2/210] Step 4290/6092 Loss 6.295 Prec@(1,5) (1.4%, 5.2%)
08/01 02:16:56 PM | Train: [ 2/210] Step 4300/6092 Loss 6.293 Prec@(1,5) (1.3%, 5.2%)
08/01 02:17:14 PM | Train: [ 2/210] Step 4310/6092 Loss 6.294 Prec@(1,5) (1.3%, 5.2%)
08/01 02:17:32 PM | Train: [ 2/210] Step 4320/6092 Loss 6.294 Prec@(1,5) (1.3%, 5.1%)
08/01 02:17:49 PM | Train: [ 2/210] Step 4330/6092 Loss 6.295 Prec@(1,5) (1.3%, 5.1%)
08/01 02:18:07 PM | Train: [ 2/210] Step 4340/6092 Loss 6.294 Prec@(1,5) (1.3%, 5.1%)
08/01 02:18:25 PM | Train: [ 2/210] Step 4350/6092 Loss 6.294 Prec@(1,5) (1.3%, 5.1%)
08/01 02:18:42 PM | Train: [ 2/210] Step 4360/6092 Loss 6.294 Prec@(1,5) (1.3%, 5.1%)
08/01 02:18:59 PM | Train: [ 2/210] Step 4370/6092 Loss 6.293 Prec@(1,5) (1.3%, 5.1%)
08/01 02:19:17 PM | Train: [ 2/210] Step 4380/6092 Loss 6.293 Prec@(1,5) (1.3%, 5.2%)
08/01 02:19:34 PM | Train: [ 2/210] Step 4390/6092 Loss 6.291 Prec@(1,5) (1.3%, 5.2%)
08/01 02:19:52 PM | Train: [ 2/210] Step 4400/6092 Loss 6.291 Prec@(1,5) (1.3%, 5.2%)
08/01 02:20:11 PM | Train: [ 2/210] Step 4410/6092 Loss 6.291 Prec@(1,5) (1.3%, 5.2%)
08/01 02:20:29 PM | Train: [ 2/210] Step 4420/6092 Loss 6.292 Prec@(1,5) (1.3%, 5.2%)
08/01 02:20:47 PM | Train: [ 2/210] Step 4430/6092 Loss 6.291 Prec@(1,5) (1.3%, 5.2%)
08/01 02:21:05 PM | Train: [ 2/210] Step 4440/6092 Loss 6.291 Prec@(1,5) (1.3%, 5.2%)
08/01 02:21:23 PM | Train: [ 2/210] Step 4450/6092 Loss 6.290 Prec@(1,5) (1.3%, 5.2%)
08/01 02:21:42 PM | Train: [ 2/210] Step 4460/6092 Loss 6.290 Prec@(1,5) (1.3%, 5.2%)
08/01 02:21:59 PM | Train: [ 2/210] Step 4470/6092 Loss 6.290 Prec@(1,5) (1.3%, 5.2%)
08/01 02:22:17 PM | Train: [ 2/210] Step 4480/6092 Loss 6.290 Prec@(1,5) (1.4%, 5.2%)
08/01 02:22:35 PM | Train: [ 2/210] Step 4490/6092 Loss 6.289 Prec@(1,5) (1.3%, 5.2%)
08/01 02:22:52 PM | Train: [ 2/210] Step 4500/6092 Loss 6.290 Prec@(1,5) (1.3%, 5.2%)
08/01 02:23:10 PM | Train: [ 2/210] Step 4510/6092 Loss 6.289 Prec@(1,5) (1.3%, 5.2%)
08/01 02:23:28 PM | Train: [ 2/210] Step 4520/6092 Loss 6.289 Prec@(1,5) (1.3%, 5.2%)
08/01 02:23:46 PM | Train: [ 2/210] Step 4530/6092 Loss 6.289 Prec@(1,5) (1.3%, 5.2%)
08/01 02:24:04 PM | Train: [ 2/210] Step 4540/6092 Loss 6.287 Prec@(1,5) (1.3%, 5.3%)
08/01 02:24:22 PM | Train: [ 2/210] Step 4550/6092 Loss 6.286 Prec@(1,5) (1.3%, 5.3%)
08/01 02:24:39 PM | Train: [ 2/210] Step 4560/6092 Loss 6.286 Prec@(1,5) (1.3%, 5.3%)
08/01 02:24:57 PM | Train: [ 2/210] Step 4570/6092 Loss 6.285 Prec@(1,5) (1.3%, 5.3%)
08/01 02:25:16 PM | Train: [ 2/210] Step 4580/6092 Loss 6.285 Prec@(1,5) (1.3%, 5.3%)
08/01 02:25:33 PM | Train: [ 2/210] Step 4590/6092 Loss 6.283 Prec@(1,5) (1.3%, 5.3%)
08/01 02:25:50 PM | Train: [ 2/210] Step 4600/6092 Loss 6.283 Prec@(1,5) (1.3%, 5.3%)
08/01 02:26:08 PM | Train: [ 2/210] Step 4610/6092 Loss 6.283 Prec@(1,5) (1.3%, 5.3%)
08/01 02:26:26 PM | Train: [ 2/210] Step 4620/6092 Loss 6.283 Prec@(1,5) (1.3%, 5.3%)
08/01 02:26:43 PM | Train: [ 2/210] Step 4630/6092 Loss 6.282 Prec@(1,5) (1.3%, 5.3%)
08/01 02:27:01 PM | Train: [ 2/210] Step 4640/6092 Loss 6.281 Prec@(1,5) (1.3%, 5.3%)
08/01 02:27:19 PM | Train: [ 2/210] Step 4650/6092 Loss 6.281 Prec@(1,5) (1.3%, 5.3%)
08/01 02:27:36 PM | Train: [ 2/210] Step 4660/6092 Loss 6.280 Prec@(1,5) (1.3%, 5.3%)
08/01 02:27:53 PM | Train: [ 2/210] Step 4670/6092 Loss 6.280 Prec@(1,5) (1.3%, 5.3%)
08/01 02:28:10 PM | Train: [ 2/210] Step 4680/6092 Loss 6.280 Prec@(1,5) (1.3%, 5.3%)
08/01 02:28:28 PM | Train: [ 2/210] Step 4690/6092 Loss 6.280 Prec@(1,5) (1.3%, 5.3%)
08/01 02:28:45 PM | Train: [ 2/210] Step 4700/6092 Loss 6.279 Prec@(1,5) (1.3%, 5.3%)
08/01 02:29:03 PM | Train: [ 2/210] Step 4710/6092 Loss 6.278 Prec@(1,5) (1.3%, 5.3%)
08/01 02:29:20 PM | Train: [ 2/210] Step 4720/6092 Loss 6.278 Prec@(1,5) (1.3%, 5.3%)
08/01 02:29:38 PM | Train: [ 2/210] Step 4730/6092 Loss 6.277 Prec@(1,5) (1.3%, 5.3%)
08/01 02:29:55 PM | Train: [ 2/210] Step 4740/6092 Loss 6.275 Prec@(1,5) (1.3%, 5.4%)
08/01 02:30:13 PM | Train: [ 2/210] Step 4750/6092 Loss 6.275 Prec@(1,5) (1.3%, 5.4%)
08/01 02:30:32 PM | Train: [ 2/210] Step 4760/6092 Loss 6.275 Prec@(1,5) (1.3%, 5.4%)
08/01 02:30:50 PM | Train: [ 2/210] Step 4770/6092 Loss 6.275 Prec@(1,5) (1.3%, 5.4%)
08/01 02:31:07 PM | Train: [ 2/210] Step 4780/6092 Loss 6.274 Prec@(1,5) (1.3%, 5.4%)
08/01 02:31:25 PM | Train: [ 2/210] Step 4790/6092 Loss 6.274 Prec@(1,5) (1.3%, 5.4%)
08/01 02:31:42 PM | Train: [ 2/210] Step 4800/6092 Loss 6.273 Prec@(1,5) (1.3%, 5.4%)
08/01 02:32:00 PM | Train: [ 2/210] Step 4810/6092 Loss 6.272 Prec@(1,5) (1.3%, 5.4%)
08/01 02:32:17 PM | Train: [ 2/210] Step 4820/6092 Loss 6.272 Prec@(1,5) (1.3%, 5.4%)
08/01 02:32:35 PM | Train: [ 2/210] Step 4830/6092 Loss 6.272 Prec@(1,5) (1.3%, 5.4%)
08/01 02:32:52 PM | Train: [ 2/210] Step 4840/6092 Loss 6.270 Prec@(1,5) (1.4%, 5.4%)
08/01 02:33:10 PM | Train: [ 2/210] Step 4850/6092 Loss 6.270 Prec@(1,5) (1.4%, 5.4%)
08/01 02:33:27 PM | Train: [ 2/210] Step 4860/6092 Loss 6.269 Prec@(1,5) (1.3%, 5.4%)
08/01 02:33:44 PM | Train: [ 2/210] Step 4870/6092 Loss 6.269 Prec@(1,5) (1.4%, 5.4%)
08/01 02:34:02 PM | Train: [ 2/210] Step 4880/6092 Loss 6.268 Prec@(1,5) (1.4%, 5.4%)
08/01 02:34:19 PM | Train: [ 2/210] Step 4890/6092 Loss 6.268 Prec@(1,5) (1.4%, 5.4%)
08/01 02:34:37 PM | Train: [ 2/210] Step 4900/6092 Loss 6.267 Prec@(1,5) (1.4%, 5.4%)
08/01 02:34:54 PM | Train: [ 2/210] Step 4910/6092 Loss 6.266 Prec@(1,5) (1.4%, 5.4%)
08/01 02:35:13 PM | Train: [ 2/210] Step 4920/6092 Loss 6.265 Prec@(1,5) (1.4%, 5.4%)
08/01 02:35:31 PM | Train: [ 2/210] Step 4930/6092 Loss 6.265 Prec@(1,5) (1.4%, 5.4%)
08/01 02:35:48 PM | Train: [ 2/210] Step 4940/6092 Loss 6.264 Prec@(1,5) (1.4%, 5.4%)
08/01 02:36:05 PM | Train: [ 2/210] Step 4950/6092 Loss 6.263 Prec@(1,5) (1.4%, 5.5%)
08/01 02:36:21 PM | Train: [ 2/210] Step 4960/6092 Loss 6.264 Prec@(1,5) (1.4%, 5.5%)
08/01 02:36:39 PM | Train: [ 2/210] Step 4970/6092 Loss 6.263 Prec@(1,5) (1.4%, 5.5%)
08/01 02:36:57 PM | Train: [ 2/210] Step 4980/6092 Loss 6.263 Prec@(1,5) (1.4%, 5.5%)
08/01 02:37:14 PM | Train: [ 2/210] Step 4990/6092 Loss 6.262 Prec@(1,5) (1.4%, 5.5%)
08/01 02:37:32 PM | Train: [ 2/210] Step 5000/6092 Loss 6.261 Prec@(1,5) (1.4%, 5.5%)
08/01 02:37:50 PM | Train: [ 2/210] Step 5010/6092 Loss 6.259 Prec@(1,5) (1.4%, 5.6%)
08/01 02:38:07 PM | Train: [ 2/210] Step 5020/6092 Loss 6.258 Prec@(1,5) (1.4%, 5.6%)
08/01 02:38:24 PM | Train: [ 2/210] Step 5030/6092 Loss 6.257 Prec@(1,5) (1.4%, 5.6%)
08/01 02:38:42 PM | Train: [ 2/210] Step 5040/6092 Loss 6.257 Prec@(1,5) (1.4%, 5.6%)
08/01 02:38:59 PM | Train: [ 2/210] Step 5050/6092 Loss 6.256 Prec@(1,5) (1.4%, 5.6%)
08/01 02:39:16 PM | Train: [ 2/210] Step 5060/6092 Loss 6.255 Prec@(1,5) (1.4%, 5.6%)
08/01 02:39:34 PM | Train: [ 2/210] Step 5070/6092 Loss 6.254 Prec@(1,5) (1.5%, 5.6%)
08/01 02:39:51 PM | Train: [ 2/210] Step 5080/6092 Loss 6.253 Prec@(1,5) (1.5%, 5.7%)
08/01 02:40:11 PM | Train: [ 2/210] Step 5090/6092 Loss 6.253 Prec@(1,5) (1.5%, 5.6%)
08/01 02:40:28 PM | Train: [ 2/210] Step 5100/6092 Loss 6.252 Prec@(1,5) (1.5%, 5.6%)
08/01 02:40:45 PM | Train: [ 2/210] Step 5110/6092 Loss 6.251 Prec@(1,5) (1.4%, 5.7%)
08/01 02:41:02 PM | Train: [ 2/210] Step 5120/6092 Loss 6.250 Prec@(1,5) (1.5%, 5.7%)
08/01 02:41:20 PM | Train: [ 2/210] Step 5130/6092 Loss 6.249 Prec@(1,5) (1.5%, 5.7%)
08/01 02:41:37 PM | Train: [ 2/210] Step 5140/6092 Loss 6.249 Prec@(1,5) (1.5%, 5.7%)
08/01 02:41:54 PM | Train: [ 2/210] Step 5150/6092 Loss 6.248 Prec@(1,5) (1.5%, 5.7%)
08/01 02:42:12 PM | Train: [ 2/210] Step 5160/6092 Loss 6.248 Prec@(1,5) (1.5%, 5.7%)
08/01 02:42:30 PM | Train: [ 2/210] Step 5170/6092 Loss 6.247 Prec@(1,5) (1.5%, 5.7%)
08/01 02:42:47 PM | Train: [ 2/210] Step 5180/6092 Loss 6.246 Prec@(1,5) (1.5%, 5.7%)
08/01 02:43:04 PM | Train: [ 2/210] Step 5190/6092 Loss 6.244 Prec@(1,5) (1.5%, 5.7%)
08/01 02:43:21 PM | Train: [ 2/210] Step 5200/6092 Loss 6.245 Prec@(1,5) (1.5%, 5.8%)
08/01 02:43:39 PM | Train: [ 2/210] Step 5210/6092 Loss 6.244 Prec@(1,5) (1.5%, 5.8%)
08/01 02:43:56 PM | Train: [ 2/210] Step 5220/6092 Loss 6.243 Prec@(1,5) (1.5%, 5.8%)
08/01 02:44:14 PM | Train: [ 2/210] Step 5230/6092 Loss 6.243 Prec@(1,5) (1.5%, 5.8%)
08/01 02:44:31 PM | Train: [ 2/210] Step 5240/6092 Loss 6.243 Prec@(1,5) (1.5%, 5.8%)
08/01 02:44:48 PM | Train: [ 2/210] Step 5250/6092 Loss 6.242 Prec@(1,5) (1.5%, 5.8%)
08/01 02:45:08 PM | Train: [ 2/210] Step 5260/6092 Loss 6.242 Prec@(1,5) (1.5%, 5.8%)
08/01 02:45:25 PM | Train: [ 2/210] Step 5270/6092 Loss 6.243 Prec@(1,5) (1.5%, 5.8%)
08/01 02:45:43 PM | Train: [ 2/210] Step 5280/6092 Loss 6.243 Prec@(1,5) (1.5%, 5.8%)
08/01 02:46:00 PM | Train: [ 2/210] Step 5290/6092 Loss 6.242 Prec@(1,5) (1.5%, 5.8%)
08/01 02:46:17 PM | Train: [ 2/210] Step 5300/6092 Loss 6.241 Prec@(1,5) (1.5%, 5.8%)
08/01 02:46:35 PM | Train: [ 2/210] Step 5310/6092 Loss 6.240 Prec@(1,5) (1.5%, 5.8%)
08/01 02:46:52 PM | Train: [ 2/210] Step 5320/6092 Loss 6.240 Prec@(1,5) (1.5%, 5.8%)
08/01 02:47:09 PM | Train: [ 2/210] Step 5330/6092 Loss 6.239 Prec@(1,5) (1.5%, 5.9%)
08/01 02:47:27 PM | Train: [ 2/210] Step 5340/6092 Loss 6.238 Prec@(1,5) (1.5%, 5.9%)
08/01 02:47:44 PM | Train: [ 2/210] Step 5350/6092 Loss 6.239 Prec@(1,5) (1.5%, 5.8%)
08/01 02:48:01 PM | Train: [ 2/210] Step 5360/6092 Loss 6.238 Prec@(1,5) (1.5%, 5.9%)
08/01 02:48:19 PM | Train: [ 2/210] Step 5370/6092 Loss 6.237 Prec@(1,5) (1.5%, 5.9%)
08/01 02:48:36 PM | Train: [ 2/210] Step 5380/6092 Loss 6.237 Prec@(1,5) (1.5%, 5.9%)
08/01 02:48:54 PM | Train: [ 2/210] Step 5390/6092 Loss 6.236 Prec@(1,5) (1.5%, 5.9%)
08/01 02:49:11 PM | Train: [ 2/210] Step 5400/6092 Loss 6.235 Prec@(1,5) (1.5%, 5.9%)
08/01 02:49:29 PM | Train: [ 2/210] Step 5410/6092 Loss 6.235 Prec@(1,5) (1.5%, 5.9%)
08/01 02:49:46 PM | Train: [ 2/210] Step 5420/6092 Loss 6.234 Prec@(1,5) (1.5%, 5.9%)
08/01 02:50:05 PM | Train: [ 2/210] Step 5430/6092 Loss 6.233 Prec@(1,5) (1.5%, 5.9%)
08/01 02:50:22 PM | Train: [ 2/210] Step 5440/6092 Loss 6.232 Prec@(1,5) (1.5%, 6.0%)
08/01 02:50:40 PM | Train: [ 2/210] Step 5450/6092 Loss 6.232 Prec@(1,5) (1.5%, 6.0%)
08/01 02:50:57 PM | Train: [ 2/210] Step 5460/6092 Loss 6.231 Prec@(1,5) (1.5%, 6.0%)
08/01 02:51:15 PM | Train: [ 2/210] Step 5470/6092 Loss 6.230 Prec@(1,5) (1.5%, 6.0%)
08/01 02:51:32 PM | Train: [ 2/210] Step 5480/6092 Loss 6.230 Prec@(1,5) (1.5%, 6.0%)
08/01 02:51:50 PM | Train: [ 2/210] Step 5490/6092 Loss 6.228 Prec@(1,5) (1.5%, 6.0%)
08/01 02:52:07 PM | Train: [ 2/210] Step 5500/6092 Loss 6.228 Prec@(1,5) (1.5%, 6.0%)
08/01 02:52:24 PM | Train: [ 2/210] Step 5510/6092 Loss 6.228 Prec@(1,5) (1.5%, 6.0%)
08/01 02:52:41 PM | Train: [ 2/210] Step 5520/6092 Loss 6.227 Prec@(1,5) (1.5%, 6.0%)
08/01 02:52:59 PM | Train: [ 2/210] Step 5530/6092 Loss 6.227 Prec@(1,5) (1.5%, 6.0%)
08/01 02:53:16 PM | Train: [ 2/210] Step 5540/6092 Loss 6.227 Prec@(1,5) (1.6%, 6.0%)
08/01 02:53:34 PM | Train: [ 2/210] Step 5550/6092 Loss 6.226 Prec@(1,5) (1.5%, 6.0%)
08/01 02:53:52 PM | Train: [ 2/210] Step 5560/6092 Loss 6.226 Prec@(1,5) (1.5%, 6.0%)
08/01 02:54:09 PM | Train: [ 2/210] Step 5570/6092 Loss 6.226 Prec@(1,5) (1.5%, 6.0%)
08/01 02:54:26 PM | Train: [ 2/210] Step 5580/6092 Loss 6.226 Prec@(1,5) (1.5%, 6.0%)
08/01 02:54:44 PM | Train: [ 2/210] Step 5590/6092 Loss 6.226 Prec@(1,5) (1.5%, 6.0%)
08/01 02:55:01 PM | Train: [ 2/210] Step 5600/6092 Loss 6.226 Prec@(1,5) (1.5%, 6.0%)
08/01 02:55:20 PM | Train: [ 2/210] Step 5610/6092 Loss 6.224 Prec@(1,5) (1.6%, 6.0%)
08/01 02:55:37 PM | Train: [ 2/210] Step 5620/6092 Loss 6.224 Prec@(1,5) (1.6%, 6.0%)
08/01 02:55:54 PM | Train: [ 2/210] Step 5630/6092 Loss 6.223 Prec@(1,5) (1.6%, 6.0%)
08/01 02:56:11 PM | Train: [ 2/210] Step 5640/6092 Loss 6.222 Prec@(1,5) (1.6%, 6.1%)
08/01 02:56:29 PM | Train: [ 2/210] Step 5650/6092 Loss 6.221 Prec@(1,5) (1.6%, 6.1%)
08/01 02:56:47 PM | Train: [ 2/210] Step 5660/6092 Loss 6.221 Prec@(1,5) (1.6%, 6.1%)
08/01 02:57:04 PM | Train: [ 2/210] Step 5670/6092 Loss 6.220 Prec@(1,5) (1.6%, 6.1%)
08/01 02:57:21 PM | Train: [ 2/210] Step 5680/6092 Loss 6.219 Prec@(1,5) (1.6%, 6.1%)
08/01 02:57:39 PM | Train: [ 2/210] Step 5690/6092 Loss 6.219 Prec@(1,5) (1.6%, 6.1%)
08/01 02:57:57 PM | Train: [ 2/210] Step 5700/6092 Loss 6.218 Prec@(1,5) (1.6%, 6.1%)
08/01 02:58:14 PM | Train: [ 2/210] Step 5710/6092 Loss 6.218 Prec@(1,5) (1.6%, 6.1%)
08/01 02:58:31 PM | Train: [ 2/210] Step 5720/6092 Loss 6.218 Prec@(1,5) (1.6%, 6.1%)
08/01 02:58:48 PM | Train: [ 2/210] Step 5730/6092 Loss 6.218 Prec@(1,5) (1.6%, 6.1%)
08/01 02:59:06 PM | Train: [ 2/210] Step 5740/6092 Loss 6.217 Prec@(1,5) (1.6%, 6.1%)
08/01 02:59:24 PM | Train: [ 2/210] Step 5750/6092 Loss 6.217 Prec@(1,5) (1.6%, 6.1%)
08/01 02:59:42 PM | Train: [ 2/210] Step 5760/6092 Loss 6.216 Prec@(1,5) (1.6%, 6.1%)
08/01 02:59:59 PM | Train: [ 2/210] Step 5770/6092 Loss 6.216 Prec@(1,5) (1.6%, 6.1%)
08/01 03:00:21 PM | Train: [ 2/210] Step 5780/6092 Loss 6.215 Prec@(1,5) (1.6%, 6.1%)
08/01 03:00:39 PM | Train: [ 2/210] Step 5790/6092 Loss 6.214 Prec@(1,5) (1.6%, 6.1%)
08/01 03:00:56 PM | Train: [ 2/210] Step 5800/6092 Loss 6.215 Prec@(1,5) (1.6%, 6.1%)
08/01 03:01:13 PM | Train: [ 2/210] Step 5810/6092 Loss 6.214 Prec@(1,5) (1.6%, 6.1%)
08/01 03:01:31 PM | Train: [ 2/210] Step 5820/6092 Loss 6.213 Prec@(1,5) (1.6%, 6.2%)
08/01 03:01:48 PM | Train: [ 2/210] Step 5830/6092 Loss 6.212 Prec@(1,5) (1.6%, 6.2%)
08/01 03:02:06 PM | Train: [ 2/210] Step 5840/6092 Loss 6.211 Prec@(1,5) (1.6%, 6.2%)
08/01 03:02:23 PM | Train: [ 2/210] Step 5850/6092 Loss 6.212 Prec@(1,5) (1.6%, 6.2%)
08/01 03:02:41 PM | Train: [ 2/210] Step 5860/6092 Loss 6.211 Prec@(1,5) (1.6%, 6.2%)
08/01 03:02:58 PM | Train: [ 2/210] Step 5870/6092 Loss 6.211 Prec@(1,5) (1.6%, 6.2%)
08/01 03:03:16 PM | Train: [ 2/210] Step 5880/6092 Loss 6.211 Prec@(1,5) (1.6%, 6.2%)
08/01 03:03:33 PM | Train: [ 2/210] Step 5890/6092 Loss 6.210 Prec@(1,5) (1.6%, 6.2%)
08/01 03:03:51 PM | Train: [ 2/210] Step 5900/6092 Loss 6.210 Prec@(1,5) (1.6%, 6.2%)
08/01 03:04:07 PM | Train: [ 2/210] Step 5910/6092 Loss 6.210 Prec@(1,5) (1.6%, 6.2%)
08/01 03:04:25 PM | Train: [ 2/210] Step 5920/6092 Loss 6.209 Prec@(1,5) (1.6%, 6.2%)
08/01 03:04:42 PM | Train: [ 2/210] Step 5930/6092 Loss 6.208 Prec@(1,5) (1.6%, 6.2%)
08/01 03:05:00 PM | Train: [ 2/210] Step 5940/6092 Loss 6.208 Prec@(1,5) (1.6%, 6.2%)
08/01 03:05:18 PM | Train: [ 2/210] Step 5950/6092 Loss 6.208 Prec@(1,5) (1.6%, 6.2%)
08/01 03:05:35 PM | Train: [ 2/210] Step 5960/6092 Loss 6.207 Prec@(1,5) (1.6%, 6.2%)
08/01 03:05:53 PM | Train: [ 2/210] Step 5970/6092 Loss 6.207 Prec@(1,5) (1.6%, 6.2%)
08/01 03:06:10 PM | Train: [ 2/210] Step 5980/6092 Loss 6.206 Prec@(1,5) (1.6%, 6.2%)
08/01 03:06:28 PM | Train: [ 2/210] Step 5990/6092 Loss 6.207 Prec@(1,5) (1.6%, 6.2%)
08/01 03:06:46 PM | Train: [ 2/210] Step 6000/6092 Loss 6.206 Prec@(1,5) (1.6%, 6.2%)
08/01 03:07:03 PM | Train: [ 2/210] Step 6010/6092 Loss 6.205 Prec@(1,5) (1.6%, 6.2%)
08/01 03:07:20 PM | Train: [ 2/210] Step 6020/6092 Loss 6.205 Prec@(1,5) (1.6%, 6.2%)
08/01 03:07:37 PM | Train: [ 2/210] Step 6030/6092 Loss 6.204 Prec@(1,5) (1.6%, 6.2%)
08/01 03:07:55 PM | Train: [ 2/210] Step 6040/6092 Loss 6.204 Prec@(1,5) (1.6%, 6.2%)
08/01 03:08:13 PM | Train: [ 2/210] Step 6050/6092 Loss 6.204 Prec@(1,5) (1.6%, 6.2%)
08/01 03:08:32 PM | Train: [ 2/210] Step 6060/6092 Loss 6.203 Prec@(1,5) (1.6%, 6.2%)
08/01 03:08:50 PM | Train: [ 2/210] Step 6070/6092 Loss 6.203 Prec@(1,5) (1.6%, 6.2%)
08/01 03:09:08 PM | Train: [ 2/210] Step 6080/6092 Loss 6.202 Prec@(1,5) (1.6%, 6.2%)
08/01 03:09:26 PM | Train: [ 2/210] Step 6090/6092 Loss 6.201 Prec@(1,5) (1.6%, 6.2%)
08/01 03:09:30 PM | Train: [ 2/210] Step 6092/6092 Loss 6.201 Prec@(1,5) (1.6%, 6.2%)
08/01 03:09:34 PM | Train: [ 2/210] Final Prec@1 1.6414%
08/01 03:12:05 PM | Valid: [ 2/210] Step 000/375 Loss 5.828 Prec@(1,5) (0.0%, 0.0%)
08/01 03:12:06 PM | Valid: [ 2/210] Step 010/375 Loss 8.531 Prec@(1,5) (0.0%, 9.1%)
08/01 03:12:07 PM | Valid: [ 2/210] Step 020/375 Loss 8.576 Prec@(1,5) (0.0%, 4.8%)
08/01 03:12:08 PM | Valid: [ 2/210] Step 030/375 Loss 8.364 Prec@(1,5) (0.0%, 4.8%)
08/01 03:12:08 PM | Valid: [ 2/210] Step 040/375 Loss 7.992 Prec@(1,5) (0.0%, 8.5%)
08/01 03:12:09 PM | Valid: [ 2/210] Step 050/375 Loss 7.810 Prec@(1,5) (0.0%, 7.8%)
08/01 03:12:10 PM | Valid: [ 2/210] Step 060/375 Loss 7.604 Prec@(1,5) (0.8%, 7.4%)
08/01 03:12:11 PM | Valid: [ 2/210] Step 070/375 Loss 7.398 Prec@(1,5) (1.4%, 9.2%)
08/01 03:12:12 PM | Valid: [ 2/210] Step 080/375 Loss 7.210 Prec@(1,5) (1.2%, 9.3%)
08/01 03:12:13 PM | Valid: [ 2/210] Step 090/375 Loss 7.280 Prec@(1,5) (1.1%, 9.3%)
08/01 03:12:14 PM | Valid: [ 2/210] Step 100/375 Loss 7.301 Prec@(1,5) (1.0%, 8.9%)
08/01 03:12:14 PM | Valid: [ 2/210] Step 110/375 Loss 7.301 Prec@(1,5) (1.4%, 9.0%)
08/01 03:12:15 PM | Valid: [ 2/210] Step 120/375 Loss 7.284 Prec@(1,5) (1.2%, 8.7%)
08/01 03:12:16 PM | Valid: [ 2/210] Step 130/375 Loss 7.308 Prec@(1,5) (1.1%, 8.4%)
08/01 03:12:17 PM | Valid: [ 2/210] Step 140/375 Loss 7.432 Prec@(1,5) (1.4%, 8.5%)
08/01 03:12:18 PM | Valid: [ 2/210] Step 150/375 Loss 7.451 Prec@(1,5) (1.3%, 7.9%)
08/01 03:12:19 PM | Valid: [ 2/210] Step 160/375 Loss 7.444 Prec@(1,5) (1.6%, 7.8%)
08/01 03:12:20 PM | Valid: [ 2/210] Step 170/375 Loss 7.438 Prec@(1,5) (1.5%, 7.6%)
08/01 03:12:21 PM | Valid: [ 2/210] Step 180/375 Loss 7.429 Prec@(1,5) (1.4%, 7.2%)
08/01 03:12:22 PM | Valid: [ 2/210] Step 190/375 Loss 7.380 Prec@(1,5) (1.6%, 7.3%)
08/01 03:12:23 PM | Valid: [ 2/210] Step 200/375 Loss 7.357 Prec@(1,5) (1.7%, 7.2%)
08/01 03:12:24 PM | Valid: [ 2/210] Step 210/375 Loss 7.412 Prec@(1,5) (1.7%, 6.9%)
08/01 03:12:25 PM | Valid: [ 2/210] Step 220/375 Loss 7.421 Prec@(1,5) (1.6%, 6.6%)
08/01 03:12:26 PM | Valid: [ 2/210] Step 230/375 Loss 7.399 Prec@(1,5) (1.5%, 6.5%)
08/01 03:12:27 PM | Valid: [ 2/210] Step 240/375 Loss 7.426 Prec@(1,5) (1.5%, 6.4%)
08/01 03:12:28 PM | Valid: [ 2/210] Step 250/375 Loss 7.434 Prec@(1,5) (1.4%, 6.4%)
08/01 03:12:29 PM | Valid: [ 2/210] Step 260/375 Loss 7.412 Prec@(1,5) (1.3%, 6.5%)
08/01 03:12:29 PM | Valid: [ 2/210] Step 270/375 Loss 7.390 Prec@(1,5) (1.3%, 6.3%)
08/01 03:12:30 PM | Valid: [ 2/210] Step 280/375 Loss 7.368 Prec@(1,5) (1.6%, 6.6%)
08/01 03:12:31 PM | Valid: [ 2/210] Step 290/375 Loss 7.440 Prec@(1,5) (1.5%, 6.5%)
08/01 03:12:32 PM | Valid: [ 2/210] Step 300/375 Loss 7.435 Prec@(1,5) (1.5%, 6.5%)
08/01 03:12:33 PM | Valid: [ 2/210] Step 310/375 Loss 7.450 Prec@(1,5) (1.4%, 6.4%)
08/01 03:12:34 PM | Valid: [ 2/210] Step 320/375 Loss 7.436 Prec@(1,5) (1.7%, 6.7%)
08/01 03:12:35 PM | Valid: [ 2/210] Step 330/375 Loss 7.438 Prec@(1,5) (1.7%, 6.6%)
08/01 03:12:36 PM | Valid: [ 2/210] Step 340/375 Loss 7.404 Prec@(1,5) (1.8%, 6.9%)
08/01 03:12:37 PM | Valid: [ 2/210] Step 350/375 Loss 7.411 Prec@(1,5) (1.9%, 6.8%)
08/01 03:12:37 PM | Valid: [ 2/210] Step 360/375 Loss 7.455 Prec@(1,5) (1.8%, 6.6%)
08/01 03:12:38 PM | Valid: [ 2/210] Step 370/375 Loss 7.425 Prec@(1,5) (1.8%, 6.6%)
08/01 03:12:41 PM | Valid: [ 2/210] Final Prec@1 1.7333%, Prec@5 6.5333%, Prec@10 12.2667%
08/01 03:12:42 PM | Final best Prec@1 = 1.7333%
####### ALPHA #######
# Alpha - normal
tensor([[0.1308, 0.0629, 0.0974, 0.0574, 0.0361, 0.0863, 0.4091, 0.1200],
        [0.6051, 0.0984, 0.1675, 0.0337, 0.0365, 0.0101, 0.0316, 0.0171]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1604, 0.0634, 0.1053, 0.0326, 0.0395, 0.0475, 0.0206, 0.5307],
        [0.3464, 0.1426, 0.2839, 0.0180, 0.0429, 0.1052, 0.0216, 0.0394],
        [0.2907, 0.1165, 0.0901, 0.0919, 0.1371, 0.0611, 0.1385, 0.0741]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0555, 0.0398, 0.0434, 0.0987, 0.1477, 0.3626, 0.1779, 0.0744],
        [0.1663, 0.0752, 0.1029, 0.0701, 0.0746, 0.0502, 0.3268, 0.1340],
        [0.7194, 0.0494, 0.0456, 0.0536, 0.0509, 0.0211, 0.0199, 0.0401],
        [0.6106, 0.0620, 0.0337, 0.0662, 0.0545, 0.0532, 0.0345, 0.0854]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.0823, 0.0922, 0.0503, 0.1484, 0.0619, 0.2429, 0.1970],
        [0.3284, 0.0817, 0.1326, 0.1290, 0.0479, 0.1516, 0.0707, 0.0582],
        [0.7353, 0.0449, 0.0379, 0.0447, 0.0275, 0.0234, 0.0596, 0.0268],
        [0.0863, 0.0471, 0.0476, 0.0336, 0.0474, 0.0265, 0.6793, 0.0321],
        [0.1563, 0.0502, 0.5805, 0.0290, 0.0413, 0.0319, 0.0820, 0.0288]],
       grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.2332, 0.0690, 0.2400, 0.1372, 0.1025, 0.0313, 0.0333, 0.1535],
        [0.1430, 0.0437, 0.0959, 0.0989, 0.0944, 0.1074, 0.0645, 0.3522]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.4522, 0.0435, 0.0942, 0.0598, 0.0594, 0.0835, 0.0934, 0.1141],
        [0.1843, 0.0550, 0.0999, 0.0707, 0.2064, 0.0739, 0.0149, 0.2950],
        [0.0955, 0.0165, 0.0213, 0.0917, 0.0504, 0.0331, 0.0374, 0.6541]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.2689, 0.0501, 0.0735, 0.0799, 0.0852, 0.0418, 0.0547, 0.3458],
        [0.3457, 0.0532, 0.0960, 0.0702, 0.0619, 0.0508, 0.0816, 0.2406],
        [0.1588, 0.0226, 0.0308, 0.1749, 0.2255, 0.0345, 0.0410, 0.3119],
        [0.0655, 0.0229, 0.0375, 0.0893, 0.1610, 0.0184, 0.0408, 0.5647]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.3847, 0.0338, 0.1178, 0.0324, 0.1201, 0.0714, 0.0245, 0.2151],
        [0.3266, 0.0797, 0.0500, 0.0353, 0.0986, 0.0339, 0.0989, 0.2769],
        [0.0607, 0.0255, 0.0386, 0.0176, 0.0433, 0.0213, 0.0393, 0.7537],
        [0.1151, 0.0260, 0.0389, 0.0441, 0.1372, 0.0198, 0.0909, 0.5279],
        [0.0314, 0.0139, 0.0194, 0.0689, 0.0511, 0.0902, 0.0563, 0.6688]],
       grad_fn=<SoftmaxBackward>)
#####################
08/01 03:16:07 PM | Train: [ 3/210] Step 000/6092 Loss 5.445 Prec@(1,5) (0.0%, 0.0%)
08/01 03:16:23 PM | Train: [ 3/210] Step 010/6092 Loss 5.789 Prec@(1,5) (0.0%, 4.5%)
08/01 03:16:40 PM | Train: [ 3/210] Step 020/6092 Loss 5.716 Prec@(1,5) (2.4%, 4.8%)
08/01 03:16:57 PM | Train: [ 3/210] Step 030/6092 Loss 5.731 Prec@(1,5) (3.2%, 4.8%)
08/01 03:17:14 PM | Train: [ 3/210] Step 040/6092 Loss 5.705 Prec@(1,5) (2.4%, 4.9%)
08/01 03:17:31 PM | Train: [ 3/210] Step 050/6092 Loss 5.693 Prec@(1,5) (2.9%, 4.9%)
08/01 03:17:48 PM | Train: [ 3/210] Step 060/6092 Loss 5.737 Prec@(1,5) (3.3%, 8.2%)
08/01 03:18:04 PM | Train: [ 3/210] Step 070/6092 Loss 5.755 Prec@(1,5) (2.8%, 7.7%)
08/01 03:18:22 PM | Train: [ 3/210] Step 080/6092 Loss 5.766 Prec@(1,5) (2.5%, 8.0%)
08/01 03:18:40 PM | Train: [ 3/210] Step 090/6092 Loss 5.792 Prec@(1,5) (2.2%, 7.1%)
08/01 03:18:57 PM | Train: [ 3/210] Step 100/6092 Loss 5.820 Prec@(1,5) (2.5%, 6.9%)
08/01 03:19:14 PM | Train: [ 3/210] Step 110/6092 Loss 5.801 Prec@(1,5) (2.3%, 7.2%)
08/01 03:19:32 PM | Train: [ 3/210] Step 120/6092 Loss 5.799 Prec@(1,5) (2.1%, 8.3%)
08/01 03:19:49 PM | Train: [ 3/210] Step 130/6092 Loss 5.791 Prec@(1,5) (1.9%, 8.8%)
08/01 03:20:07 PM | Train: [ 3/210] Step 140/6092 Loss 5.820 Prec@(1,5) (1.8%, 8.9%)
08/01 03:20:28 PM | Train: [ 3/210] Step 150/6092 Loss 5.846 Prec@(1,5) (2.0%, 8.9%)
08/01 03:20:45 PM | Train: [ 3/210] Step 160/6092 Loss 5.864 Prec@(1,5) (1.9%, 8.7%)
08/01 03:21:03 PM | Train: [ 3/210] Step 170/6092 Loss 5.829 Prec@(1,5) (2.3%, 9.4%)
08/01 03:21:21 PM | Train: [ 3/210] Step 180/6092 Loss 5.830 Prec@(1,5) (2.2%, 9.4%)
08/01 03:21:38 PM | Train: [ 3/210] Step 190/6092 Loss 5.835 Prec@(1,5) (2.1%, 8.9%)
08/01 03:21:56 PM | Train: [ 3/210] Step 200/6092 Loss 5.811 Prec@(1,5) (2.2%, 9.7%)
08/01 03:22:14 PM | Train: [ 3/210] Step 210/6092 Loss 5.807 Prec@(1,5) (2.4%, 9.5%)
08/01 03:22:33 PM | Train: [ 3/210] Step 220/6092 Loss 5.808 Prec@(1,5) (2.3%, 9.0%)
08/01 03:22:52 PM | Train: [ 3/210] Step 230/6092 Loss 5.794 Prec@(1,5) (2.6%, 9.1%)
08/01 03:23:09 PM | Train: [ 3/210] Step 240/6092 Loss 5.816 Prec@(1,5) (2.7%, 8.9%)
08/01 03:23:28 PM | Train: [ 3/210] Step 250/6092 Loss 5.807 Prec@(1,5) (2.6%, 9.2%)
08/01 03:23:46 PM | Train: [ 3/210] Step 260/6092 Loss 5.788 Prec@(1,5) (3.1%, 10.0%)
08/01 03:24:03 PM | Train: [ 3/210] Step 270/6092 Loss 5.771 Prec@(1,5) (3.3%, 10.3%)
08/01 03:24:21 PM | Train: [ 3/210] Step 280/6092 Loss 5.767 Prec@(1,5) (3.4%, 10.7%)
08/01 03:24:39 PM | Train: [ 3/210] Step 290/6092 Loss 5.787 Prec@(1,5) (3.3%, 10.5%)
08/01 03:24:56 PM | Train: [ 3/210] Step 300/6092 Loss 5.786 Prec@(1,5) (3.5%, 10.6%)
08/01 03:25:15 PM | Train: [ 3/210] Step 310/6092 Loss 5.775 Prec@(1,5) (3.5%, 11.1%)
08/01 03:25:33 PM | Train: [ 3/210] Step 320/6092 Loss 5.792 Prec@(1,5) (3.4%, 10.9%)
08/01 03:25:50 PM | Train: [ 3/210] Step 330/6092 Loss 5.785 Prec@(1,5) (3.5%, 11.0%)
08/01 03:26:08 PM | Train: [ 3/210] Step 340/6092 Loss 5.776 Prec@(1,5) (3.5%, 11.1%)
08/01 03:26:26 PM | Train: [ 3/210] Step 350/6092 Loss 5.777 Prec@(1,5) (3.4%, 11.1%)
08/01 03:26:43 PM | Train: [ 3/210] Step 360/6092 Loss 5.765 Prec@(1,5) (3.3%, 11.6%)
08/01 03:27:01 PM | Train: [ 3/210] Step 370/6092 Loss 5.775 Prec@(1,5) (3.2%, 11.6%)
08/01 03:27:19 PM | Train: [ 3/210] Step 380/6092 Loss 5.764 Prec@(1,5) (3.3%, 11.7%)
08/01 03:27:37 PM | Train: [ 3/210] Step 390/6092 Loss 5.765 Prec@(1,5) (3.2%, 11.5%)
08/01 03:27:55 PM | Train: [ 3/210] Step 400/6092 Loss 5.773 Prec@(1,5) (3.1%, 11.3%)
08/01 03:28:12 PM | Train: [ 3/210] Step 410/6092 Loss 5.780 Prec@(1,5) (3.2%, 11.3%)
08/01 03:28:30 PM | Train: [ 3/210] Step 420/6092 Loss 5.782 Prec@(1,5) (3.1%, 11.2%)
08/01 03:28:48 PM | Train: [ 3/210] Step 430/6092 Loss 5.783 Prec@(1,5) (3.0%, 11.0%)
08/01 03:29:05 PM | Train: [ 3/210] Step 440/6092 Loss 5.783 Prec@(1,5) (3.1%, 11.0%)
08/01 03:29:23 PM | Train: [ 3/210] Step 450/6092 Loss 5.779 Prec@(1,5) (3.1%, 11.1%)
08/01 03:29:41 PM | Train: [ 3/210] Step 460/6092 Loss 5.774 Prec@(1,5) (3.1%, 11.1%)
08/01 03:29:58 PM | Train: [ 3/210] Step 470/6092 Loss 5.766 Prec@(1,5) (3.2%, 11.0%)
08/01 03:30:18 PM | Train: [ 3/210] Step 480/6092 Loss 5.757 Prec@(1,5) (3.2%, 11.4%)
08/01 03:30:35 PM | Train: [ 3/210] Step 490/6092 Loss 5.763 Prec@(1,5) (3.2%, 11.2%)
08/01 03:30:53 PM | Train: [ 3/210] Step 500/6092 Loss 5.766 Prec@(1,5) (3.1%, 11.2%)
08/01 03:31:10 PM | Train: [ 3/210] Step 510/6092 Loss 5.769 Prec@(1,5) (3.1%, 11.1%)
08/01 03:31:28 PM | Train: [ 3/210] Step 520/6092 Loss 5.768 Prec@(1,5) (3.1%, 11.0%)
08/01 03:31:46 PM | Train: [ 3/210] Step 530/6092 Loss 5.774 Prec@(1,5) (3.0%, 11.0%)
08/01 03:32:04 PM | Train: [ 3/210] Step 540/6092 Loss 5.780 Prec@(1,5) (3.0%, 10.9%)
08/01 03:32:21 PM | Train: [ 3/210] Step 550/6092 Loss 5.785 Prec@(1,5) (2.9%, 10.8%)
08/01 03:32:39 PM | Train: [ 3/210] Step 560/6092 Loss 5.782 Prec@(1,5) (2.9%, 10.9%)
08/01 03:32:57 PM | Train: [ 3/210] Step 570/6092 Loss 5.779 Prec@(1,5) (3.0%, 10.9%)
08/01 03:33:14 PM | Train: [ 3/210] Step 580/6092 Loss 5.767 Prec@(1,5) (3.1%, 11.1%)
08/01 03:33:32 PM | Train: [ 3/210] Step 590/6092 Loss 5.762 Prec@(1,5) (3.1%, 11.2%)
08/01 03:33:50 PM | Train: [ 3/210] Step 600/6092 Loss 5.763 Prec@(1,5) (3.1%, 11.1%)
08/01 03:34:07 PM | Train: [ 3/210] Step 610/6092 Loss 5.767 Prec@(1,5) (3.2%, 11.2%)
08/01 03:34:25 PM | Train: [ 3/210] Step 620/6092 Loss 5.764 Prec@(1,5) (3.2%, 11.2%)
08/01 03:34:42 PM | Train: [ 3/210] Step 630/6092 Loss 5.766 Prec@(1,5) (3.2%, 11.2%)
08/01 03:35:00 PM | Train: [ 3/210] Step 640/6092 Loss 5.767 Prec@(1,5) (3.3%, 11.1%)
08/01 03:35:19 PM | Train: [ 3/210] Step 650/6092 Loss 5.769 Prec@(1,5) (3.3%, 11.1%)
08/01 03:35:37 PM | Train: [ 3/210] Step 660/6092 Loss 5.761 Prec@(1,5) (3.3%, 11.3%)
08/01 03:35:55 PM | Train: [ 3/210] Step 670/6092 Loss 5.757 Prec@(1,5) (3.4%, 11.5%)
08/01 03:36:12 PM | Train: [ 3/210] Step 680/6092 Loss 5.757 Prec@(1,5) (3.4%, 11.5%)
08/01 03:36:30 PM | Train: [ 3/210] Step 690/6092 Loss 5.755 Prec@(1,5) (3.4%, 11.5%)
08/01 03:36:47 PM | Train: [ 3/210] Step 700/6092 Loss 5.750 Prec@(1,5) (3.4%, 11.6%)
08/01 03:37:05 PM | Train: [ 3/210] Step 710/6092 Loss 5.753 Prec@(1,5) (3.4%, 11.5%)
08/01 03:37:23 PM | Train: [ 3/210] Step 720/6092 Loss 5.746 Prec@(1,5) (3.4%, 11.4%)
08/01 03:37:42 PM | Train: [ 3/210] Step 730/6092 Loss 5.744 Prec@(1,5) (3.4%, 11.4%)
08/01 03:38:01 PM | Train: [ 3/210] Step 740/6092 Loss 5.739 Prec@(1,5) (3.4%, 11.5%)
08/01 03:38:18 PM | Train: [ 3/210] Step 750/6092 Loss 5.740 Prec@(1,5) (3.4%, 11.5%)
08/01 03:38:36 PM | Train: [ 3/210] Step 760/6092 Loss 5.747 Prec@(1,5) (3.4%, 11.3%)
08/01 03:38:54 PM | Train: [ 3/210] Step 770/6092 Loss 5.753 Prec@(1,5) (3.3%, 11.3%)
08/01 03:39:12 PM | Train: [ 3/210] Step 780/6092 Loss 5.750 Prec@(1,5) (3.3%, 11.4%)
08/01 03:39:29 PM | Train: [ 3/210] Step 790/6092 Loss 5.750 Prec@(1,5) (3.4%, 11.3%)
08/01 03:39:47 PM | Train: [ 3/210] Step 800/6092 Loss 5.748 Prec@(1,5) (3.3%, 11.4%)
08/01 03:40:06 PM | Train: [ 3/210] Step 810/6092 Loss 5.748 Prec@(1,5) (3.3%, 11.3%)
08/01 03:40:25 PM | Train: [ 3/210] Step 820/6092 Loss 5.749 Prec@(1,5) (3.3%, 11.3%)
08/01 03:40:42 PM | Train: [ 3/210] Step 830/6092 Loss 5.745 Prec@(1,5) (3.5%, 11.4%)
08/01 03:41:00 PM | Train: [ 3/210] Step 840/6092 Loss 5.745 Prec@(1,5) (3.4%, 11.4%)
08/01 03:41:17 PM | Train: [ 3/210] Step 850/6092 Loss 5.746 Prec@(1,5) (3.5%, 11.3%)
08/01 03:41:35 PM | Train: [ 3/210] Step 860/6092 Loss 5.745 Prec@(1,5) (3.5%, 11.3%)
08/01 03:41:52 PM | Train: [ 3/210] Step 870/6092 Loss 5.744 Prec@(1,5) (3.5%, 11.4%)
08/01 03:42:10 PM | Train: [ 3/210] Step 880/6092 Loss 5.741 Prec@(1,5) (3.5%, 11.4%)
08/01 03:42:28 PM | Train: [ 3/210] Step 890/6092 Loss 5.742 Prec@(1,5) (3.4%, 11.3%)
08/01 03:42:45 PM | Train: [ 3/210] Step 900/6092 Loss 5.742 Prec@(1,5) (3.4%, 11.4%)
08/01 03:43:03 PM | Train: [ 3/210] Step 910/6092 Loss 5.740 Prec@(1,5) (3.4%, 11.4%)
08/01 03:43:21 PM | Train: [ 3/210] Step 920/6092 Loss 5.743 Prec@(1,5) (3.4%, 11.3%)
08/01 03:43:38 PM | Train: [ 3/210] Step 930/6092 Loss 5.738 Prec@(1,5) (3.4%, 11.3%)
08/01 03:43:57 PM | Train: [ 3/210] Step 940/6092 Loss 5.738 Prec@(1,5) (3.4%, 11.4%)
08/01 03:44:14 PM | Train: [ 3/210] Step 950/6092 Loss 5.733 Prec@(1,5) (3.5%, 11.4%)
08/01 03:44:32 PM | Train: [ 3/210] Step 960/6092 Loss 5.736 Prec@(1,5) (3.5%, 11.3%)
08/01 03:44:49 PM | Train: [ 3/210] Step 970/6092 Loss 5.736 Prec@(1,5) (3.5%, 11.4%)
08/01 03:45:09 PM | Train: [ 3/210] Step 980/6092 Loss 5.739 Prec@(1,5) (3.5%, 11.3%)
08/01 03:45:27 PM | Train: [ 3/210] Step 990/6092 Loss 5.733 Prec@(1,5) (3.6%, 11.5%)
08/01 03:45:44 PM | Train: [ 3/210] Step 1000/6092 Loss 5.738 Prec@(1,5) (3.5%, 11.3%)
08/01 03:46:01 PM | Train: [ 3/210] Step 1010/6092 Loss 5.736 Prec@(1,5) (3.6%, 11.4%)
08/01 03:46:19 PM | Train: [ 3/210] Step 1020/6092 Loss 5.738 Prec@(1,5) (3.6%, 11.5%)
08/01 03:46:36 PM | Train: [ 3/210] Step 1030/6092 Loss 5.742 Prec@(1,5) (3.5%, 11.4%)
08/01 03:46:54 PM | Train: [ 3/210] Step 1040/6092 Loss 5.741 Prec@(1,5) (3.6%, 11.5%)
08/01 03:47:12 PM | Train: [ 3/210] Step 1050/6092 Loss 5.741 Prec@(1,5) (3.6%, 11.5%)
08/01 03:47:29 PM | Train: [ 3/210] Step 1060/6092 Loss 5.741 Prec@(1,5) (3.6%, 11.6%)
08/01 03:47:47 PM | Train: [ 3/210] Step 1070/6092 Loss 5.744 Prec@(1,5) (3.6%, 11.6%)
08/01 03:48:04 PM | Train: [ 3/210] Step 1080/6092 Loss 5.740 Prec@(1,5) (3.7%, 11.7%)
08/01 03:48:22 PM | Train: [ 3/210] Step 1090/6092 Loss 5.740 Prec@(1,5) (3.7%, 11.6%)
08/01 03:48:40 PM | Train: [ 3/210] Step 1100/6092 Loss 5.743 Prec@(1,5) (3.6%, 11.6%)
08/01 03:48:57 PM | Train: [ 3/210] Step 1110/6092 Loss 5.741 Prec@(1,5) (3.6%, 11.6%)
08/01 03:49:15 PM | Train: [ 3/210] Step 1120/6092 Loss 5.744 Prec@(1,5) (3.6%, 11.6%)
08/01 03:49:33 PM | Train: [ 3/210] Step 1130/6092 Loss 5.744 Prec@(1,5) (3.6%, 11.6%)
08/01 03:49:50 PM | Train: [ 3/210] Step 1140/6092 Loss 5.745 Prec@(1,5) (3.5%, 11.5%)
08/01 03:50:08 PM | Train: [ 3/210] Step 1150/6092 Loss 5.748 Prec@(1,5) (3.5%, 11.4%)
08/01 03:50:27 PM | Train: [ 3/210] Step 1160/6092 Loss 5.752 Prec@(1,5) (3.5%, 11.3%)
08/01 03:50:45 PM | Train: [ 3/210] Step 1170/6092 Loss 5.757 Prec@(1,5) (3.5%, 11.2%)
08/01 03:51:02 PM | Train: [ 3/210] Step 1180/6092 Loss 5.757 Prec@(1,5) (3.5%, 11.2%)
08/01 03:51:20 PM | Train: [ 3/210] Step 1190/6092 Loss 5.759 Prec@(1,5) (3.4%, 11.1%)
08/01 03:51:38 PM | Train: [ 3/210] Step 1200/6092 Loss 5.754 Prec@(1,5) (3.5%, 11.2%)
08/01 03:51:55 PM | Train: [ 3/210] Step 1210/6092 Loss 5.756 Prec@(1,5) (3.5%, 11.1%)
08/01 03:52:13 PM | Train: [ 3/210] Step 1220/6092 Loss 5.753 Prec@(1,5) (3.5%, 11.1%)
08/01 03:52:30 PM | Train: [ 3/210] Step 1230/6092 Loss 5.754 Prec@(1,5) (3.5%, 11.2%)
08/01 03:52:48 PM | Train: [ 3/210] Step 1240/6092 Loss 5.751 Prec@(1,5) (3.5%, 11.2%)
08/01 03:53:06 PM | Train: [ 3/210] Step 1250/6092 Loss 5.750 Prec@(1,5) (3.5%, 11.3%)
08/01 03:53:25 PM | Train: [ 3/210] Step 1260/6092 Loss 5.752 Prec@(1,5) (3.5%, 11.3%)
08/01 03:53:43 PM | Train: [ 3/210] Step 1270/6092 Loss 5.752 Prec@(1,5) (3.5%, 11.3%)
08/01 03:54:00 PM | Train: [ 3/210] Step 1280/6092 Loss 5.753 Prec@(1,5) (3.5%, 11.2%)
08/01 03:54:19 PM | Train: [ 3/210] Step 1290/6092 Loss 5.747 Prec@(1,5) (3.5%, 11.4%)
08/01 03:54:37 PM | Train: [ 3/210] Step 1300/6092 Loss 5.745 Prec@(1,5) (3.5%, 11.5%)
08/01 03:54:54 PM | Train: [ 3/210] Step 1310/6092 Loss 5.745 Prec@(1,5) (3.4%, 11.5%)
08/01 03:55:16 PM | Train: [ 3/210] Step 1320/6092 Loss 5.744 Prec@(1,5) (3.4%, 11.5%)
08/01 03:55:33 PM | Train: [ 3/210] Step 1330/6092 Loss 5.744 Prec@(1,5) (3.5%, 11.5%)
08/01 03:55:52 PM | Train: [ 3/210] Step 1340/6092 Loss 5.746 Prec@(1,5) (3.4%, 11.4%)
08/01 03:56:10 PM | Train: [ 3/210] Step 1350/6092 Loss 5.748 Prec@(1,5) (3.4%, 11.4%)
08/01 03:56:27 PM | Train: [ 3/210] Step 1360/6092 Loss 5.747 Prec@(1,5) (3.5%, 11.4%)
08/01 03:56:45 PM | Train: [ 3/210] Step 1370/6092 Loss 5.749 Prec@(1,5) (3.4%, 11.4%)
08/01 03:57:03 PM | Train: [ 3/210] Step 1380/6092 Loss 5.753 Prec@(1,5) (3.4%, 11.3%)
08/01 03:57:20 PM | Train: [ 3/210] Step 1390/6092 Loss 5.754 Prec@(1,5) (3.4%, 11.4%)
08/01 03:57:38 PM | Train: [ 3/210] Step 1400/6092 Loss 5.755 Prec@(1,5) (3.4%, 11.4%)
08/01 03:57:55 PM | Train: [ 3/210] Step 1410/6092 Loss 5.756 Prec@(1,5) (3.4%, 11.4%)
08/01 03:58:13 PM | Train: [ 3/210] Step 1420/6092 Loss 5.753 Prec@(1,5) (3.4%, 11.3%)
08/01 03:58:30 PM | Train: [ 3/210] Step 1430/6092 Loss 5.753 Prec@(1,5) (3.4%, 11.4%)
08/01 03:58:48 PM | Train: [ 3/210] Step 1440/6092 Loss 5.752 Prec@(1,5) (3.4%, 11.3%)
08/01 03:59:05 PM | Train: [ 3/210] Step 1450/6092 Loss 5.750 Prec@(1,5) (3.4%, 11.4%)
08/01 03:59:23 PM | Train: [ 3/210] Step 1460/6092 Loss 5.750 Prec@(1,5) (3.4%, 11.4%)
08/01 03:59:41 PM | Train: [ 3/210] Step 1470/6092 Loss 5.749 Prec@(1,5) (3.5%, 11.4%)
08/01 03:59:58 PM | Train: [ 3/210] Step 1480/6092 Loss 5.748 Prec@(1,5) (3.5%, 11.4%)
08/01 04:00:16 PM | Train: [ 3/210] Step 1490/6092 Loss 5.745 Prec@(1,5) (3.6%, 11.5%)
08/01 04:00:35 PM | Train: [ 3/210] Step 1500/6092 Loss 5.744 Prec@(1,5) (3.6%, 11.5%)
08/01 04:00:52 PM | Train: [ 3/210] Step 1510/6092 Loss 5.743 Prec@(1,5) (3.6%, 11.5%)
08/01 04:01:09 PM | Train: [ 3/210] Step 1520/6092 Loss 5.737 Prec@(1,5) (3.6%, 11.6%)
08/01 04:01:27 PM | Train: [ 3/210] Step 1530/6092 Loss 5.738 Prec@(1,5) (3.6%, 11.6%)
08/01 04:01:44 PM | Train: [ 3/210] Step 1540/6092 Loss 5.738 Prec@(1,5) (3.6%, 11.6%)
08/01 04:02:01 PM | Train: [ 3/210] Step 1550/6092 Loss 5.739 Prec@(1,5) (3.6%, 11.7%)
08/01 04:02:19 PM | Train: [ 3/210] Step 1560/6092 Loss 5.740 Prec@(1,5) (3.7%, 11.7%)
08/01 04:02:36 PM | Train: [ 3/210] Step 1570/6092 Loss 5.742 Prec@(1,5) (3.7%, 11.7%)
08/01 04:02:54 PM | Train: [ 3/210] Step 1580/6092 Loss 5.743 Prec@(1,5) (3.7%, 11.7%)
08/01 04:03:11 PM | Train: [ 3/210] Step 1590/6092 Loss 5.741 Prec@(1,5) (3.6%, 11.7%)
08/01 04:03:28 PM | Train: [ 3/210] Step 1600/6092 Loss 5.741 Prec@(1,5) (3.7%, 11.7%)
08/01 04:03:46 PM | Train: [ 3/210] Step 1610/6092 Loss 5.739 Prec@(1,5) (3.6%, 11.8%)
08/01 04:04:06 PM | Train: [ 3/210] Step 1620/6092 Loss 5.740 Prec@(1,5) (3.6%, 11.7%)
08/01 04:04:23 PM | Train: [ 3/210] Step 1630/6092 Loss 5.739 Prec@(1,5) (3.6%, 11.7%)
08/01 04:04:41 PM | Train: [ 3/210] Step 1640/6092 Loss 5.738 Prec@(1,5) (3.7%, 11.8%)
08/01 04:04:58 PM | Train: [ 3/210] Step 1650/6092 Loss 5.734 Prec@(1,5) (3.7%, 11.9%)
08/01 04:05:19 PM | Train: [ 3/210] Step 1660/6092 Loss 5.733 Prec@(1,5) (3.7%, 11.8%)
08/01 04:05:36 PM | Train: [ 3/210] Step 1670/6092 Loss 5.734 Prec@(1,5) (3.7%, 11.8%)
08/01 04:05:54 PM | Train: [ 3/210] Step 1680/6092 Loss 5.734 Prec@(1,5) (3.7%, 11.8%)
08/01 04:06:11 PM | Train: [ 3/210] Step 1690/6092 Loss 5.731 Prec@(1,5) (3.6%, 11.9%)
08/01 04:06:30 PM | Train: [ 3/210] Step 1700/6092 Loss 5.731 Prec@(1,5) (3.6%, 11.8%)
08/01 04:06:47 PM | Train: [ 3/210] Step 1710/6092 Loss 5.729 Prec@(1,5) (3.6%, 11.9%)
08/01 04:07:05 PM | Train: [ 3/210] Step 1720/6092 Loss 5.729 Prec@(1,5) (3.6%, 11.8%)
08/01 04:07:22 PM | Train: [ 3/210] Step 1730/6092 Loss 5.726 Prec@(1,5) (3.6%, 11.8%)
08/01 04:07:39 PM | Train: [ 3/210] Step 1740/6092 Loss 5.725 Prec@(1,5) (3.6%, 11.9%)
08/01 04:07:56 PM | Train: [ 3/210] Step 1750/6092 Loss 5.728 Prec@(1,5) (3.7%, 11.9%)
08/01 04:08:14 PM | Train: [ 3/210] Step 1760/6092 Loss 5.730 Prec@(1,5) (3.6%, 11.9%)
08/01 04:08:31 PM | Train: [ 3/210] Step 1770/6092 Loss 5.732 Prec@(1,5) (3.6%, 11.9%)
08/01 04:08:48 PM | Train: [ 3/210] Step 1780/6092 Loss 5.733 Prec@(1,5) (3.6%, 11.9%)
08/01 04:09:06 PM | Train: [ 3/210] Step 1790/6092 Loss 5.736 Prec@(1,5) (3.6%, 11.9%)
08/01 04:09:23 PM | Train: [ 3/210] Step 1800/6092 Loss 5.737 Prec@(1,5) (3.6%, 11.9%)
08/01 04:09:41 PM | Train: [ 3/210] Step 1810/6092 Loss 5.734 Prec@(1,5) (3.6%, 11.9%)
08/01 04:09:58 PM | Train: [ 3/210] Step 1820/6092 Loss 5.735 Prec@(1,5) (3.6%, 11.9%)
08/01 04:10:17 PM | Train: [ 3/210] Step 1830/6092 Loss 5.732 Prec@(1,5) (3.6%, 12.0%)
08/01 04:10:34 PM | Train: [ 3/210] Step 1840/6092 Loss 5.734 Prec@(1,5) (3.6%, 12.0%)
08/01 04:10:52 PM | Train: [ 3/210] Step 1850/6092 Loss 5.731 Prec@(1,5) (3.6%, 12.0%)
08/01 04:11:09 PM | Train: [ 3/210] Step 1860/6092 Loss 5.731 Prec@(1,5) (3.5%, 12.0%)
08/01 04:11:26 PM | Train: [ 3/210] Step 1870/6092 Loss 5.732 Prec@(1,5) (3.5%, 11.9%)
08/01 04:11:44 PM | Train: [ 3/210] Step 1880/6092 Loss 5.731 Prec@(1,5) (3.6%, 11.9%)
08/01 04:12:01 PM | Train: [ 3/210] Step 1890/6092 Loss 5.731 Prec@(1,5) (3.5%, 11.9%)
08/01 04:12:18 PM | Train: [ 3/210] Step 1900/6092 Loss 5.734 Prec@(1,5) (3.6%, 11.9%)
08/01 04:12:36 PM | Train: [ 3/210] Step 1910/6092 Loss 5.735 Prec@(1,5) (3.6%, 11.9%)
08/01 04:12:53 PM | Train: [ 3/210] Step 1920/6092 Loss 5.734 Prec@(1,5) (3.6%, 11.9%)
08/01 04:13:11 PM | Train: [ 3/210] Step 1930/6092 Loss 5.738 Prec@(1,5) (3.5%, 11.9%)
08/01 04:13:28 PM | Train: [ 3/210] Step 1940/6092 Loss 5.740 Prec@(1,5) (3.5%, 11.8%)
08/01 04:13:46 PM | Train: [ 3/210] Step 1950/6092 Loss 5.740 Prec@(1,5) (3.5%, 11.8%)
08/01 04:14:03 PM | Train: [ 3/210] Step 1960/6092 Loss 5.741 Prec@(1,5) (3.5%, 11.8%)
08/01 04:14:21 PM | Train: [ 3/210] Step 1970/6092 Loss 5.745 Prec@(1,5) (3.5%, 11.8%)
08/01 04:14:38 PM | Train: [ 3/210] Step 1980/6092 Loss 5.744 Prec@(1,5) (3.6%, 11.8%)
08/01 04:14:56 PM | Train: [ 3/210] Step 1990/6092 Loss 5.743 Prec@(1,5) (3.6%, 11.8%)
08/01 04:15:15 PM | Train: [ 3/210] Step 2000/6092 Loss 5.740 Prec@(1,5) (3.6%, 11.8%)
08/01 04:15:32 PM | Train: [ 3/210] Step 2010/6092 Loss 5.738 Prec@(1,5) (3.6%, 11.9%)
08/01 04:15:50 PM | Train: [ 3/210] Step 2020/6092 Loss 5.738 Prec@(1,5) (3.6%, 11.9%)
08/01 04:16:07 PM | Train: [ 3/210] Step 2030/6092 Loss 5.738 Prec@(1,5) (3.6%, 11.9%)
08/01 04:16:24 PM | Train: [ 3/210] Step 2040/6092 Loss 5.734 Prec@(1,5) (3.6%, 11.9%)
08/01 04:16:42 PM | Train: [ 3/210] Step 2050/6092 Loss 5.736 Prec@(1,5) (3.6%, 11.9%)
08/01 04:16:59 PM | Train: [ 3/210] Step 2060/6092 Loss 5.737 Prec@(1,5) (3.6%, 11.9%)
08/01 04:17:16 PM | Train: [ 3/210] Step 2070/6092 Loss 5.734 Prec@(1,5) (3.6%, 12.0%)
08/01 04:17:34 PM | Train: [ 3/210] Step 2080/6092 Loss 5.733 Prec@(1,5) (3.6%, 11.9%)
08/01 04:17:51 PM | Train: [ 3/210] Step 2090/6092 Loss 5.735 Prec@(1,5) (3.6%, 11.9%)
08/01 04:18:08 PM | Train: [ 3/210] Step 2100/6092 Loss 5.736 Prec@(1,5) (3.6%, 11.9%)
08/01 04:18:26 PM | Train: [ 3/210] Step 2110/6092 Loss 5.737 Prec@(1,5) (3.6%, 12.0%)
08/01 04:18:44 PM | Train: [ 3/210] Step 2120/6092 Loss 5.738 Prec@(1,5) (3.6%, 11.9%)
08/01 04:19:01 PM | Train: [ 3/210] Step 2130/6092 Loss 5.739 Prec@(1,5) (3.6%, 11.9%)
08/01 04:19:18 PM | Train: [ 3/210] Step 2140/6092 Loss 5.740 Prec@(1,5) (3.6%, 12.0%)
08/01 04:19:36 PM | Train: [ 3/210] Step 2150/6092 Loss 5.742 Prec@(1,5) (3.6%, 11.9%)
08/01 04:19:53 PM | Train: [ 3/210] Step 2160/6092 Loss 5.742 Prec@(1,5) (3.6%, 11.9%)
08/01 04:20:10 PM | Train: [ 3/210] Step 2170/6092 Loss 5.739 Prec@(1,5) (3.7%, 12.0%)
08/01 04:20:30 PM | Train: [ 3/210] Step 2180/6092 Loss 5.738 Prec@(1,5) (3.7%, 12.0%)
08/01 04:20:47 PM | Train: [ 3/210] Step 2190/6092 Loss 5.737 Prec@(1,5) (3.7%, 12.0%)
08/01 04:21:04 PM | Train: [ 3/210] Step 2200/6092 Loss 5.738 Prec@(1,5) (3.6%, 12.0%)
08/01 04:21:21 PM | Train: [ 3/210] Step 2210/6092 Loss 5.738 Prec@(1,5) (3.6%, 12.0%)
08/01 04:21:39 PM | Train: [ 3/210] Step 2220/6092 Loss 5.739 Prec@(1,5) (3.6%, 12.0%)
08/01 04:21:56 PM | Train: [ 3/210] Step 2230/6092 Loss 5.740 Prec@(1,5) (3.6%, 12.0%)
08/01 04:22:14 PM | Train: [ 3/210] Step 2240/6092 Loss 5.739 Prec@(1,5) (3.6%, 12.0%)
08/01 04:22:32 PM | Train: [ 3/210] Step 2250/6092 Loss 5.736 Prec@(1,5) (3.6%, 12.0%)
08/01 04:22:49 PM | Train: [ 3/210] Step 2260/6092 Loss 5.735 Prec@(1,5) (3.7%, 12.1%)
08/01 04:23:06 PM | Train: [ 3/210] Step 2270/6092 Loss 5.733 Prec@(1,5) (3.7%, 12.1%)
08/01 04:23:24 PM | Train: [ 3/210] Step 2280/6092 Loss 5.731 Prec@(1,5) (3.7%, 12.1%)
08/01 04:23:41 PM | Train: [ 3/210] Step 2290/6092 Loss 5.730 Prec@(1,5) (3.7%, 12.2%)
08/01 04:23:59 PM | Train: [ 3/210] Step 2300/6092 Loss 5.727 Prec@(1,5) (3.7%, 12.3%)
08/01 04:24:17 PM | Train: [ 3/210] Step 2310/6092 Loss 5.730 Prec@(1,5) (3.7%, 12.2%)
08/01 04:24:34 PM | Train: [ 3/210] Step 2320/6092 Loss 5.732 Prec@(1,5) (3.7%, 12.2%)
08/01 04:24:52 PM | Train: [ 3/210] Step 2330/6092 Loss 5.732 Prec@(1,5) (3.7%, 12.1%)
08/01 04:25:09 PM | Train: [ 3/210] Step 2340/6092 Loss 5.731 Prec@(1,5) (3.7%, 12.1%)
08/01 04:25:26 PM | Train: [ 3/210] Step 2350/6092 Loss 5.730 Prec@(1,5) (3.7%, 12.1%)
08/01 04:25:44 PM | Train: [ 3/210] Step 2360/6092 Loss 5.730 Prec@(1,5) (3.7%, 12.1%)
08/01 04:26:01 PM | Train: [ 3/210] Step 2370/6092 Loss 5.730 Prec@(1,5) (3.6%, 12.1%)
08/01 04:26:19 PM | Train: [ 3/210] Step 2380/6092 Loss 5.729 Prec@(1,5) (3.7%, 12.2%)
08/01 04:26:36 PM | Train: [ 3/210] Step 2390/6092 Loss 5.729 Prec@(1,5) (3.6%, 12.2%)
08/01 04:26:54 PM | Train: [ 3/210] Step 2400/6092 Loss 5.730 Prec@(1,5) (3.6%, 12.1%)
08/01 04:27:11 PM | Train: [ 3/210] Step 2410/6092 Loss 5.730 Prec@(1,5) (3.6%, 12.1%)
08/01 04:27:29 PM | Train: [ 3/210] Step 2420/6092 Loss 5.731 Prec@(1,5) (3.7%, 12.1%)
08/01 04:27:46 PM | Train: [ 3/210] Step 2430/6092 Loss 5.731 Prec@(1,5) (3.7%, 12.1%)
08/01 04:28:03 PM | Train: [ 3/210] Step 2440/6092 Loss 5.730 Prec@(1,5) (3.7%, 12.1%)
08/01 04:28:21 PM | Train: [ 3/210] Step 2450/6092 Loss 5.727 Prec@(1,5) (3.7%, 12.1%)
08/01 04:28:38 PM | Train: [ 3/210] Step 2460/6092 Loss 5.729 Prec@(1,5) (3.7%, 12.1%)
08/01 04:28:55 PM | Train: [ 3/210] Step 2470/6092 Loss 5.732 Prec@(1,5) (3.7%, 12.1%)
08/01 04:29:14 PM | Train: [ 3/210] Step 2480/6092 Loss 5.730 Prec@(1,5) (3.7%, 12.1%)
08/01 04:29:31 PM | Train: [ 3/210] Step 2490/6092 Loss 5.731 Prec@(1,5) (3.7%, 12.1%)
08/01 04:29:49 PM | Train: [ 3/210] Step 2500/6092 Loss 5.730 Prec@(1,5) (3.7%, 12.1%)
08/01 04:30:08 PM | Train: [ 3/210] Step 2510/6092 Loss 5.730 Prec@(1,5) (3.7%, 12.1%)
08/01 04:30:29 PM | Train: [ 3/210] Step 2520/6092 Loss 5.730 Prec@(1,5) (3.7%, 12.1%)
08/01 04:30:47 PM | Train: [ 3/210] Step 2530/6092 Loss 5.730 Prec@(1,5) (3.7%, 12.1%)
08/01 04:31:05 PM | Train: [ 3/210] Step 2540/6092 Loss 5.730 Prec@(1,5) (3.7%, 12.1%)
08/01 04:31:23 PM | Train: [ 3/210] Step 2550/6092 Loss 5.729 Prec@(1,5) (3.6%, 12.1%)
08/01 04:31:40 PM | Train: [ 3/210] Step 2560/6092 Loss 5.731 Prec@(1,5) (3.6%, 12.0%)
08/01 04:31:58 PM | Train: [ 3/210] Step 2570/6092 Loss 5.727 Prec@(1,5) (3.7%, 12.2%)
08/01 04:32:15 PM | Train: [ 3/210] Step 2580/6092 Loss 5.724 Prec@(1,5) (3.7%, 12.2%)
08/01 04:32:34 PM | Train: [ 3/210] Step 2590/6092 Loss 5.724 Prec@(1,5) (3.7%, 12.2%)
08/01 04:32:52 PM | Train: [ 3/210] Step 2600/6092 Loss 5.727 Prec@(1,5) (3.7%, 12.2%)
08/01 04:33:09 PM | Train: [ 3/210] Step 2610/6092 Loss 5.725 Prec@(1,5) (3.7%, 12.2%)
08/01 04:33:27 PM | Train: [ 3/210] Step 2620/6092 Loss 5.724 Prec@(1,5) (3.7%, 12.2%)
08/01 04:33:44 PM | Train: [ 3/210] Step 2630/6092 Loss 5.724 Prec@(1,5) (3.7%, 12.2%)
08/01 04:34:02 PM | Train: [ 3/210] Step 2640/6092 Loss 5.724 Prec@(1,5) (3.7%, 12.2%)
08/01 04:34:19 PM | Train: [ 3/210] Step 2650/6092 Loss 5.722 Prec@(1,5) (3.7%, 12.2%)
08/01 04:34:36 PM | Train: [ 3/210] Step 2660/6092 Loss 5.722 Prec@(1,5) (3.7%, 12.1%)
08/01 04:34:54 PM | Train: [ 3/210] Step 2670/6092 Loss 5.722 Prec@(1,5) (3.7%, 12.1%)
08/01 04:35:14 PM | Train: [ 3/210] Step 2680/6092 Loss 5.722 Prec@(1,5) (3.6%, 12.1%)
08/01 04:35:31 PM | Train: [ 3/210] Step 2690/6092 Loss 5.721 Prec@(1,5) (3.6%, 12.1%)
08/01 04:35:49 PM | Train: [ 3/210] Step 2700/6092 Loss 5.722 Prec@(1,5) (3.6%, 12.1%)
08/01 04:36:06 PM | Train: [ 3/210] Step 2710/6092 Loss 5.722 Prec@(1,5) (3.6%, 12.1%)
08/01 04:36:23 PM | Train: [ 3/210] Step 2720/6092 Loss 5.724 Prec@(1,5) (3.6%, 12.1%)
08/01 04:36:40 PM | Train: [ 3/210] Step 2730/6092 Loss 5.724 Prec@(1,5) (3.6%, 12.0%)
08/01 04:36:57 PM | Train: [ 3/210] Step 2740/6092 Loss 5.722 Prec@(1,5) (3.6%, 12.0%)
08/01 04:37:15 PM | Train: [ 3/210] Step 2750/6092 Loss 5.723 Prec@(1,5) (3.6%, 12.0%)
08/01 04:37:32 PM | Train: [ 3/210] Step 2760/6092 Loss 5.722 Prec@(1,5) (3.6%, 12.0%)
08/01 04:37:49 PM | Train: [ 3/210] Step 2770/6092 Loss 5.722 Prec@(1,5) (3.6%, 12.0%)
08/01 04:38:07 PM | Train: [ 3/210] Step 2780/6092 Loss 5.721 Prec@(1,5) (3.6%, 12.0%)
08/01 04:38:24 PM | Train: [ 3/210] Step 2790/6092 Loss 5.721 Prec@(1,5) (3.6%, 12.0%)
08/01 04:38:41 PM | Train: [ 3/210] Step 2800/6092 Loss 5.719 Prec@(1,5) (3.6%, 12.0%)
08/01 04:38:59 PM | Train: [ 3/210] Step 2810/6092 Loss 5.721 Prec@(1,5) (3.6%, 12.0%)
08/01 04:39:17 PM | Train: [ 3/210] Step 2820/6092 Loss 5.721 Prec@(1,5) (3.6%, 12.0%)
08/01 04:39:34 PM | Train: [ 3/210] Step 2830/6092 Loss 5.719 Prec@(1,5) (3.6%, 12.1%)
08/01 04:39:53 PM | Train: [ 3/210] Step 2840/6092 Loss 5.718 Prec@(1,5) (3.7%, 12.1%)
08/01 04:40:14 PM | Train: [ 3/210] Step 2850/6092 Loss 5.716 Prec@(1,5) (3.7%, 12.1%)
08/01 04:40:32 PM | Train: [ 3/210] Step 2860/6092 Loss 5.715 Prec@(1,5) (3.7%, 12.1%)
08/01 04:40:49 PM | Train: [ 3/210] Step 2870/6092 Loss 5.715 Prec@(1,5) (3.7%, 12.1%)
08/01 04:41:06 PM | Train: [ 3/210] Step 2880/6092 Loss 5.717 Prec@(1,5) (3.7%, 12.1%)
08/01 04:41:23 PM | Train: [ 3/210] Step 2890/6092 Loss 5.716 Prec@(1,5) (3.6%, 12.1%)
08/01 04:41:41 PM | Train: [ 3/210] Step 2900/6092 Loss 5.714 Prec@(1,5) (3.7%, 12.1%)
08/01 04:41:58 PM | Train: [ 3/210] Step 2910/6092 Loss 5.715 Prec@(1,5) (3.7%, 12.1%)
08/01 04:42:16 PM | Train: [ 3/210] Step 2920/6092 Loss 5.714 Prec@(1,5) (3.6%, 12.1%)
08/01 04:42:34 PM | Train: [ 3/210] Step 2930/6092 Loss 5.713 Prec@(1,5) (3.7%, 12.1%)
08/01 04:42:53 PM | Train: [ 3/210] Step 2940/6092 Loss 5.712 Prec@(1,5) (3.6%, 12.1%)
08/01 04:43:12 PM | Train: [ 3/210] Step 2950/6092 Loss 5.711 Prec@(1,5) (3.6%, 12.1%)
08/01 04:43:30 PM | Train: [ 3/210] Step 2960/6092 Loss 5.711 Prec@(1,5) (3.6%, 12.1%)
08/01 04:43:47 PM | Train: [ 3/210] Step 2970/6092 Loss 5.709 Prec@(1,5) (3.7%, 12.2%)
08/01 04:44:05 PM | Train: [ 3/210] Step 2980/6092 Loss 5.708 Prec@(1,5) (3.7%, 12.2%)
08/01 04:44:22 PM | Train: [ 3/210] Step 2990/6092 Loss 5.709 Prec@(1,5) (3.6%, 12.2%)
08/01 04:44:40 PM | Train: [ 3/210] Step 3000/6092 Loss 5.710 Prec@(1,5) (3.6%, 12.1%)
08/01 04:44:58 PM | Train: [ 3/210] Step 3010/6092 Loss 5.709 Prec@(1,5) (3.7%, 12.2%)
08/01 04:45:18 PM | Train: [ 3/210] Step 3020/6092 Loss 5.707 Prec@(1,5) (3.7%, 12.3%)
08/01 04:45:36 PM | Train: [ 3/210] Step 3030/6092 Loss 5.709 Prec@(1,5) (3.7%, 12.3%)
08/01 04:45:54 PM | Train: [ 3/210] Step 3040/6092 Loss 5.710 Prec@(1,5) (3.7%, 12.2%)
08/01 04:46:11 PM | Train: [ 3/210] Step 3050/6092 Loss 5.710 Prec@(1,5) (3.7%, 12.2%)
08/01 04:46:28 PM | Train: [ 3/210] Step 3060/6092 Loss 5.711 Prec@(1,5) (3.7%, 12.2%)
08/01 04:46:46 PM | Train: [ 3/210] Step 3070/6092 Loss 5.710 Prec@(1,5) (3.7%, 12.2%)
08/01 04:47:04 PM | Train: [ 3/210] Step 3080/6092 Loss 5.710 Prec@(1,5) (3.7%, 12.2%)
08/01 04:47:22 PM | Train: [ 3/210] Step 3090/6092 Loss 5.711 Prec@(1,5) (3.7%, 12.2%)
08/01 04:47:41 PM | Train: [ 3/210] Step 3100/6092 Loss 5.710 Prec@(1,5) (3.7%, 12.2%)
08/01 04:47:59 PM | Train: [ 3/210] Step 3110/6092 Loss 5.709 Prec@(1,5) (3.7%, 12.3%)
08/01 04:48:16 PM | Train: [ 3/210] Step 3120/6092 Loss 5.708 Prec@(1,5) (3.7%, 12.3%)
08/01 04:48:33 PM | Train: [ 3/210] Step 3130/6092 Loss 5.709 Prec@(1,5) (3.7%, 12.3%)
08/01 04:48:51 PM | Train: [ 3/210] Step 3140/6092 Loss 5.708 Prec@(1,5) (3.7%, 12.3%)
08/01 04:49:09 PM | Train: [ 3/210] Step 3150/6092 Loss 5.709 Prec@(1,5) (3.7%, 12.3%)
08/01 04:49:26 PM | Train: [ 3/210] Step 3160/6092 Loss 5.709 Prec@(1,5) (3.7%, 12.3%)
08/01 04:49:44 PM | Train: [ 3/210] Step 3170/6092 Loss 5.708 Prec@(1,5) (3.7%, 12.3%)
08/01 04:50:01 PM | Train: [ 3/210] Step 3180/6092 Loss 5.708 Prec@(1,5) (3.7%, 12.3%)
08/01 04:50:20 PM | Train: [ 3/210] Step 3190/6092 Loss 5.708 Prec@(1,5) (3.7%, 12.3%)
08/01 04:50:37 PM | Train: [ 3/210] Step 3200/6092 Loss 5.708 Prec@(1,5) (3.7%, 12.3%)
08/01 04:50:54 PM | Train: [ 3/210] Step 3210/6092 Loss 5.709 Prec@(1,5) (3.7%, 12.3%)
08/01 04:51:11 PM | Train: [ 3/210] Step 3220/6092 Loss 5.709 Prec@(1,5) (3.7%, 12.3%)
08/01 04:51:29 PM | Train: [ 3/210] Step 3230/6092 Loss 5.708 Prec@(1,5) (3.8%, 12.3%)
08/01 04:51:46 PM | Train: [ 3/210] Step 3240/6092 Loss 5.707 Prec@(1,5) (3.8%, 12.3%)
08/01 04:52:04 PM | Train: [ 3/210] Step 3250/6092 Loss 5.707 Prec@(1,5) (3.8%, 12.3%)
08/01 04:52:21 PM | Train: [ 3/210] Step 3260/6092 Loss 5.705 Prec@(1,5) (3.8%, 12.3%)
08/01 04:52:40 PM | Train: [ 3/210] Step 3270/6092 Loss 5.703 Prec@(1,5) (3.8%, 12.4%)
08/01 04:52:58 PM | Train: [ 3/210] Step 3280/6092 Loss 5.702 Prec@(1,5) (3.8%, 12.4%)
08/01 04:53:15 PM | Train: [ 3/210] Step 3290/6092 Loss 5.701 Prec@(1,5) (3.8%, 12.4%)
08/01 04:53:33 PM | Train: [ 3/210] Step 3300/6092 Loss 5.702 Prec@(1,5) (3.8%, 12.4%)
08/01 04:53:50 PM | Train: [ 3/210] Step 3310/6092 Loss 5.702 Prec@(1,5) (3.8%, 12.4%)
08/01 04:54:08 PM | Train: [ 3/210] Step 3320/6092 Loss 5.702 Prec@(1,5) (3.8%, 12.4%)
08/01 04:54:25 PM | Train: [ 3/210] Step 3330/6092 Loss 5.700 Prec@(1,5) (3.8%, 12.4%)
08/01 04:54:42 PM | Train: [ 3/210] Step 3340/6092 Loss 5.700 Prec@(1,5) (3.8%, 12.4%)
08/01 04:55:00 PM | Train: [ 3/210] Step 3350/6092 Loss 5.700 Prec@(1,5) (3.8%, 12.4%)
08/01 04:55:20 PM | Train: [ 3/210] Step 3360/6092 Loss 5.699 Prec@(1,5) (3.8%, 12.4%)
08/01 04:55:37 PM | Train: [ 3/210] Step 3370/6092 Loss 5.697 Prec@(1,5) (3.8%, 12.4%)
08/01 04:55:55 PM | Train: [ 3/210] Step 3380/6092 Loss 5.698 Prec@(1,5) (3.8%, 12.4%)
08/01 04:56:12 PM | Train: [ 3/210] Step 3390/6092 Loss 5.697 Prec@(1,5) (3.8%, 12.4%)
08/01 04:56:29 PM | Train: [ 3/210] Step 3400/6092 Loss 5.696 Prec@(1,5) (3.8%, 12.4%)
08/01 04:56:47 PM | Train: [ 3/210] Step 3410/6092 Loss 5.696 Prec@(1,5) (3.8%, 12.4%)
08/01 04:57:05 PM | Train: [ 3/210] Step 3420/6092 Loss 5.696 Prec@(1,5) (3.8%, 12.4%)
08/01 04:57:22 PM | Train: [ 3/210] Step 3430/6092 Loss 5.695 Prec@(1,5) (3.8%, 12.5%)
08/01 04:57:39 PM | Train: [ 3/210] Step 3440/6092 Loss 5.694 Prec@(1,5) (3.8%, 12.5%)
08/01 04:57:56 PM | Train: [ 3/210] Step 3450/6092 Loss 5.694 Prec@(1,5) (3.8%, 12.4%)
08/01 04:58:14 PM | Train: [ 3/210] Step 3460/6092 Loss 5.693 Prec@(1,5) (3.8%, 12.4%)
08/01 04:58:31 PM | Train: [ 3/210] Step 3470/6092 Loss 5.693 Prec@(1,5) (3.8%, 12.4%)
08/01 04:58:48 PM | Train: [ 3/210] Step 3480/6092 Loss 5.693 Prec@(1,5) (3.8%, 12.4%)
08/01 04:59:06 PM | Train: [ 3/210] Step 3490/6092 Loss 5.694 Prec@(1,5) (3.8%, 12.4%)
08/01 04:59:27 PM | Train: [ 3/210] Step 3500/6092 Loss 5.693 Prec@(1,5) (3.8%, 12.4%)
08/01 04:59:45 PM | Train: [ 3/210] Step 3510/6092 Loss 5.694 Prec@(1,5) (3.8%, 12.3%)
08/01 05:00:02 PM | Train: [ 3/210] Step 3520/6092 Loss 5.694 Prec@(1,5) (3.8%, 12.4%)
08/01 05:00:21 PM | Train: [ 3/210] Step 3530/6092 Loss 5.694 Prec@(1,5) (3.8%, 12.4%)
08/01 05:00:39 PM | Train: [ 3/210] Step 3540/6092 Loss 5.694 Prec@(1,5) (3.8%, 12.4%)
08/01 05:00:56 PM | Train: [ 3/210] Step 3550/6092 Loss 5.693 Prec@(1,5) (3.8%, 12.4%)
08/01 05:01:13 PM | Train: [ 3/210] Step 3560/6092 Loss 5.693 Prec@(1,5) (3.8%, 12.4%)
08/01 05:01:30 PM | Train: [ 3/210] Step 3570/6092 Loss 5.693 Prec@(1,5) (3.8%, 12.4%)
08/01 05:01:47 PM | Train: [ 3/210] Step 3580/6092 Loss 5.692 Prec@(1,5) (3.8%, 12.4%)
08/01 05:02:05 PM | Train: [ 3/210] Step 3590/6092 Loss 5.693 Prec@(1,5) (3.8%, 12.4%)
08/01 05:02:23 PM | Train: [ 3/210] Step 3600/6092 Loss 5.693 Prec@(1,5) (3.8%, 12.4%)
08/01 05:02:40 PM | Train: [ 3/210] Step 3610/6092 Loss 5.692 Prec@(1,5) (3.8%, 12.4%)
08/01 05:02:58 PM | Train: [ 3/210] Step 3620/6092 Loss 5.691 Prec@(1,5) (3.8%, 12.4%)
08/01 05:03:15 PM | Train: [ 3/210] Step 3630/6092 Loss 5.689 Prec@(1,5) (3.9%, 12.4%)
08/01 05:03:33 PM | Train: [ 3/210] Step 3640/6092 Loss 5.689 Prec@(1,5) (3.9%, 12.4%)
08/01 05:03:51 PM | Train: [ 3/210] Step 3650/6092 Loss 5.688 Prec@(1,5) (3.9%, 12.4%)
08/01 05:04:08 PM | Train: [ 3/210] Step 3660/6092 Loss 5.689 Prec@(1,5) (3.9%, 12.4%)
08/01 05:04:25 PM | Train: [ 3/210] Step 3670/6092 Loss 5.690 Prec@(1,5) (3.9%, 12.4%)
08/01 05:04:42 PM | Train: [ 3/210] Step 3680/6092 Loss 5.689 Prec@(1,5) (3.9%, 12.4%)
08/01 05:04:59 PM | Train: [ 3/210] Step 3690/6092 Loss 5.689 Prec@(1,5) (3.9%, 12.4%)
08/01 05:05:19 PM | Train: [ 3/210] Step 3700/6092 Loss 5.689 Prec@(1,5) (3.9%, 12.4%)
08/01 05:05:36 PM | Train: [ 3/210] Step 3710/6092 Loss 5.688 Prec@(1,5) (3.8%, 12.4%)
08/01 05:05:54 PM | Train: [ 3/210] Step 3720/6092 Loss 5.687 Prec@(1,5) (3.8%, 12.4%)
08/01 05:06:11 PM | Train: [ 3/210] Step 3730/6092 Loss 5.688 Prec@(1,5) (3.8%, 12.4%)
08/01 05:06:28 PM | Train: [ 3/210] Step 3740/6092 Loss 5.688 Prec@(1,5) (3.8%, 12.4%)
08/01 05:06:46 PM | Train: [ 3/210] Step 3750/6092 Loss 5.689 Prec@(1,5) (3.8%, 12.4%)
08/01 05:07:03 PM | Train: [ 3/210] Step 3760/6092 Loss 5.690 Prec@(1,5) (3.8%, 12.4%)
08/01 05:07:20 PM | Train: [ 3/210] Step 3770/6092 Loss 5.689 Prec@(1,5) (3.8%, 12.4%)
08/01 05:07:38 PM | Train: [ 3/210] Step 3780/6092 Loss 5.688 Prec@(1,5) (3.8%, 12.4%)
08/01 05:07:55 PM | Train: [ 3/210] Step 3790/6092 Loss 5.688 Prec@(1,5) (3.9%, 12.4%)
08/01 05:08:12 PM | Train: [ 3/210] Step 3800/6092 Loss 5.688 Prec@(1,5) (3.8%, 12.4%)
08/01 05:08:29 PM | Train: [ 3/210] Step 3810/6092 Loss 5.687 Prec@(1,5) (3.8%, 12.4%)
08/01 05:08:48 PM | Train: [ 3/210] Step 3820/6092 Loss 5.687 Prec@(1,5) (3.8%, 12.4%)
08/01 05:09:07 PM | Train: [ 3/210] Step 3830/6092 Loss 5.687 Prec@(1,5) (3.8%, 12.4%)
08/01 05:09:26 PM | Train: [ 3/210] Step 3840/6092 Loss 5.686 Prec@(1,5) (3.9%, 12.5%)
08/01 05:09:45 PM | Train: [ 3/210] Step 3850/6092 Loss 5.686 Prec@(1,5) (3.9%, 12.5%)
08/01 05:10:06 PM | Train: [ 3/210] Step 3860/6092 Loss 5.684 Prec@(1,5) (3.9%, 12.5%)
08/01 05:10:24 PM | Train: [ 3/210] Step 3870/6092 Loss 5.684 Prec@(1,5) (3.9%, 12.5%)
08/01 05:10:41 PM | Train: [ 3/210] Step 3880/6092 Loss 5.682 Prec@(1,5) (3.9%, 12.5%)
08/01 05:10:59 PM | Train: [ 3/210] Step 3890/6092 Loss 5.681 Prec@(1,5) (3.8%, 12.5%)
08/01 05:11:15 PM | Train: [ 3/210] Step 3900/6092 Loss 5.681 Prec@(1,5) (3.8%, 12.5%)
08/01 05:11:33 PM | Train: [ 3/210] Step 3910/6092 Loss 5.681 Prec@(1,5) (3.8%, 12.5%)
08/01 05:11:51 PM | Train: [ 3/210] Step 3920/6092 Loss 5.680 Prec@(1,5) (3.9%, 12.5%)
08/01 05:12:08 PM | Train: [ 3/210] Step 3930/6092 Loss 5.681 Prec@(1,5) (3.9%, 12.5%)
08/01 05:12:27 PM | Train: [ 3/210] Step 3940/6092 Loss 5.679 Prec@(1,5) (3.9%, 12.5%)
08/01 05:12:44 PM | Train: [ 3/210] Step 3950/6092 Loss 5.679 Prec@(1,5) (3.9%, 12.5%)
08/01 05:13:01 PM | Train: [ 3/210] Step 3960/6092 Loss 5.680 Prec@(1,5) (3.9%, 12.5%)
08/01 05:13:19 PM | Train: [ 3/210] Step 3970/6092 Loss 5.681 Prec@(1,5) (3.9%, 12.5%)
08/01 05:13:37 PM | Train: [ 3/210] Step 3980/6092 Loss 5.682 Prec@(1,5) (3.9%, 12.5%)
08/01 05:13:54 PM | Train: [ 3/210] Step 3990/6092 Loss 5.681 Prec@(1,5) (3.9%, 12.5%)
08/01 05:14:11 PM | Train: [ 3/210] Step 4000/6092 Loss 5.682 Prec@(1,5) (3.9%, 12.5%)
08/01 05:14:29 PM | Train: [ 3/210] Step 4010/6092 Loss 5.682 Prec@(1,5) (3.9%, 12.5%)
08/01 05:14:46 PM | Train: [ 3/210] Step 4020/6092 Loss 5.681 Prec@(1,5) (3.9%, 12.5%)
08/01 05:15:04 PM | Train: [ 3/210] Step 4030/6092 Loss 5.680 Prec@(1,5) (3.9%, 12.5%)
08/01 05:15:22 PM | Train: [ 3/210] Step 4040/6092 Loss 5.680 Prec@(1,5) (3.9%, 12.5%)
08/01 05:15:39 PM | Train: [ 3/210] Step 4050/6092 Loss 5.681 Prec@(1,5) (3.9%, 12.5%)
08/01 05:15:56 PM | Train: [ 3/210] Step 4060/6092 Loss 5.683 Prec@(1,5) (3.8%, 12.5%)
08/01 05:16:13 PM | Train: [ 3/210] Step 4070/6092 Loss 5.683 Prec@(1,5) (3.8%, 12.5%)
08/01 05:16:32 PM | Train: [ 3/210] Step 4080/6092 Loss 5.682 Prec@(1,5) (3.8%, 12.5%)
08/01 05:16:51 PM | Train: [ 3/210] Step 4090/6092 Loss 5.683 Prec@(1,5) (3.8%, 12.5%)
08/01 05:17:09 PM | Train: [ 3/210] Step 4100/6092 Loss 5.683 Prec@(1,5) (3.9%, 12.5%)
08/01 05:17:26 PM | Train: [ 3/210] Step 4110/6092 Loss 5.682 Prec@(1,5) (3.9%, 12.5%)
08/01 05:17:43 PM | Train: [ 3/210] Step 4120/6092 Loss 5.682 Prec@(1,5) (3.9%, 12.5%)
08/01 05:18:01 PM | Train: [ 3/210] Step 4130/6092 Loss 5.682 Prec@(1,5) (3.9%, 12.5%)
08/01 05:18:19 PM | Train: [ 3/210] Step 4140/6092 Loss 5.682 Prec@(1,5) (3.9%, 12.5%)
08/01 05:18:36 PM | Train: [ 3/210] Step 4150/6092 Loss 5.681 Prec@(1,5) (3.9%, 12.5%)
08/01 05:18:54 PM | Train: [ 3/210] Step 4160/6092 Loss 5.681 Prec@(1,5) (3.9%, 12.5%)
08/01 05:19:11 PM | Train: [ 3/210] Step 4170/6092 Loss 5.681 Prec@(1,5) (3.9%, 12.5%)
08/01 05:19:28 PM | Train: [ 3/210] Step 4180/6092 Loss 5.681 Prec@(1,5) (3.9%, 12.4%)
08/01 05:19:45 PM | Train: [ 3/210] Step 4190/6092 Loss 5.681 Prec@(1,5) (3.9%, 12.4%)
08/01 05:20:03 PM | Train: [ 3/210] Step 4200/6092 Loss 5.682 Prec@(1,5) (3.9%, 12.4%)
08/01 05:20:21 PM | Train: [ 3/210] Step 4210/6092 Loss 5.683 Prec@(1,5) (3.8%, 12.4%)
08/01 05:20:39 PM | Train: [ 3/210] Step 4220/6092 Loss 5.683 Prec@(1,5) (3.8%, 12.4%)
08/01 05:20:56 PM | Train: [ 3/210] Step 4230/6092 Loss 5.681 Prec@(1,5) (3.9%, 12.4%)
08/01 05:21:14 PM | Train: [ 3/210] Step 4240/6092 Loss 5.681 Prec@(1,5) (3.8%, 12.4%)
08/01 05:21:31 PM | Train: [ 3/210] Step 4250/6092 Loss 5.681 Prec@(1,5) (3.8%, 12.4%)
08/01 05:21:48 PM | Train: [ 3/210] Step 4260/6092 Loss 5.682 Prec@(1,5) (3.8%, 12.4%)
08/01 05:22:06 PM | Train: [ 3/210] Step 4270/6092 Loss 5.682 Prec@(1,5) (3.9%, 12.4%)
08/01 05:22:23 PM | Train: [ 3/210] Step 4280/6092 Loss 5.682 Prec@(1,5) (3.8%, 12.4%)
08/01 05:22:41 PM | Train: [ 3/210] Step 4290/6092 Loss 5.683 Prec@(1,5) (3.8%, 12.4%)
08/01 05:22:59 PM | Train: [ 3/210] Step 4300/6092 Loss 5.683 Prec@(1,5) (3.8%, 12.4%)
08/01 05:23:16 PM | Train: [ 3/210] Step 4310/6092 Loss 5.683 Prec@(1,5) (3.9%, 12.4%)
08/01 05:23:34 PM | Train: [ 3/210] Step 4320/6092 Loss 5.682 Prec@(1,5) (3.9%, 12.5%)
08/01 05:23:51 PM | Train: [ 3/210] Step 4330/6092 Loss 5.680 Prec@(1,5) (3.9%, 12.5%)
08/01 05:24:09 PM | Train: [ 3/210] Step 4340/6092 Loss 5.680 Prec@(1,5) (3.9%, 12.5%)
08/01 05:24:26 PM | Train: [ 3/210] Step 4350/6092 Loss 5.679 Prec@(1,5) (3.9%, 12.5%)
08/01 05:24:44 PM | Train: [ 3/210] Step 4360/6092 Loss 5.680 Prec@(1,5) (3.9%, 12.5%)
08/01 05:25:02 PM | Train: [ 3/210] Step 4370/6092 Loss 5.679 Prec@(1,5) (3.9%, 12.5%)
08/01 05:25:21 PM | Train: [ 3/210] Step 4380/6092 Loss 5.679 Prec@(1,5) (3.9%, 12.5%)
08/01 05:25:38 PM | Train: [ 3/210] Step 4390/6092 Loss 5.680 Prec@(1,5) (3.9%, 12.5%)
08/01 05:25:55 PM | Train: [ 3/210] Step 4400/6092 Loss 5.681 Prec@(1,5) (3.9%, 12.5%)
08/01 05:26:13 PM | Train: [ 3/210] Step 4410/6092 Loss 5.680 Prec@(1,5) (3.9%, 12.5%)
08/01 05:26:31 PM | Train: [ 3/210] Step 4420/6092 Loss 5.680 Prec@(1,5) (3.9%, 12.5%)
08/01 05:26:50 PM | Train: [ 3/210] Step 4430/6092 Loss 5.679 Prec@(1,5) (3.9%, 12.5%)
08/01 05:27:07 PM | Train: [ 3/210] Step 4440/6092 Loss 5.680 Prec@(1,5) (3.9%, 12.4%)
08/01 05:27:25 PM | Train: [ 3/210] Step 4450/6092 Loss 5.679 Prec@(1,5) (3.9%, 12.5%)
08/01 05:27:43 PM | Train: [ 3/210] Step 4460/6092 Loss 5.679 Prec@(1,5) (3.9%, 12.5%)
08/01 05:28:00 PM | Train: [ 3/210] Step 4470/6092 Loss 5.681 Prec@(1,5) (3.9%, 12.5%)
08/01 05:28:18 PM | Train: [ 3/210] Step 4480/6092 Loss 5.681 Prec@(1,5) (3.9%, 12.5%)
08/01 05:28:35 PM | Train: [ 3/210] Step 4490/6092 Loss 5.682 Prec@(1,5) (3.9%, 12.5%)
08/01 05:28:52 PM | Train: [ 3/210] Step 4500/6092 Loss 5.682 Prec@(1,5) (3.9%, 12.4%)
08/01 05:29:09 PM | Train: [ 3/210] Step 4510/6092 Loss 5.682 Prec@(1,5) (3.8%, 12.4%)
08/01 05:29:27 PM | Train: [ 3/210] Step 4520/6092 Loss 5.682 Prec@(1,5) (3.8%, 12.4%)
08/01 05:29:45 PM | Train: [ 3/210] Step 4530/6092 Loss 5.681 Prec@(1,5) (3.8%, 12.4%)
08/01 05:30:04 PM | Train: [ 3/210] Step 4540/6092 Loss 5.679 Prec@(1,5) (3.9%, 12.5%)
08/01 05:30:24 PM | Train: [ 3/210] Step 4550/6092 Loss 5.679 Prec@(1,5) (3.9%, 12.4%)
08/01 05:30:43 PM | Train: [ 3/210] Step 4560/6092 Loss 5.680 Prec@(1,5) (3.9%, 12.4%)
08/01 05:31:01 PM | Train: [ 3/210] Step 4570/6092 Loss 5.679 Prec@(1,5) (3.9%, 12.4%)
08/01 05:31:20 PM | Train: [ 3/210] Step 4580/6092 Loss 5.679 Prec@(1,5) (3.9%, 12.4%)
08/01 05:31:38 PM | Train: [ 3/210] Step 4590/6092 Loss 5.679 Prec@(1,5) (3.9%, 12.5%)
08/01 05:31:56 PM | Train: [ 3/210] Step 4600/6092 Loss 5.679 Prec@(1,5) (3.8%, 12.5%)
08/01 05:32:13 PM | Train: [ 3/210] Step 4610/6092 Loss 5.679 Prec@(1,5) (3.8%, 12.4%)
08/01 05:32:32 PM | Train: [ 3/210] Step 4620/6092 Loss 5.679 Prec@(1,5) (3.8%, 12.4%)
08/01 05:32:50 PM | Train: [ 3/210] Step 4630/6092 Loss 5.678 Prec@(1,5) (3.8%, 12.4%)
08/01 05:33:08 PM | Train: [ 3/210] Step 4640/6092 Loss 5.678 Prec@(1,5) (3.8%, 12.4%)
08/01 05:33:26 PM | Train: [ 3/210] Step 4650/6092 Loss 5.678 Prec@(1,5) (3.8%, 12.4%)
08/01 05:33:43 PM | Train: [ 3/210] Step 4660/6092 Loss 5.679 Prec@(1,5) (3.9%, 12.4%)
08/01 05:34:01 PM | Train: [ 3/210] Step 4670/6092 Loss 5.678 Prec@(1,5) (3.9%, 12.4%)
08/01 05:34:19 PM | Train: [ 3/210] Step 4680/6092 Loss 5.677 Prec@(1,5) (3.8%, 12.5%)
08/01 05:34:36 PM | Train: [ 3/210] Step 4690/6092 Loss 5.678 Prec@(1,5) (3.8%, 12.5%)
08/01 05:34:53 PM | Train: [ 3/210] Step 4700/6092 Loss 5.678 Prec@(1,5) (3.8%, 12.5%)
08/01 05:35:13 PM | Train: [ 3/210] Step 4710/6092 Loss 5.677 Prec@(1,5) (3.9%, 12.5%)
08/01 05:35:31 PM | Train: [ 3/210] Step 4720/6092 Loss 5.677 Prec@(1,5) (3.9%, 12.5%)
08/01 05:35:48 PM | Train: [ 3/210] Step 4730/6092 Loss 5.676 Prec@(1,5) (3.9%, 12.5%)
08/01 05:36:06 PM | Train: [ 3/210] Step 4740/6092 Loss 5.676 Prec@(1,5) (3.9%, 12.5%)
08/01 05:36:23 PM | Train: [ 3/210] Step 4750/6092 Loss 5.676 Prec@(1,5) (3.9%, 12.5%)
08/01 05:36:40 PM | Train: [ 3/210] Step 4760/6092 Loss 5.675 Prec@(1,5) (3.9%, 12.5%)
08/01 05:36:57 PM | Train: [ 3/210] Step 4770/6092 Loss 5.673 Prec@(1,5) (3.9%, 12.5%)
08/01 05:37:14 PM | Train: [ 3/210] Step 4780/6092 Loss 5.674 Prec@(1,5) (3.9%, 12.5%)
08/01 05:37:32 PM | Train: [ 3/210] Step 4790/6092 Loss 5.674 Prec@(1,5) (3.9%, 12.5%)
08/01 05:37:50 PM | Train: [ 3/210] Step 4800/6092 Loss 5.675 Prec@(1,5) (3.9%, 12.5%)
08/01 05:38:10 PM | Train: [ 3/210] Step 4810/6092 Loss 5.675 Prec@(1,5) (3.9%, 12.5%)
08/01 05:38:27 PM | Train: [ 3/210] Step 4820/6092 Loss 5.674 Prec@(1,5) (3.9%, 12.5%)
08/01 05:38:44 PM | Train: [ 3/210] Step 4830/6092 Loss 5.673 Prec@(1,5) (3.9%, 12.5%)
08/01 05:39:02 PM | Train: [ 3/210] Step 4840/6092 Loss 5.673 Prec@(1,5) (3.9%, 12.5%)
08/01 05:39:19 PM | Train: [ 3/210] Step 4850/6092 Loss 5.673 Prec@(1,5) (3.9%, 12.5%)
08/01 05:39:37 PM | Train: [ 3/210] Step 4860/6092 Loss 5.673 Prec@(1,5) (3.9%, 12.5%)
08/01 05:39:54 PM | Train: [ 3/210] Step 4870/6092 Loss 5.673 Prec@(1,5) (3.9%, 12.5%)
08/01 05:40:13 PM | Train: [ 3/210] Step 4880/6092 Loss 5.673 Prec@(1,5) (3.9%, 12.5%)
08/01 05:40:31 PM | Train: [ 3/210] Step 4890/6092 Loss 5.673 Prec@(1,5) (3.9%, 12.5%)
08/01 05:40:48 PM | Train: [ 3/210] Step 4900/6092 Loss 5.673 Prec@(1,5) (3.9%, 12.5%)
08/01 05:41:05 PM | Train: [ 3/210] Step 4910/6092 Loss 5.672 Prec@(1,5) (3.9%, 12.5%)
08/01 05:41:22 PM | Train: [ 3/210] Step 4920/6092 Loss 5.672 Prec@(1,5) (3.9%, 12.6%)
08/01 05:41:39 PM | Train: [ 3/210] Step 4930/6092 Loss 5.670 Prec@(1,5) (3.9%, 12.6%)
08/01 05:41:56 PM | Train: [ 3/210] Step 4940/6092 Loss 5.670 Prec@(1,5) (3.9%, 12.6%)
08/01 05:42:14 PM | Train: [ 3/210] Step 4950/6092 Loss 5.670 Prec@(1,5) (3.9%, 12.6%)
08/01 05:42:31 PM | Train: [ 3/210] Step 4960/6092 Loss 5.668 Prec@(1,5) (3.9%, 12.6%)
08/01 05:42:48 PM | Train: [ 3/210] Step 4970/6092 Loss 5.668 Prec@(1,5) (3.9%, 12.6%)
08/01 05:43:06 PM | Train: [ 3/210] Step 4980/6092 Loss 5.668 Prec@(1,5) (3.9%, 12.6%)
08/01 05:43:23 PM | Train: [ 3/210] Step 4990/6092 Loss 5.668 Prec@(1,5) (3.9%, 12.7%)
08/01 05:43:41 PM | Train: [ 3/210] Step 5000/6092 Loss 5.667 Prec@(1,5) (3.9%, 12.7%)
08/01 05:44:00 PM | Train: [ 3/210] Step 5010/6092 Loss 5.667 Prec@(1,5) (3.9%, 12.7%)
08/01 05:44:19 PM | Train: [ 3/210] Step 5020/6092 Loss 5.668 Prec@(1,5) (3.9%, 12.7%)
08/01 05:44:37 PM | Train: [ 3/210] Step 5030/6092 Loss 5.669 Prec@(1,5) (3.9%, 12.7%)
08/01 05:44:55 PM | Train: [ 3/210] Step 5040/6092 Loss 5.669 Prec@(1,5) (3.9%, 12.7%)
08/01 05:45:14 PM | Train: [ 3/210] Step 5050/6092 Loss 5.669 Prec@(1,5) (3.9%, 12.7%)
08/01 05:45:31 PM | Train: [ 3/210] Step 5060/6092 Loss 5.668 Prec@(1,5) (3.9%, 12.7%)
08/01 05:45:48 PM | Train: [ 3/210] Step 5070/6092 Loss 5.667 Prec@(1,5) (3.9%, 12.7%)
08/01 05:46:06 PM | Train: [ 3/210] Step 5080/6092 Loss 5.667 Prec@(1,5) (3.9%, 12.7%)
08/01 05:46:23 PM | Train: [ 3/210] Step 5090/6092 Loss 5.666 Prec@(1,5) (3.9%, 12.7%)
08/01 05:46:40 PM | Train: [ 3/210] Step 5100/6092 Loss 5.666 Prec@(1,5) (3.9%, 12.7%)
08/01 05:46:58 PM | Train: [ 3/210] Step 5110/6092 Loss 5.666 Prec@(1,5) (3.9%, 12.7%)
08/01 05:47:16 PM | Train: [ 3/210] Step 5120/6092 Loss 5.666 Prec@(1,5) (3.9%, 12.7%)
08/01 05:47:33 PM | Train: [ 3/210] Step 5130/6092 Loss 5.666 Prec@(1,5) (3.9%, 12.7%)
08/01 05:47:50 PM | Train: [ 3/210] Step 5140/6092 Loss 5.666 Prec@(1,5) (3.9%, 12.7%)
08/01 05:48:08 PM | Train: [ 3/210] Step 5150/6092 Loss 5.666 Prec@(1,5) (3.9%, 12.7%)
08/01 05:48:25 PM | Train: [ 3/210] Step 5160/6092 Loss 5.666 Prec@(1,5) (3.9%, 12.7%)
08/01 05:48:43 PM | Train: [ 3/210] Step 5170/6092 Loss 5.666 Prec@(1,5) (3.9%, 12.8%)
08/01 05:49:01 PM | Train: [ 3/210] Step 5180/6092 Loss 5.668 Prec@(1,5) (3.9%, 12.7%)
08/01 05:49:18 PM | Train: [ 3/210] Step 5190/6092 Loss 5.668 Prec@(1,5) (3.9%, 12.7%)
08/01 05:49:36 PM | Train: [ 3/210] Step 5200/6092 Loss 5.667 Prec@(1,5) (3.9%, 12.7%)
08/01 05:49:53 PM | Train: [ 3/210] Step 5210/6092 Loss 5.666 Prec@(1,5) (3.9%, 12.8%)
08/01 05:50:12 PM | Train: [ 3/210] Step 5220/6092 Loss 5.666 Prec@(1,5) (3.9%, 12.8%)
08/01 05:50:30 PM | Train: [ 3/210] Step 5230/6092 Loss 5.666 Prec@(1,5) (3.9%, 12.7%)
08/01 05:50:49 PM | Train: [ 3/210] Step 5240/6092 Loss 5.667 Prec@(1,5) (3.9%, 12.7%)
08/01 05:51:07 PM | Train: [ 3/210] Step 5250/6092 Loss 5.666 Prec@(1,5) (3.9%, 12.8%)
08/01 05:51:24 PM | Train: [ 3/210] Step 5260/6092 Loss 5.665 Prec@(1,5) (3.9%, 12.8%)
08/01 05:51:43 PM | Train: [ 3/210] Step 5270/6092 Loss 5.665 Prec@(1,5) (3.9%, 12.8%)
08/01 05:52:00 PM | Train: [ 3/210] Step 5280/6092 Loss 5.666 Prec@(1,5) (3.9%, 12.8%)
08/01 05:52:17 PM | Train: [ 3/210] Step 5290/6092 Loss 5.665 Prec@(1,5) (3.9%, 12.8%)
08/01 05:52:35 PM | Train: [ 3/210] Step 5300/6092 Loss 5.665 Prec@(1,5) (3.9%, 12.8%)
08/01 05:52:52 PM | Train: [ 3/210] Step 5310/6092 Loss 5.665 Prec@(1,5) (3.9%, 12.8%)
08/01 05:53:10 PM | Train: [ 3/210] Step 5320/6092 Loss 5.666 Prec@(1,5) (3.9%, 12.8%)
08/01 05:53:27 PM | Train: [ 3/210] Step 5330/6092 Loss 5.666 Prec@(1,5) (3.9%, 12.8%)
08/01 05:53:44 PM | Train: [ 3/210] Step 5340/6092 Loss 5.665 Prec@(1,5) (3.9%, 12.8%)
08/01 05:54:02 PM | Train: [ 3/210] Step 5350/6092 Loss 5.665 Prec@(1,5) (3.9%, 12.8%)
08/01 05:54:21 PM | Train: [ 3/210] Step 5360/6092 Loss 5.666 Prec@(1,5) (3.9%, 12.8%)
08/01 05:54:39 PM | Train: [ 3/210] Step 5370/6092 Loss 5.665 Prec@(1,5) (3.9%, 12.8%)
08/01 05:54:57 PM | Train: [ 3/210] Step 5380/6092 Loss 5.664 Prec@(1,5) (3.9%, 12.8%)
08/01 05:55:16 PM | Train: [ 3/210] Step 5390/6092 Loss 5.664 Prec@(1,5) (3.9%, 12.8%)
08/01 05:55:32 PM | Train: [ 3/210] Step 5400/6092 Loss 5.664 Prec@(1,5) (3.9%, 12.8%)
08/01 05:55:50 PM | Train: [ 3/210] Step 5410/6092 Loss 5.664 Prec@(1,5) (4.0%, 12.8%)
08/01 05:56:07 PM | Train: [ 3/210] Step 5420/6092 Loss 5.663 Prec@(1,5) (4.0%, 12.8%)
08/01 05:56:25 PM | Train: [ 3/210] Step 5430/6092 Loss 5.662 Prec@(1,5) (4.0%, 12.9%)
08/01 05:56:42 PM | Train: [ 3/210] Step 5440/6092 Loss 5.663 Prec@(1,5) (4.0%, 12.8%)
08/01 05:57:00 PM | Train: [ 3/210] Step 5450/6092 Loss 5.662 Prec@(1,5) (4.0%, 12.8%)
08/01 05:57:17 PM | Train: [ 3/210] Step 5460/6092 Loss 5.662 Prec@(1,5) (4.0%, 12.8%)
08/01 05:57:34 PM | Train: [ 3/210] Step 5470/6092 Loss 5.662 Prec@(1,5) (4.0%, 12.8%)
08/01 05:57:51 PM | Train: [ 3/210] Step 5480/6092 Loss 5.663 Prec@(1,5) (4.0%, 12.8%)
08/01 05:58:09 PM | Train: [ 3/210] Step 5490/6092 Loss 5.662 Prec@(1,5) (4.0%, 12.8%)
08/01 05:58:27 PM | Train: [ 3/210] Step 5500/6092 Loss 5.663 Prec@(1,5) (3.9%, 12.8%)
08/01 05:58:45 PM | Train: [ 3/210] Step 5510/6092 Loss 5.663 Prec@(1,5) (3.9%, 12.9%)
08/01 05:59:02 PM | Train: [ 3/210] Step 5520/6092 Loss 5.663 Prec@(1,5) (3.9%, 12.9%)
08/01 05:59:19 PM | Train: [ 3/210] Step 5530/6092 Loss 5.662 Prec@(1,5) (4.0%, 12.9%)
08/01 05:59:38 PM | Train: [ 3/210] Step 5540/6092 Loss 5.661 Prec@(1,5) (4.0%, 12.9%)
08/01 05:59:55 PM | Train: [ 3/210] Step 5550/6092 Loss 5.660 Prec@(1,5) (4.0%, 12.9%)
08/01 06:00:13 PM | Train: [ 3/210] Step 5560/6092 Loss 5.660 Prec@(1,5) (4.0%, 12.9%)
08/01 06:00:31 PM | Train: [ 3/210] Step 5570/6092 Loss 5.659 Prec@(1,5) (4.0%, 12.9%)
08/01 06:00:47 PM | Train: [ 3/210] Step 5580/6092 Loss 5.658 Prec@(1,5) (4.0%, 13.0%)
08/01 06:01:05 PM | Train: [ 3/210] Step 5590/6092 Loss 5.658 Prec@(1,5) (4.0%, 12.9%)
08/01 06:01:22 PM | Train: [ 3/210] Step 5600/6092 Loss 5.657 Prec@(1,5) (4.0%, 12.9%)
08/01 06:01:40 PM | Train: [ 3/210] Step 5610/6092 Loss 5.657 Prec@(1,5) (4.0%, 12.9%)
08/01 06:01:57 PM | Train: [ 3/210] Step 5620/6092 Loss 5.657 Prec@(1,5) (4.0%, 13.0%)
08/01 06:02:15 PM | Train: [ 3/210] Step 5630/6092 Loss 5.657 Prec@(1,5) (4.0%, 13.0%)
08/01 06:02:32 PM | Train: [ 3/210] Step 5640/6092 Loss 5.657 Prec@(1,5) (4.0%, 13.0%)
08/01 06:02:51 PM | Train: [ 3/210] Step 5650/6092 Loss 5.657 Prec@(1,5) (4.0%, 13.0%)
08/01 06:03:09 PM | Train: [ 3/210] Step 5660/6092 Loss 5.655 Prec@(1,5) (4.0%, 13.0%)
08/01 06:03:27 PM | Train: [ 3/210] Step 5670/6092 Loss 5.654 Prec@(1,5) (4.0%, 13.0%)
08/01 06:03:44 PM | Train: [ 3/210] Step 5680/6092 Loss 5.655 Prec@(1,5) (4.0%, 13.0%)
08/01 06:04:01 PM | Train: [ 3/210] Step 5690/6092 Loss 5.655 Prec@(1,5) (4.0%, 13.0%)
08/01 06:04:19 PM | Train: [ 3/210] Step 5700/6092 Loss 5.654 Prec@(1,5) (4.0%, 13.0%)
08/01 06:04:36 PM | Train: [ 3/210] Step 5710/6092 Loss 5.653 Prec@(1,5) (4.0%, 13.0%)
08/01 06:04:54 PM | Train: [ 3/210] Step 5720/6092 Loss 5.653 Prec@(1,5) (4.0%, 13.0%)
08/01 06:05:11 PM | Train: [ 3/210] Step 5730/6092 Loss 5.653 Prec@(1,5) (4.0%, 13.1%)
08/01 06:05:29 PM | Train: [ 3/210] Step 5740/6092 Loss 5.652 Prec@(1,5) (4.0%, 13.0%)
08/01 06:05:46 PM | Train: [ 3/210] Step 5750/6092 Loss 5.652 Prec@(1,5) (4.0%, 13.1%)
08/01 06:06:03 PM | Train: [ 3/210] Step 5760/6092 Loss 5.652 Prec@(1,5) (4.0%, 13.1%)
08/01 06:06:22 PM | Train: [ 3/210] Step 5770/6092 Loss 5.652 Prec@(1,5) (4.0%, 13.1%)
08/01 06:06:39 PM | Train: [ 3/210] Step 5780/6092 Loss 5.651 Prec@(1,5) (4.0%, 13.1%)
08/01 06:06:56 PM | Train: [ 3/210] Step 5790/6092 Loss 5.651 Prec@(1,5) (4.0%, 13.1%)
08/01 06:07:14 PM | Train: [ 3/210] Step 5800/6092 Loss 5.651 Prec@(1,5) (4.0%, 13.0%)
08/01 06:07:31 PM | Train: [ 3/210] Step 5810/6092 Loss 5.650 Prec@(1,5) (4.0%, 13.1%)
08/01 06:07:50 PM | Train: [ 3/210] Step 5820/6092 Loss 5.650 Prec@(1,5) (4.0%, 13.0%)
08/01 06:08:08 PM | Train: [ 3/210] Step 5830/6092 Loss 5.650 Prec@(1,5) (4.0%, 13.0%)
08/01 06:08:26 PM | Train: [ 3/210] Step 5840/6092 Loss 5.650 Prec@(1,5) (4.0%, 13.0%)
08/01 06:08:43 PM | Train: [ 3/210] Step 5850/6092 Loss 5.650 Prec@(1,5) (4.0%, 13.0%)
08/01 06:09:01 PM | Train: [ 3/210] Step 5860/6092 Loss 5.649 Prec@(1,5) (4.0%, 13.0%)
08/01 06:09:19 PM | Train: [ 3/210] Step 5870/6092 Loss 5.649 Prec@(1,5) (4.0%, 13.0%)
08/01 06:09:36 PM | Train: [ 3/210] Step 5880/6092 Loss 5.649 Prec@(1,5) (4.0%, 13.0%)
08/01 06:09:54 PM | Train: [ 3/210] Step 5890/6092 Loss 5.649 Prec@(1,5) (4.0%, 13.0%)
08/01 06:10:11 PM | Train: [ 3/210] Step 5900/6092 Loss 5.649 Prec@(1,5) (4.0%, 13.0%)
08/01 06:10:29 PM | Train: [ 3/210] Step 5910/6092 Loss 5.649 Prec@(1,5) (4.0%, 13.1%)
08/01 06:10:46 PM | Train: [ 3/210] Step 5920/6092 Loss 5.649 Prec@(1,5) (4.0%, 13.1%)
08/01 06:11:04 PM | Train: [ 3/210] Step 5930/6092 Loss 5.648 Prec@(1,5) (4.0%, 13.1%)
08/01 06:11:21 PM | Train: [ 3/210] Step 5940/6092 Loss 5.647 Prec@(1,5) (4.0%, 13.1%)
08/01 06:11:38 PM | Train: [ 3/210] Step 5950/6092 Loss 5.648 Prec@(1,5) (4.0%, 13.1%)
08/01 06:11:57 PM | Train: [ 3/210] Step 5960/6092 Loss 5.647 Prec@(1,5) (4.0%, 13.1%)
08/01 06:12:14 PM | Train: [ 3/210] Step 5970/6092 Loss 5.647 Prec@(1,5) (4.0%, 13.1%)
08/01 06:12:32 PM | Train: [ 3/210] Step 5980/6092 Loss 5.646 Prec@(1,5) (4.0%, 13.1%)
08/01 06:12:49 PM | Train: [ 3/210] Step 5990/6092 Loss 5.647 Prec@(1,5) (4.0%, 13.1%)
08/01 06:13:07 PM | Train: [ 3/210] Step 6000/6092 Loss 5.647 Prec@(1,5) (4.0%, 13.1%)
08/01 06:13:24 PM | Train: [ 3/210] Step 6010/6092 Loss 5.646 Prec@(1,5) (4.0%, 13.1%)
08/01 06:13:42 PM | Train: [ 3/210] Step 6020/6092 Loss 5.646 Prec@(1,5) (4.0%, 13.1%)
08/01 06:14:02 PM | Train: [ 3/210] Step 6030/6092 Loss 5.646 Prec@(1,5) (4.0%, 13.1%)
08/01 06:14:20 PM | Train: [ 3/210] Step 6040/6092 Loss 5.646 Prec@(1,5) (4.0%, 13.1%)
08/01 06:14:38 PM | Train: [ 3/210] Step 6050/6092 Loss 5.646 Prec@(1,5) (4.0%, 13.1%)
08/01 06:14:55 PM | Train: [ 3/210] Step 6060/6092 Loss 5.645 Prec@(1,5) (4.0%, 13.1%)
08/01 06:15:13 PM | Train: [ 3/210] Step 6070/6092 Loss 5.645 Prec@(1,5) (4.0%, 13.1%)
08/01 06:15:30 PM | Train: [ 3/210] Step 6080/6092 Loss 5.644 Prec@(1,5) (4.0%, 13.1%)
08/01 06:15:47 PM | Train: [ 3/210] Step 6090/6092 Loss 5.643 Prec@(1,5) (4.0%, 13.1%)
08/01 06:15:50 PM | Train: [ 3/210] Step 6092/6092 Loss 5.643 Prec@(1,5) (4.0%, 13.1%)
08/01 06:15:52 PM | Train: [ 3/210] Final Prec@1 3.9885%
08/01 06:17:02 PM | Valid: [ 3/210] Step 000/375 Loss 7.464 Prec@(1,5) (0.0%, 0.0%)
08/01 06:17:03 PM | Valid: [ 3/210] Step 010/375 Loss 7.555 Prec@(1,5) (0.0%, 13.6%)
08/01 06:17:04 PM | Valid: [ 3/210] Step 020/375 Loss 7.382 Prec@(1,5) (0.0%, 7.1%)
08/01 06:17:05 PM | Valid: [ 3/210] Step 030/375 Loss 7.103 Prec@(1,5) (1.6%, 8.1%)
08/01 06:17:06 PM | Valid: [ 3/210] Step 040/375 Loss 7.001 Prec@(1,5) (2.4%, 11.0%)
08/01 06:17:07 PM | Valid: [ 3/210] Step 050/375 Loss 7.397 Prec@(1,5) (2.0%, 8.8%)
08/01 06:17:07 PM | Valid: [ 3/210] Step 060/375 Loss 7.542 Prec@(1,5) (3.3%, 9.8%)
08/01 06:17:08 PM | Valid: [ 3/210] Step 070/375 Loss 7.279 Prec@(1,5) (4.2%, 11.3%)
08/01 06:17:09 PM | Valid: [ 3/210] Step 080/375 Loss 7.155 Prec@(1,5) (4.9%, 11.1%)
08/01 06:17:10 PM | Valid: [ 3/210] Step 090/375 Loss 7.249 Prec@(1,5) (4.9%, 11.5%)
08/01 06:17:11 PM | Valid: [ 3/210] Step 100/375 Loss 7.471 Prec@(1,5) (4.5%, 10.4%)
08/01 06:17:12 PM | Valid: [ 3/210] Step 110/375 Loss 7.382 Prec@(1,5) (4.1%, 9.9%)
08/01 06:17:13 PM | Valid: [ 3/210] Step 120/375 Loss 7.446 Prec@(1,5) (4.1%, 9.9%)
08/01 06:17:14 PM | Valid: [ 3/210] Step 130/375 Loss 7.538 Prec@(1,5) (4.2%, 9.5%)
08/01 06:17:15 PM | Valid: [ 3/210] Step 140/375 Loss 7.560 Prec@(1,5) (3.9%, 9.6%)
08/01 06:17:16 PM | Valid: [ 3/210] Step 150/375 Loss 7.550 Prec@(1,5) (3.6%, 9.6%)
08/01 06:17:16 PM | Valid: [ 3/210] Step 160/375 Loss 7.509 Prec@(1,5) (3.7%, 9.9%)
08/01 06:17:17 PM | Valid: [ 3/210] Step 170/375 Loss 7.495 Prec@(1,5) (3.5%, 9.6%)
08/01 06:17:18 PM | Valid: [ 3/210] Step 180/375 Loss 7.464 Prec@(1,5) (3.3%, 9.4%)
08/01 06:17:19 PM | Valid: [ 3/210] Step 190/375 Loss 7.385 Prec@(1,5) (3.1%, 9.7%)
08/01 06:17:20 PM | Valid: [ 3/210] Step 200/375 Loss 7.326 Prec@(1,5) (3.0%, 10.4%)
08/01 06:17:21 PM | Valid: [ 3/210] Step 210/375 Loss 7.317 Prec@(1,5) (2.8%, 10.7%)
08/01 06:17:21 PM | Valid: [ 3/210] Step 220/375 Loss 7.281 Prec@(1,5) (2.9%, 10.4%)
08/01 06:17:22 PM | Valid: [ 3/210] Step 230/375 Loss 7.264 Prec@(1,5) (2.8%, 10.4%)
08/01 06:17:23 PM | Valid: [ 3/210] Step 240/375 Loss 7.253 Prec@(1,5) (2.9%, 10.2%)
08/01 06:17:24 PM | Valid: [ 3/210] Step 250/375 Loss 7.255 Prec@(1,5) (2.8%, 9.8%)
08/01 06:17:25 PM | Valid: [ 3/210] Step 260/375 Loss 7.188 Prec@(1,5) (3.1%, 10.0%)
08/01 06:17:26 PM | Valid: [ 3/210] Step 270/375 Loss 7.144 Prec@(1,5) (3.1%, 10.1%)
08/01 06:17:27 PM | Valid: [ 3/210] Step 280/375 Loss 7.150 Prec@(1,5) (3.0%, 10.1%)
08/01 06:17:28 PM | Valid: [ 3/210] Step 290/375 Loss 7.225 Prec@(1,5) (2.9%, 10.0%)
08/01 06:17:29 PM | Valid: [ 3/210] Step 300/375 Loss 7.222 Prec@(1,5) (2.8%, 9.8%)
08/01 06:17:30 PM | Valid: [ 3/210] Step 310/375 Loss 7.209 Prec@(1,5) (2.7%, 9.5%)
08/01 06:17:31 PM | Valid: [ 3/210] Step 320/375 Loss 7.143 Prec@(1,5) (3.0%, 10.1%)
08/01 06:17:31 PM | Valid: [ 3/210] Step 330/375 Loss 7.166 Prec@(1,5) (2.9%, 10.3%)
08/01 06:17:32 PM | Valid: [ 3/210] Step 340/375 Loss 7.131 Prec@(1,5) (2.9%, 10.4%)
08/01 06:17:33 PM | Valid: [ 3/210] Step 350/375 Loss 7.139 Prec@(1,5) (2.8%, 10.1%)
08/01 06:17:34 PM | Valid: [ 3/210] Step 360/375 Loss 7.184 Prec@(1,5) (2.9%, 10.1%)
08/01 06:17:35 PM | Valid: [ 3/210] Step 370/375 Loss 7.174 Prec@(1,5) (3.1%, 10.1%)
08/01 06:17:37 PM | Valid: [ 3/210] Final Prec@1 3.0667%, Prec@5 10.1333%, Prec@10 19.4667%
08/01 06:17:37 PM | Final best Prec@1 = 3.0667%
####### ALPHA #######
# Alpha - normal
tensor([[0.2056, 0.1358, 0.3909, 0.0423, 0.0243, 0.0305, 0.1486, 0.0220],
        [0.4426, 0.1126, 0.2961, 0.0179, 0.0201, 0.0144, 0.0695, 0.0268]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1349, 0.0952, 0.1914, 0.4244, 0.0220, 0.0387, 0.0345, 0.0589],
        [0.1684, 0.1103, 0.3646, 0.0379, 0.0130, 0.0671, 0.0285, 0.2101],
        [0.5053, 0.1226, 0.0698, 0.0255, 0.0409, 0.0178, 0.1353, 0.0829]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0660, 0.0505, 0.0663, 0.0303, 0.0623, 0.5973, 0.1057, 0.0215],
        [0.2045, 0.1051, 0.1854, 0.0484, 0.0398, 0.1158, 0.1294, 0.1716],
        [0.7538, 0.0950, 0.0464, 0.0180, 0.0112, 0.0126, 0.0428, 0.0203],
        [0.3911, 0.2089, 0.0961, 0.0855, 0.0209, 0.0552, 0.0890, 0.0532]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1753, 0.0682, 0.0956, 0.0171, 0.0643, 0.0443, 0.5119, 0.0234],
        [0.1253, 0.0539, 0.0695, 0.2384, 0.0684, 0.2946, 0.0812, 0.0688],
        [0.8433, 0.0545, 0.0261, 0.0206, 0.0156, 0.0096, 0.0127, 0.0176],
        [0.2374, 0.1657, 0.1041, 0.0305, 0.1882, 0.1094, 0.0802, 0.0845],
        [0.0661, 0.0617, 0.3594, 0.0542, 0.0235, 0.0221, 0.3829, 0.0301]],
       grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1504, 0.0186, 0.1542, 0.1950, 0.0939, 0.1665, 0.1061, 0.1152],
        [0.2050, 0.0280, 0.1422, 0.0716, 0.1046, 0.0932, 0.1302, 0.2253]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.4605, 0.0133, 0.1805, 0.0676, 0.0427, 0.0832, 0.0734, 0.0787],
        [0.1213, 0.0219, 0.1190, 0.1046, 0.2193, 0.1080, 0.0265, 0.2794],
        [0.0888, 0.0069, 0.0146, 0.1046, 0.0234, 0.0133, 0.0175, 0.7309]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.3923, 0.0181, 0.0643, 0.0614, 0.0734, 0.0590, 0.1097, 0.2218],
        [0.2257, 0.0128, 0.0779, 0.0456, 0.1479, 0.0382, 0.0347, 0.4173],
        [0.2001, 0.0057, 0.0110, 0.1266, 0.4282, 0.0212, 0.0373, 0.1699],
        [0.0628, 0.0102, 0.0128, 0.0312, 0.0799, 0.0169, 0.0263, 0.7599]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.4458, 0.0073, 0.1503, 0.0384, 0.1162, 0.0865, 0.0436, 0.1119],
        [0.1920, 0.0184, 0.0687, 0.0395, 0.0558, 0.0578, 0.0551, 0.5126],
        [0.0371, 0.0042, 0.0070, 0.0286, 0.0167, 0.0214, 0.0152, 0.8696],
        [0.1473, 0.0103, 0.0116, 0.0504, 0.1347, 0.0182, 0.0494, 0.5781],
        [0.0296, 0.0099, 0.0106, 0.0563, 0.0675, 0.1290, 0.0358, 0.6613]],
       grad_fn=<SoftmaxBackward>)
#####################
08/01 06:23:15 PM | Train: [ 4/210] Step 000/6092 Loss 6.706 Prec@(1,5) (0.0%, 0.0%)
08/01 06:23:31 PM | Train: [ 4/210] Step 010/6092 Loss 5.071 Prec@(1,5) (13.6%, 27.3%)
08/01 06:23:48 PM | Train: [ 4/210] Step 020/6092 Loss 4.938 Prec@(1,5) (11.9%, 28.6%)
08/01 06:24:04 PM | Train: [ 4/210] Step 030/6092 Loss 5.018 Prec@(1,5) (9.7%, 25.8%)
08/01 06:24:21 PM | Train: [ 4/210] Step 040/6092 Loss 5.188 Prec@(1,5) (8.5%, 22.0%)
08/01 06:24:37 PM | Train: [ 4/210] Step 050/6092 Loss 5.345 Prec@(1,5) (6.9%, 18.6%)
08/01 06:24:54 PM | Train: [ 4/210] Step 060/6092 Loss 5.272 Prec@(1,5) (5.7%, 19.7%)
08/01 06:25:13 PM | Train: [ 4/210] Step 070/6092 Loss 5.341 Prec@(1,5) (5.6%, 17.6%)
08/01 06:25:30 PM | Train: [ 4/210] Step 080/6092 Loss 5.372 Prec@(1,5) (4.9%, 17.3%)
08/01 06:25:47 PM | Train: [ 4/210] Step 090/6092 Loss 5.369 Prec@(1,5) (4.9%, 17.0%)
08/01 06:26:04 PM | Train: [ 4/210] Step 100/6092 Loss 5.402 Prec@(1,5) (4.5%, 16.3%)
08/01 06:26:21 PM | Train: [ 4/210] Step 110/6092 Loss 5.436 Prec@(1,5) (4.1%, 15.8%)
08/01 06:26:39 PM | Train: [ 4/210] Step 120/6092 Loss 5.505 Prec@(1,5) (3.7%, 14.9%)
08/01 06:26:56 PM | Train: [ 4/210] Step 130/6092 Loss 5.503 Prec@(1,5) (3.4%, 14.9%)
08/01 06:27:14 PM | Train: [ 4/210] Step 140/6092 Loss 5.447 Prec@(1,5) (4.3%, 16.0%)
08/01 06:27:31 PM | Train: [ 4/210] Step 150/6092 Loss 5.428 Prec@(1,5) (4.6%, 15.9%)
08/01 06:27:49 PM | Train: [ 4/210] Step 160/6092 Loss 5.417 Prec@(1,5) (5.0%, 16.5%)
08/01 06:28:06 PM | Train: [ 4/210] Step 170/6092 Loss 5.408 Prec@(1,5) (5.6%, 17.0%)
08/01 06:28:25 PM | Train: [ 4/210] Step 180/6092 Loss 5.394 Prec@(1,5) (5.8%, 17.7%)
08/01 06:28:42 PM | Train: [ 4/210] Step 190/6092 Loss 5.405 Prec@(1,5) (5.8%, 18.1%)
08/01 06:28:59 PM | Train: [ 4/210] Step 200/6092 Loss 5.406 Prec@(1,5) (6.0%, 17.7%)
08/01 06:29:16 PM | Train: [ 4/210] Step 210/6092 Loss 5.424 Prec@(1,5) (6.2%, 17.3%)
08/01 06:29:34 PM | Train: [ 4/210] Step 220/6092 Loss 5.419 Prec@(1,5) (6.1%, 17.0%)
08/01 06:29:52 PM | Train: [ 4/210] Step 230/6092 Loss 5.404 Prec@(1,5) (6.9%, 17.5%)
08/01 06:30:13 PM | Train: [ 4/210] Step 240/6092 Loss 5.419 Prec@(1,5) (6.8%, 17.2%)
08/01 06:30:30 PM | Train: [ 4/210] Step 250/6092 Loss 5.423 Prec@(1,5) (6.6%, 16.5%)
08/01 06:30:47 PM | Train: [ 4/210] Step 260/6092 Loss 5.419 Prec@(1,5) (6.5%, 16.7%)
08/01 06:31:04 PM | Train: [ 4/210] Step 270/6092 Loss 5.430 Prec@(1,5) (6.5%, 16.6%)
08/01 06:31:22 PM | Train: [ 4/210] Step 280/6092 Loss 5.438 Prec@(1,5) (6.2%, 16.4%)
08/01 06:31:40 PM | Train: [ 4/210] Step 290/6092 Loss 5.439 Prec@(1,5) (6.4%, 16.5%)
08/01 06:31:58 PM | Train: [ 4/210] Step 300/6092 Loss 5.444 Prec@(1,5) (6.1%, 16.4%)
08/01 06:32:17 PM | Train: [ 4/210] Step 310/6092 Loss 5.447 Prec@(1,5) (6.1%, 16.2%)
08/01 06:32:34 PM | Train: [ 4/210] Step 320/6092 Loss 5.443 Prec@(1,5) (6.1%, 16.4%)
08/01 06:32:52 PM | Train: [ 4/210] Step 330/6092 Loss 5.441 Prec@(1,5) (6.0%, 16.5%)
08/01 06:33:09 PM | Train: [ 4/210] Step 340/6092 Loss 5.442 Prec@(1,5) (6.5%, 16.7%)
08/01 06:33:26 PM | Train: [ 4/210] Step 350/6092 Loss 5.456 Prec@(1,5) (6.4%, 16.4%)
08/01 06:33:43 PM | Train: [ 4/210] Step 360/6092 Loss 5.463 Prec@(1,5) (6.2%, 16.1%)
08/01 06:34:01 PM | Train: [ 4/210] Step 370/6092 Loss 5.453 Prec@(1,5) (6.2%, 15.9%)
08/01 06:34:19 PM | Train: [ 4/210] Step 380/6092 Loss 5.468 Prec@(1,5) (6.0%, 15.6%)
08/01 06:34:36 PM | Train: [ 4/210] Step 390/6092 Loss 5.462 Prec@(1,5) (6.0%, 15.6%)
08/01 06:34:53 PM | Train: [ 4/210] Step 400/6092 Loss 5.469 Prec@(1,5) (5.9%, 15.3%)
08/01 06:35:13 PM | Train: [ 4/210] Step 410/6092 Loss 5.450 Prec@(1,5) (5.7%, 15.8%)
08/01 06:35:30 PM | Train: [ 4/210] Step 420/6092 Loss 5.446 Prec@(1,5) (5.6%, 16.2%)
08/01 06:35:48 PM | Train: [ 4/210] Step 430/6092 Loss 5.444 Prec@(1,5) (5.6%, 16.1%)
08/01 06:36:05 PM | Train: [ 4/210] Step 440/6092 Loss 5.453 Prec@(1,5) (5.4%, 15.9%)
08/01 06:36:22 PM | Train: [ 4/210] Step 450/6092 Loss 5.458 Prec@(1,5) (5.5%, 15.7%)
08/01 06:36:40 PM | Train: [ 4/210] Step 460/6092 Loss 5.449 Prec@(1,5) (5.5%, 16.1%)
08/01 06:36:57 PM | Train: [ 4/210] Step 470/6092 Loss 5.452 Prec@(1,5) (5.5%, 15.9%)
08/01 06:37:14 PM | Train: [ 4/210] Step 480/6092 Loss 5.449 Prec@(1,5) (5.4%, 15.9%)
08/01 06:37:32 PM | Train: [ 4/210] Step 490/6092 Loss 5.456 Prec@(1,5) (5.4%, 15.7%)
08/01 06:37:49 PM | Train: [ 4/210] Step 500/6092 Loss 5.461 Prec@(1,5) (5.4%, 15.6%)
08/01 06:38:07 PM | Train: [ 4/210] Step 510/6092 Loss 5.456 Prec@(1,5) (5.4%, 15.7%)
08/01 06:38:24 PM | Train: [ 4/210] Step 520/6092 Loss 5.453 Prec@(1,5) (5.4%, 15.8%)
08/01 06:38:42 PM | Train: [ 4/210] Step 530/6092 Loss 5.451 Prec@(1,5) (5.3%, 15.6%)
08/01 06:38:59 PM | Train: [ 4/210] Step 540/6092 Loss 5.450 Prec@(1,5) (5.4%, 15.6%)
08/01 06:39:16 PM | Train: [ 4/210] Step 550/6092 Loss 5.450 Prec@(1,5) (5.4%, 15.5%)
08/01 06:39:34 PM | Train: [ 4/210] Step 560/6092 Loss 5.444 Prec@(1,5) (5.3%, 15.4%)
08/01 06:39:51 PM | Train: [ 4/210] Step 570/6092 Loss 5.442 Prec@(1,5) (5.2%, 15.3%)
08/01 06:40:09 PM | Train: [ 4/210] Step 580/6092 Loss 5.456 Prec@(1,5) (5.1%, 15.1%)
08/01 06:40:27 PM | Train: [ 4/210] Step 590/6092 Loss 5.463 Prec@(1,5) (5.0%, 15.0%)
08/01 06:40:44 PM | Train: [ 4/210] Step 600/6092 Loss 5.476 Prec@(1,5) (5.0%, 14.9%)
08/01 06:41:01 PM | Train: [ 4/210] Step 610/6092 Loss 5.468 Prec@(1,5) (5.0%, 15.1%)
08/01 06:41:20 PM | Train: [ 4/210] Step 620/6092 Loss 5.473 Prec@(1,5) (4.9%, 14.9%)
08/01 06:41:37 PM | Train: [ 4/210] Step 630/6092 Loss 5.473 Prec@(1,5) (4.8%, 14.8%)
08/01 06:41:54 PM | Train: [ 4/210] Step 640/6092 Loss 5.470 Prec@(1,5) (4.9%, 15.0%)
08/01 06:42:12 PM | Train: [ 4/210] Step 650/6092 Loss 5.460 Prec@(1,5) (5.1%, 15.1%)
08/01 06:42:30 PM | Train: [ 4/210] Step 660/6092 Loss 5.457 Prec@(1,5) (5.1%, 15.4%)
08/01 06:42:49 PM | Train: [ 4/210] Step 670/6092 Loss 5.457 Prec@(1,5) (5.2%, 15.3%)
08/01 06:43:07 PM | Train: [ 4/210] Step 680/6092 Loss 5.462 Prec@(1,5) (5.1%, 15.4%)
08/01 06:43:24 PM | Train: [ 4/210] Step 690/6092 Loss 5.459 Prec@(1,5) (5.1%, 15.5%)
08/01 06:43:42 PM | Train: [ 4/210] Step 700/6092 Loss 5.457 Prec@(1,5) (5.3%, 15.6%)
08/01 06:44:01 PM | Train: [ 4/210] Step 710/6092 Loss 5.453 Prec@(1,5) (5.3%, 15.6%)
08/01 06:44:19 PM | Train: [ 4/210] Step 720/6092 Loss 5.448 Prec@(1,5) (5.5%, 15.7%)
08/01 06:44:38 PM | Train: [ 4/210] Step 730/6092 Loss 5.446 Prec@(1,5) (5.4%, 15.7%)
08/01 06:44:57 PM | Train: [ 4/210] Step 740/6092 Loss 5.444 Prec@(1,5) (5.4%, 15.7%)
08/01 06:45:16 PM | Train: [ 4/210] Step 750/6092 Loss 5.431 Prec@(1,5) (5.6%, 15.9%)
08/01 06:45:34 PM | Train: [ 4/210] Step 760/6092 Loss 5.428 Prec@(1,5) (5.5%, 15.8%)
08/01 06:45:51 PM | Train: [ 4/210] Step 770/6092 Loss 5.428 Prec@(1,5) (5.5%, 15.8%)
08/01 06:46:09 PM | Train: [ 4/210] Step 780/6092 Loss 5.433 Prec@(1,5) (5.4%, 15.6%)
08/01 06:46:26 PM | Train: [ 4/210] Step 790/6092 Loss 5.433 Prec@(1,5) (5.4%, 15.7%)
08/01 06:46:43 PM | Train: [ 4/210] Step 800/6092 Loss 5.431 Prec@(1,5) (5.4%, 15.8%)
08/01 06:47:01 PM | Train: [ 4/210] Step 810/6092 Loss 5.427 Prec@(1,5) (5.4%, 15.8%)
08/01 06:47:18 PM | Train: [ 4/210] Step 820/6092 Loss 5.425 Prec@(1,5) (5.4%, 15.9%)
08/01 06:47:36 PM | Train: [ 4/210] Step 830/6092 Loss 5.427 Prec@(1,5) (5.3%, 15.8%)
08/01 06:47:54 PM | Train: [ 4/210] Step 840/6092 Loss 5.431 Prec@(1,5) (5.3%, 15.8%)
08/01 06:48:12 PM | Train: [ 4/210] Step 850/6092 Loss 5.433 Prec@(1,5) (5.2%, 15.8%)
08/01 06:48:29 PM | Train: [ 4/210] Step 860/6092 Loss 5.438 Prec@(1,5) (5.2%, 15.7%)
08/01 06:48:47 PM | Train: [ 4/210] Step 870/6092 Loss 5.428 Prec@(1,5) (5.3%, 15.8%)
08/01 06:49:04 PM | Train: [ 4/210] Step 880/6092 Loss 5.434 Prec@(1,5) (5.2%, 15.8%)
08/01 06:49:21 PM | Train: [ 4/210] Step 890/6092 Loss 5.434 Prec@(1,5) (5.2%, 15.8%)
08/01 06:49:39 PM | Train: [ 4/210] Step 900/6092 Loss 5.433 Prec@(1,5) (5.2%, 15.7%)
08/01 06:49:56 PM | Train: [ 4/210] Step 910/6092 Loss 5.423 Prec@(1,5) (5.4%, 15.9%)
08/01 06:50:13 PM | Train: [ 4/210] Step 920/6092 Loss 5.423 Prec@(1,5) (5.5%, 16.0%)
08/01 06:50:32 PM | Train: [ 4/210] Step 930/6092 Loss 5.426 Prec@(1,5) (5.5%, 15.8%)
08/01 06:50:49 PM | Train: [ 4/210] Step 940/6092 Loss 5.428 Prec@(1,5) (5.5%, 15.8%)
08/01 06:51:06 PM | Train: [ 4/210] Step 950/6092 Loss 5.428 Prec@(1,5) (5.5%, 15.8%)
08/01 06:51:23 PM | Train: [ 4/210] Step 960/6092 Loss 5.426 Prec@(1,5) (5.6%, 15.8%)
08/01 06:51:41 PM | Train: [ 4/210] Step 970/6092 Loss 5.423 Prec@(1,5) (5.6%, 15.8%)
08/01 06:51:59 PM | Train: [ 4/210] Step 980/6092 Loss 5.423 Prec@(1,5) (5.6%, 15.9%)
08/01 06:52:16 PM | Train: [ 4/210] Step 990/6092 Loss 5.417 Prec@(1,5) (5.5%, 15.9%)
08/01 06:52:33 PM | Train: [ 4/210] Step 1000/6092 Loss 5.415 Prec@(1,5) (5.5%, 15.8%)
08/01 06:52:50 PM | Train: [ 4/210] Step 1010/6092 Loss 5.415 Prec@(1,5) (5.6%, 15.9%)
08/01 06:53:08 PM | Train: [ 4/210] Step 1020/6092 Loss 5.410 Prec@(1,5) (5.7%, 15.9%)
08/01 06:53:25 PM | Train: [ 4/210] Step 1030/6092 Loss 5.402 Prec@(1,5) (5.8%, 16.1%)
08/01 06:53:43 PM | Train: [ 4/210] Step 1040/6092 Loss 5.399 Prec@(1,5) (5.9%, 16.2%)
08/01 06:54:00 PM | Train: [ 4/210] Step 1050/6092 Loss 5.393 Prec@(1,5) (5.9%, 16.3%)
08/01 06:54:17 PM | Train: [ 4/210] Step 1060/6092 Loss 5.393 Prec@(1,5) (5.9%, 16.2%)
08/01 06:54:35 PM | Train: [ 4/210] Step 1070/6092 Loss 5.397 Prec@(1,5) (5.8%, 16.1%)
08/01 06:54:52 PM | Train: [ 4/210] Step 1080/6092 Loss 5.393 Prec@(1,5) (5.8%, 16.1%)
08/01 06:55:10 PM | Train: [ 4/210] Step 1090/6092 Loss 5.393 Prec@(1,5) (5.8%, 16.2%)
08/01 06:55:27 PM | Train: [ 4/210] Step 1100/6092 Loss 5.392 Prec@(1,5) (5.8%, 16.3%)
08/01 06:55:44 PM | Train: [ 4/210] Step 1110/6092 Loss 5.394 Prec@(1,5) (5.8%, 16.3%)
08/01 06:56:01 PM | Train: [ 4/210] Step 1120/6092 Loss 5.393 Prec@(1,5) (5.8%, 16.2%)
08/01 06:56:19 PM | Train: [ 4/210] Step 1130/6092 Loss 5.392 Prec@(1,5) (5.8%, 16.3%)
08/01 06:56:36 PM | Train: [ 4/210] Step 1140/6092 Loss 5.388 Prec@(1,5) (5.8%, 16.3%)
08/01 06:56:53 PM | Train: [ 4/210] Step 1150/6092 Loss 5.393 Prec@(1,5) (5.8%, 16.2%)
08/01 06:57:10 PM | Train: [ 4/210] Step 1160/6092 Loss 5.396 Prec@(1,5) (5.8%, 16.2%)
08/01 06:57:28 PM | Train: [ 4/210] Step 1170/6092 Loss 5.398 Prec@(1,5) (5.8%, 16.1%)
08/01 06:57:45 PM | Train: [ 4/210] Step 1180/6092 Loss 5.392 Prec@(1,5) (5.8%, 16.2%)
08/01 06:58:02 PM | Train: [ 4/210] Step 1190/6092 Loss 5.395 Prec@(1,5) (5.8%, 16.1%)
08/01 06:58:21 PM | Train: [ 4/210] Step 1200/6092 Loss 5.398 Prec@(1,5) (5.8%, 16.1%)
08/01 06:58:37 PM | Train: [ 4/210] Step 1210/6092 Loss 5.397 Prec@(1,5) (5.8%, 16.1%)
08/01 06:58:55 PM | Train: [ 4/210] Step 1220/6092 Loss 5.398 Prec@(1,5) (5.8%, 16.1%)
08/01 06:59:13 PM | Train: [ 4/210] Step 1230/6092 Loss 5.392 Prec@(1,5) (5.8%, 16.3%)
08/01 06:59:31 PM | Train: [ 4/210] Step 1240/6092 Loss 5.391 Prec@(1,5) (5.8%, 16.3%)
08/01 06:59:48 PM | Train: [ 4/210] Step 1250/6092 Loss 5.393 Prec@(1,5) (5.8%, 16.3%)
08/01 07:00:06 PM | Train: [ 4/210] Step 1260/6092 Loss 5.388 Prec@(1,5) (5.9%, 16.4%)
08/01 07:00:27 PM | Train: [ 4/210] Step 1270/6092 Loss 5.388 Prec@(1,5) (5.8%, 16.4%)
08/01 07:00:43 PM | Train: [ 4/210] Step 1280/6092 Loss 5.390 Prec@(1,5) (5.9%, 16.4%)
08/01 07:01:01 PM | Train: [ 4/210] Step 1290/6092 Loss 5.392 Prec@(1,5) (5.8%, 16.4%)
08/01 07:01:18 PM | Train: [ 4/210] Step 1300/6092 Loss 5.386 Prec@(1,5) (5.8%, 16.5%)
08/01 07:01:36 PM | Train: [ 4/210] Step 1310/6092 Loss 5.380 Prec@(1,5) (5.9%, 16.6%)
08/01 07:01:54 PM | Train: [ 4/210] Step 1320/6092 Loss 5.374 Prec@(1,5) (6.0%, 16.7%)
08/01 07:02:12 PM | Train: [ 4/210] Step 1330/6092 Loss 5.376 Prec@(1,5) (6.0%, 16.7%)
08/01 07:02:29 PM | Train: [ 4/210] Step 1340/6092 Loss 5.382 Prec@(1,5) (5.9%, 16.7%)
08/01 07:02:46 PM | Train: [ 4/210] Step 1350/6092 Loss 5.386 Prec@(1,5) (5.9%, 16.6%)
08/01 07:03:03 PM | Train: [ 4/210] Step 1360/6092 Loss 5.387 Prec@(1,5) (5.8%, 16.6%)
08/01 07:03:21 PM | Train: [ 4/210] Step 1370/6092 Loss 5.390 Prec@(1,5) (5.8%, 16.6%)
08/01 07:03:38 PM | Train: [ 4/210] Step 1380/6092 Loss 5.388 Prec@(1,5) (5.8%, 16.6%)
08/01 07:03:55 PM | Train: [ 4/210] Step 1390/6092 Loss 5.390 Prec@(1,5) (5.8%, 16.6%)
08/01 07:04:13 PM | Train: [ 4/210] Step 1400/6092 Loss 5.390 Prec@(1,5) (5.8%, 16.6%)
08/01 07:04:30 PM | Train: [ 4/210] Step 1410/6092 Loss 5.389 Prec@(1,5) (5.8%, 16.6%)
08/01 07:04:47 PM | Train: [ 4/210] Step 1420/6092 Loss 5.388 Prec@(1,5) (5.8%, 16.6%)
08/01 07:05:05 PM | Train: [ 4/210] Step 1430/6092 Loss 5.390 Prec@(1,5) (5.8%, 16.5%)
08/01 07:05:22 PM | Train: [ 4/210] Step 1440/6092 Loss 5.387 Prec@(1,5) (5.9%, 16.6%)
08/01 07:05:39 PM | Train: [ 4/210] Step 1450/6092 Loss 5.389 Prec@(1,5) (5.9%, 16.5%)
08/01 07:05:57 PM | Train: [ 4/210] Step 1460/6092 Loss 5.385 Prec@(1,5) (5.8%, 16.5%)
08/01 07:06:14 PM | Train: [ 4/210] Step 1470/6092 Loss 5.385 Prec@(1,5) (5.8%, 16.5%)
08/01 07:06:33 PM | Train: [ 4/210] Step 1480/6092 Loss 5.385 Prec@(1,5) (5.8%, 16.6%)
08/01 07:06:50 PM | Train: [ 4/210] Step 1490/6092 Loss 5.383 Prec@(1,5) (5.8%, 16.7%)
08/01 07:07:07 PM | Train: [ 4/210] Step 1500/6092 Loss 5.385 Prec@(1,5) (5.8%, 16.7%)
08/01 07:07:25 PM | Train: [ 4/210] Step 1510/6092 Loss 5.386 Prec@(1,5) (5.8%, 16.5%)
08/01 07:07:42 PM | Train: [ 4/210] Step 1520/6092 Loss 5.387 Prec@(1,5) (5.8%, 16.4%)
08/01 07:07:59 PM | Train: [ 4/210] Step 1530/6092 Loss 5.390 Prec@(1,5) (5.7%, 16.4%)
08/01 07:08:17 PM | Train: [ 4/210] Step 1540/6092 Loss 5.390 Prec@(1,5) (5.7%, 16.3%)
08/01 07:08:36 PM | Train: [ 4/210] Step 1550/6092 Loss 5.391 Prec@(1,5) (5.7%, 16.3%)
08/01 07:08:53 PM | Train: [ 4/210] Step 1560/6092 Loss 5.386 Prec@(1,5) (5.7%, 16.5%)
08/01 07:09:10 PM | Train: [ 4/210] Step 1570/6092 Loss 5.387 Prec@(1,5) (5.7%, 16.4%)
08/01 07:09:27 PM | Train: [ 4/210] Step 1580/6092 Loss 5.387 Prec@(1,5) (5.7%, 16.4%)
08/01 07:09:44 PM | Train: [ 4/210] Step 1590/6092 Loss 5.388 Prec@(1,5) (5.6%, 16.3%)
08/01 07:10:02 PM | Train: [ 4/210] Step 1600/6092 Loss 5.386 Prec@(1,5) (5.6%, 16.4%)
08/01 07:10:21 PM | Train: [ 4/210] Step 1610/6092 Loss 5.385 Prec@(1,5) (5.6%, 16.3%)
08/01 07:10:38 PM | Train: [ 4/210] Step 1620/6092 Loss 5.389 Prec@(1,5) (5.6%, 16.3%)
08/01 07:10:56 PM | Train: [ 4/210] Step 1630/6092 Loss 5.394 Prec@(1,5) (5.5%, 16.2%)
08/01 07:11:13 PM | Train: [ 4/210] Step 1640/6092 Loss 5.397 Prec@(1,5) (5.5%, 16.2%)
08/01 07:11:31 PM | Train: [ 4/210] Step 1650/6092 Loss 5.401 Prec@(1,5) (5.5%, 16.1%)
08/01 07:11:48 PM | Train: [ 4/210] Step 1660/6092 Loss 5.401 Prec@(1,5) (5.5%, 16.2%)
08/01 07:12:05 PM | Train: [ 4/210] Step 1670/6092 Loss 5.397 Prec@(1,5) (5.6%, 16.2%)
08/01 07:12:22 PM | Train: [ 4/210] Step 1680/6092 Loss 5.395 Prec@(1,5) (5.6%, 16.3%)
08/01 07:12:40 PM | Train: [ 4/210] Step 1690/6092 Loss 5.394 Prec@(1,5) (5.6%, 16.3%)
08/01 07:12:57 PM | Train: [ 4/210] Step 1700/6092 Loss 5.393 Prec@(1,5) (5.6%, 16.3%)
08/01 07:13:16 PM | Train: [ 4/210] Step 1710/6092 Loss 5.394 Prec@(1,5) (5.6%, 16.3%)
08/01 07:13:32 PM | Train: [ 4/210] Step 1720/6092 Loss 5.398 Prec@(1,5) (5.6%, 16.3%)
08/01 07:13:50 PM | Train: [ 4/210] Step 1730/6092 Loss 5.398 Prec@(1,5) (5.6%, 16.3%)
08/01 07:14:07 PM | Train: [ 4/210] Step 1740/6092 Loss 5.398 Prec@(1,5) (5.6%, 16.3%)
08/01 07:14:25 PM | Train: [ 4/210] Step 1750/6092 Loss 5.397 Prec@(1,5) (5.6%, 16.3%)
08/01 07:14:42 PM | Train: [ 4/210] Step 1760/6092 Loss 5.393 Prec@(1,5) (5.7%, 16.4%)
08/01 07:14:59 PM | Train: [ 4/210] Step 1770/6092 Loss 5.396 Prec@(1,5) (5.7%, 16.3%)
08/01 07:15:18 PM | Train: [ 4/210] Step 1780/6092 Loss 5.396 Prec@(1,5) (5.7%, 16.3%)
08/01 07:15:35 PM | Train: [ 4/210] Step 1790/6092 Loss 5.401 Prec@(1,5) (5.7%, 16.3%)
08/01 07:15:52 PM | Train: [ 4/210] Step 1800/6092 Loss 5.399 Prec@(1,5) (5.7%, 16.3%)
08/01 07:16:10 PM | Train: [ 4/210] Step 1810/6092 Loss 5.401 Prec@(1,5) (5.6%, 16.2%)
08/01 07:16:27 PM | Train: [ 4/210] Step 1820/6092 Loss 5.400 Prec@(1,5) (5.6%, 16.2%)
08/01 07:16:44 PM | Train: [ 4/210] Step 1830/6092 Loss 5.401 Prec@(1,5) (5.7%, 16.2%)
08/01 07:17:01 PM | Train: [ 4/210] Step 1840/6092 Loss 5.400 Prec@(1,5) (5.6%, 16.2%)
08/01 07:17:19 PM | Train: [ 4/210] Step 1850/6092 Loss 5.402 Prec@(1,5) (5.6%, 16.2%)
08/01 07:17:37 PM | Train: [ 4/210] Step 1860/6092 Loss 5.401 Prec@(1,5) (5.6%, 16.2%)
08/01 07:17:54 PM | Train: [ 4/210] Step 1870/6092 Loss 5.401 Prec@(1,5) (5.6%, 16.3%)
08/01 07:18:12 PM | Train: [ 4/210] Step 1880/6092 Loss 5.403 Prec@(1,5) (5.6%, 16.2%)
08/01 07:18:29 PM | Train: [ 4/210] Step 1890/6092 Loss 5.405 Prec@(1,5) (5.6%, 16.2%)
08/01 07:18:46 PM | Train: [ 4/210] Step 1900/6092 Loss 5.404 Prec@(1,5) (5.6%, 16.3%)
08/01 07:19:04 PM | Train: [ 4/210] Step 1910/6092 Loss 5.404 Prec@(1,5) (5.6%, 16.3%)
08/01 07:19:21 PM | Train: [ 4/210] Step 1920/6092 Loss 5.397 Prec@(1,5) (5.7%, 16.4%)
08/01 07:19:40 PM | Train: [ 4/210] Step 1930/6092 Loss 5.401 Prec@(1,5) (5.6%, 16.3%)
08/01 07:19:57 PM | Train: [ 4/210] Step 1940/6092 Loss 5.402 Prec@(1,5) (5.7%, 16.3%)
08/01 07:20:19 PM | Train: [ 4/210] Step 1950/6092 Loss 5.403 Prec@(1,5) (5.6%, 16.3%)
08/01 07:20:38 PM | Train: [ 4/210] Step 1960/6092 Loss 5.403 Prec@(1,5) (5.6%, 16.3%)
08/01 07:20:57 PM | Train: [ 4/210] Step 1970/6092 Loss 5.400 Prec@(1,5) (5.7%, 16.3%)
08/01 07:21:16 PM | Train: [ 4/210] Step 1980/6092 Loss 5.401 Prec@(1,5) (5.7%, 16.3%)
08/01 07:21:34 PM | Train: [ 4/210] Step 1990/6092 Loss 5.403 Prec@(1,5) (5.6%, 16.2%)
08/01 07:21:52 PM | Train: [ 4/210] Step 2000/6092 Loss 5.402 Prec@(1,5) (5.6%, 16.2%)
08/01 07:22:11 PM | Train: [ 4/210] Step 2010/6092 Loss 5.398 Prec@(1,5) (5.6%, 16.3%)
08/01 07:22:28 PM | Train: [ 4/210] Step 2020/6092 Loss 5.398 Prec@(1,5) (5.6%, 16.2%)
08/01 07:22:45 PM | Train: [ 4/210] Step 2030/6092 Loss 5.397 Prec@(1,5) (5.7%, 16.3%)
08/01 07:23:03 PM | Train: [ 4/210] Step 2040/6092 Loss 5.398 Prec@(1,5) (5.7%, 16.3%)
08/01 07:23:20 PM | Train: [ 4/210] Step 2050/6092 Loss 5.397 Prec@(1,5) (5.8%, 16.3%)
08/01 07:23:37 PM | Train: [ 4/210] Step 2060/6092 Loss 5.395 Prec@(1,5) (5.8%, 16.4%)
08/01 07:23:55 PM | Train: [ 4/210] Step 2070/6092 Loss 5.395 Prec@(1,5) (5.8%, 16.4%)
08/01 07:24:12 PM | Train: [ 4/210] Step 2080/6092 Loss 5.394 Prec@(1,5) (5.8%, 16.3%)
08/01 07:24:29 PM | Train: [ 4/210] Step 2090/6092 Loss 5.395 Prec@(1,5) (5.8%, 16.3%)
08/01 07:24:46 PM | Train: [ 4/210] Step 2100/6092 Loss 5.394 Prec@(1,5) (5.8%, 16.3%)
08/01 07:25:05 PM | Train: [ 4/210] Step 2110/6092 Loss 5.394 Prec@(1,5) (5.8%, 16.3%)
08/01 07:25:23 PM | Train: [ 4/210] Step 2120/6092 Loss 5.391 Prec@(1,5) (5.8%, 16.4%)
08/01 07:25:40 PM | Train: [ 4/210] Step 2130/6092 Loss 5.393 Prec@(1,5) (5.8%, 16.3%)
08/01 07:25:57 PM | Train: [ 4/210] Step 2140/6092 Loss 5.393 Prec@(1,5) (5.8%, 16.3%)
08/01 07:26:15 PM | Train: [ 4/210] Step 2150/6092 Loss 5.393 Prec@(1,5) (5.8%, 16.3%)
08/01 07:26:32 PM | Train: [ 4/210] Step 2160/6092 Loss 5.396 Prec@(1,5) (5.8%, 16.3%)
08/01 07:26:50 PM | Train: [ 4/210] Step 2170/6092 Loss 5.395 Prec@(1,5) (5.8%, 16.3%)
08/01 07:27:06 PM | Train: [ 4/210] Step 2180/6092 Loss 5.395 Prec@(1,5) (5.8%, 16.3%)
08/01 07:27:24 PM | Train: [ 4/210] Step 2190/6092 Loss 5.395 Prec@(1,5) (5.8%, 16.3%)
08/01 07:27:41 PM | Train: [ 4/210] Step 2200/6092 Loss 5.393 Prec@(1,5) (5.8%, 16.3%)
08/01 07:27:58 PM | Train: [ 4/210] Step 2210/6092 Loss 5.391 Prec@(1,5) (5.8%, 16.4%)
08/01 07:28:16 PM | Train: [ 4/210] Step 2220/6092 Loss 5.394 Prec@(1,5) (5.8%, 16.3%)
08/01 07:28:33 PM | Train: [ 4/210] Step 2230/6092 Loss 5.393 Prec@(1,5) (5.8%, 16.3%)
08/01 07:28:50 PM | Train: [ 4/210] Step 2240/6092 Loss 5.394 Prec@(1,5) (5.8%, 16.3%)
08/01 07:29:07 PM | Train: [ 4/210] Step 2250/6092 Loss 5.395 Prec@(1,5) (5.8%, 16.3%)
08/01 07:29:24 PM | Train: [ 4/210] Step 2260/6092 Loss 5.395 Prec@(1,5) (5.8%, 16.3%)
08/01 07:29:41 PM | Train: [ 4/210] Step 2270/6092 Loss 5.392 Prec@(1,5) (5.9%, 16.4%)
08/01 07:29:58 PM | Train: [ 4/210] Step 2280/6092 Loss 5.392 Prec@(1,5) (5.9%, 16.3%)
08/01 07:30:18 PM | Train: [ 4/210] Step 2290/6092 Loss 5.392 Prec@(1,5) (5.9%, 16.3%)
08/01 07:30:35 PM | Train: [ 4/210] Step 2300/6092 Loss 5.388 Prec@(1,5) (5.9%, 16.5%)
08/01 07:30:52 PM | Train: [ 4/210] Step 2310/6092 Loss 5.385 Prec@(1,5) (5.9%, 16.5%)
08/01 07:31:09 PM | Train: [ 4/210] Step 2320/6092 Loss 5.383 Prec@(1,5) (6.0%, 16.5%)
08/01 07:31:27 PM | Train: [ 4/210] Step 2330/6092 Loss 5.386 Prec@(1,5) (6.0%, 16.5%)
08/01 07:31:44 PM | Train: [ 4/210] Step 2340/6092 Loss 5.387 Prec@(1,5) (6.0%, 16.5%)
08/01 07:32:02 PM | Train: [ 4/210] Step 2350/6092 Loss 5.387 Prec@(1,5) (6.0%, 16.5%)
08/01 07:32:19 PM | Train: [ 4/210] Step 2360/6092 Loss 5.387 Prec@(1,5) (6.0%, 16.5%)
08/01 07:32:36 PM | Train: [ 4/210] Step 2370/6092 Loss 5.386 Prec@(1,5) (6.0%, 16.6%)
08/01 07:32:53 PM | Train: [ 4/210] Step 2380/6092 Loss 5.384 Prec@(1,5) (6.0%, 16.5%)
08/01 07:33:11 PM | Train: [ 4/210] Step 2390/6092 Loss 5.385 Prec@(1,5) (6.0%, 16.5%)
08/01 07:33:28 PM | Train: [ 4/210] Step 2400/6092 Loss 5.386 Prec@(1,5) (6.0%, 16.5%)
08/01 07:33:44 PM | Train: [ 4/210] Step 2410/6092 Loss 5.384 Prec@(1,5) (6.0%, 16.5%)
08/01 07:34:02 PM | Train: [ 4/210] Step 2420/6092 Loss 5.385 Prec@(1,5) (6.0%, 16.5%)
08/01 07:34:20 PM | Train: [ 4/210] Step 2430/6092 Loss 5.387 Prec@(1,5) (6.0%, 16.5%)
08/01 07:34:37 PM | Train: [ 4/210] Step 2440/6092 Loss 5.385 Prec@(1,5) (6.0%, 16.5%)
08/01 07:34:54 PM | Train: [ 4/210] Step 2450/6092 Loss 5.382 Prec@(1,5) (6.0%, 16.5%)
08/01 07:35:13 PM | Train: [ 4/210] Step 2460/6092 Loss 5.380 Prec@(1,5) (6.1%, 16.5%)
08/01 07:35:31 PM | Train: [ 4/210] Step 2470/6092 Loss 5.380 Prec@(1,5) (6.1%, 16.5%)
08/01 07:35:49 PM | Train: [ 4/210] Step 2480/6092 Loss 5.378 Prec@(1,5) (6.1%, 16.6%)
08/01 07:36:07 PM | Train: [ 4/210] Step 2490/6092 Loss 5.379 Prec@(1,5) (6.1%, 16.6%)
08/01 07:36:24 PM | Train: [ 4/210] Step 2500/6092 Loss 5.379 Prec@(1,5) (6.1%, 16.6%)
08/01 07:36:41 PM | Train: [ 4/210] Step 2510/6092 Loss 5.379 Prec@(1,5) (6.1%, 16.7%)
08/01 07:36:58 PM | Train: [ 4/210] Step 2520/6092 Loss 5.380 Prec@(1,5) (6.1%, 16.7%)
08/01 07:37:16 PM | Train: [ 4/210] Step 2530/6092 Loss 5.381 Prec@(1,5) (6.1%, 16.7%)
08/01 07:37:33 PM | Train: [ 4/210] Step 2540/6092 Loss 5.382 Prec@(1,5) (6.1%, 16.7%)
08/01 07:37:51 PM | Train: [ 4/210] Step 2550/6092 Loss 5.383 Prec@(1,5) (6.1%, 16.7%)
08/01 07:38:08 PM | Train: [ 4/210] Step 2560/6092 Loss 5.385 Prec@(1,5) (6.1%, 16.7%)
08/01 07:38:25 PM | Train: [ 4/210] Step 2570/6092 Loss 5.381 Prec@(1,5) (6.1%, 16.8%)
08/01 07:38:44 PM | Train: [ 4/210] Step 2580/6092 Loss 5.377 Prec@(1,5) (6.1%, 16.9%)
08/01 07:39:01 PM | Train: [ 4/210] Step 2590/6092 Loss 5.376 Prec@(1,5) (6.1%, 16.9%)
08/01 07:39:18 PM | Train: [ 4/210] Step 2600/6092 Loss 5.376 Prec@(1,5) (6.1%, 16.9%)
08/01 07:39:35 PM | Train: [ 4/210] Step 2610/6092 Loss 5.378 Prec@(1,5) (6.1%, 16.9%)
08/01 07:39:52 PM | Train: [ 4/210] Step 2620/6092 Loss 5.377 Prec@(1,5) (6.1%, 16.9%)
08/01 07:40:13 PM | Train: [ 4/210] Step 2630/6092 Loss 5.377 Prec@(1,5) (6.1%, 16.8%)
08/01 07:40:30 PM | Train: [ 4/210] Step 2640/6092 Loss 5.378 Prec@(1,5) (6.1%, 16.8%)
08/01 07:40:48 PM | Train: [ 4/210] Step 2650/6092 Loss 5.380 Prec@(1,5) (6.1%, 16.8%)
08/01 07:41:04 PM | Train: [ 4/210] Step 2660/6092 Loss 5.381 Prec@(1,5) (6.1%, 16.8%)
08/01 07:41:22 PM | Train: [ 4/210] Step 2670/6092 Loss 5.382 Prec@(1,5) (6.1%, 16.8%)
08/01 07:41:40 PM | Train: [ 4/210] Step 2680/6092 Loss 5.381 Prec@(1,5) (6.1%, 16.8%)
08/01 07:41:57 PM | Train: [ 4/210] Step 2690/6092 Loss 5.381 Prec@(1,5) (6.0%, 16.8%)
08/01 07:42:14 PM | Train: [ 4/210] Step 2700/6092 Loss 5.383 Prec@(1,5) (6.0%, 16.7%)
08/01 07:42:31 PM | Train: [ 4/210] Step 2710/6092 Loss 5.383 Prec@(1,5) (6.0%, 16.7%)
08/01 07:42:48 PM | Train: [ 4/210] Step 2720/6092 Loss 5.383 Prec@(1,5) (6.0%, 16.7%)
08/01 07:43:05 PM | Train: [ 4/210] Step 2730/6092 Loss 5.385 Prec@(1,5) (6.0%, 16.7%)
08/01 07:43:23 PM | Train: [ 4/210] Step 2740/6092 Loss 5.385 Prec@(1,5) (6.0%, 16.7%)
08/01 07:43:41 PM | Train: [ 4/210] Step 2750/6092 Loss 5.385 Prec@(1,5) (6.0%, 16.6%)
08/01 07:43:58 PM | Train: [ 4/210] Step 2760/6092 Loss 5.382 Prec@(1,5) (6.0%, 16.7%)
08/01 07:44:15 PM | Train: [ 4/210] Step 2770/6092 Loss 5.382 Prec@(1,5) (6.0%, 16.7%)
08/01 07:44:33 PM | Train: [ 4/210] Step 2780/6092 Loss 5.381 Prec@(1,5) (6.0%, 16.7%)
08/01 07:44:50 PM | Train: [ 4/210] Step 2790/6092 Loss 5.382 Prec@(1,5) (6.0%, 16.7%)
08/01 07:45:10 PM | Train: [ 4/210] Step 2800/6092 Loss 5.380 Prec@(1,5) (6.0%, 16.7%)
08/01 07:45:27 PM | Train: [ 4/210] Step 2810/6092 Loss 5.381 Prec@(1,5) (6.0%, 16.7%)
08/01 07:45:45 PM | Train: [ 4/210] Step 2820/6092 Loss 5.380 Prec@(1,5) (6.0%, 16.7%)
08/01 07:46:03 PM | Train: [ 4/210] Step 2830/6092 Loss 5.381 Prec@(1,5) (6.0%, 16.7%)
08/01 07:46:20 PM | Train: [ 4/210] Step 2840/6092 Loss 5.378 Prec@(1,5) (6.0%, 16.7%)
08/01 07:46:37 PM | Train: [ 4/210] Step 2850/6092 Loss 5.378 Prec@(1,5) (6.0%, 16.7%)
08/01 07:46:55 PM | Train: [ 4/210] Step 2860/6092 Loss 5.379 Prec@(1,5) (6.0%, 16.7%)
08/01 07:47:12 PM | Train: [ 4/210] Step 2870/6092 Loss 5.377 Prec@(1,5) (6.1%, 16.8%)
08/01 07:47:29 PM | Train: [ 4/210] Step 2880/6092 Loss 5.378 Prec@(1,5) (6.1%, 16.8%)
08/01 07:47:46 PM | Train: [ 4/210] Step 2890/6092 Loss 5.378 Prec@(1,5) (6.1%, 16.8%)
08/01 07:48:02 PM | Train: [ 4/210] Step 2900/6092 Loss 5.379 Prec@(1,5) (6.0%, 16.8%)
08/01 07:48:20 PM | Train: [ 4/210] Step 2910/6092 Loss 5.380 Prec@(1,5) (6.0%, 16.8%)
08/01 07:48:37 PM | Train: [ 4/210] Step 2920/6092 Loss 5.381 Prec@(1,5) (6.0%, 16.7%)
08/01 07:48:54 PM | Train: [ 4/210] Step 2930/6092 Loss 5.381 Prec@(1,5) (6.0%, 16.7%)
08/01 07:49:12 PM | Train: [ 4/210] Step 2940/6092 Loss 5.384 Prec@(1,5) (6.0%, 16.7%)
08/01 07:49:29 PM | Train: [ 4/210] Step 2950/6092 Loss 5.386 Prec@(1,5) (6.0%, 16.7%)
08/01 07:49:46 PM | Train: [ 4/210] Step 2960/6092 Loss 5.385 Prec@(1,5) (6.0%, 16.6%)
08/01 07:50:04 PM | Train: [ 4/210] Step 2970/6092 Loss 5.386 Prec@(1,5) (6.0%, 16.6%)
08/01 07:50:24 PM | Train: [ 4/210] Step 2980/6092 Loss 5.386 Prec@(1,5) (6.0%, 16.6%)
08/01 07:50:42 PM | Train: [ 4/210] Step 2990/6092 Loss 5.386 Prec@(1,5) (6.0%, 16.6%)
08/01 07:50:59 PM | Train: [ 4/210] Step 3000/6092 Loss 5.386 Prec@(1,5) (5.9%, 16.6%)
08/01 07:51:17 PM | Train: [ 4/210] Step 3010/6092 Loss 5.389 Prec@(1,5) (5.9%, 16.6%)
08/01 07:51:34 PM | Train: [ 4/210] Step 3020/6092 Loss 5.388 Prec@(1,5) (5.9%, 16.6%)
08/01 07:51:51 PM | Train: [ 4/210] Step 3030/6092 Loss 5.387 Prec@(1,5) (5.9%, 16.5%)
08/01 07:52:08 PM | Train: [ 4/210] Step 3040/6092 Loss 5.387 Prec@(1,5) (6.0%, 16.5%)
08/01 07:52:26 PM | Train: [ 4/210] Step 3050/6092 Loss 5.389 Prec@(1,5) (5.9%, 16.5%)
08/01 07:52:43 PM | Train: [ 4/210] Step 3060/6092 Loss 5.388 Prec@(1,5) (5.9%, 16.5%)
08/01 07:53:00 PM | Train: [ 4/210] Step 3070/6092 Loss 5.390 Prec@(1,5) (5.9%, 16.5%)
08/01 07:53:17 PM | Train: [ 4/210] Step 3080/6092 Loss 5.389 Prec@(1,5) (5.9%, 16.6%)
08/01 07:53:34 PM | Train: [ 4/210] Step 3090/6092 Loss 5.391 Prec@(1,5) (5.9%, 16.5%)
08/01 07:53:51 PM | Train: [ 4/210] Step 3100/6092 Loss 5.391 Prec@(1,5) (5.9%, 16.5%)
08/01 07:54:10 PM | Train: [ 4/210] Step 3110/6092 Loss 5.392 Prec@(1,5) (5.9%, 16.5%)
08/01 07:54:27 PM | Train: [ 4/210] Step 3120/6092 Loss 5.393 Prec@(1,5) (5.9%, 16.5%)
08/01 07:54:44 PM | Train: [ 4/210] Step 3130/6092 Loss 5.393 Prec@(1,5) (5.9%, 16.5%)
08/01 07:55:02 PM | Train: [ 4/210] Step 3140/6092 Loss 5.392 Prec@(1,5) (5.9%, 16.5%)
08/01 07:55:22 PM | Train: [ 4/210] Step 3150/6092 Loss 5.392 Prec@(1,5) (5.9%, 16.5%)
08/01 07:55:38 PM | Train: [ 4/210] Step 3160/6092 Loss 5.393 Prec@(1,5) (5.9%, 16.5%)
08/01 07:55:56 PM | Train: [ 4/210] Step 3170/6092 Loss 5.392 Prec@(1,5) (5.8%, 16.4%)
08/01 07:56:13 PM | Train: [ 4/210] Step 3180/6092 Loss 5.390 Prec@(1,5) (5.9%, 16.5%)
08/01 07:56:30 PM | Train: [ 4/210] Step 3190/6092 Loss 5.390 Prec@(1,5) (5.9%, 16.5%)
08/01 07:56:48 PM | Train: [ 4/210] Step 3200/6092 Loss 5.389 Prec@(1,5) (5.9%, 16.5%)
08/01 07:57:07 PM | Train: [ 4/210] Step 3210/6092 Loss 5.389 Prec@(1,5) (5.9%, 16.5%)
08/01 07:57:24 PM | Train: [ 4/210] Step 3220/6092 Loss 5.389 Prec@(1,5) (5.9%, 16.5%)
08/01 07:57:41 PM | Train: [ 4/210] Step 3230/6092 Loss 5.390 Prec@(1,5) (5.9%, 16.5%)
08/01 07:57:59 PM | Train: [ 4/210] Step 3240/6092 Loss 5.388 Prec@(1,5) (5.9%, 16.6%)
08/01 07:58:16 PM | Train: [ 4/210] Step 3250/6092 Loss 5.388 Prec@(1,5) (5.9%, 16.6%)
08/01 07:58:33 PM | Train: [ 4/210] Step 3260/6092 Loss 5.385 Prec@(1,5) (5.9%, 16.6%)
08/01 07:58:51 PM | Train: [ 4/210] Step 3270/6092 Loss 5.384 Prec@(1,5) (5.9%, 16.7%)
08/01 07:59:08 PM | Train: [ 4/210] Step 3280/6092 Loss 5.384 Prec@(1,5) (5.9%, 16.7%)
08/01 07:59:27 PM | Train: [ 4/210] Step 3290/6092 Loss 5.385 Prec@(1,5) (6.0%, 16.7%)
08/01 07:59:44 PM | Train: [ 4/210] Step 3300/6092 Loss 5.385 Prec@(1,5) (6.0%, 16.7%)
08/01 08:00:01 PM | Train: [ 4/210] Step 3310/6092 Loss 5.385 Prec@(1,5) (6.0%, 16.7%)
08/01 08:00:22 PM | Train: [ 4/210] Step 3320/6092 Loss 5.385 Prec@(1,5) (5.9%, 16.7%)
08/01 08:00:40 PM | Train: [ 4/210] Step 3330/6092 Loss 5.387 Prec@(1,5) (5.9%, 16.7%)
08/01 08:00:57 PM | Train: [ 4/210] Step 3340/6092 Loss 5.386 Prec@(1,5) (5.9%, 16.7%)
08/01 08:01:14 PM | Train: [ 4/210] Step 3350/6092 Loss 5.385 Prec@(1,5) (5.9%, 16.7%)
08/01 08:01:31 PM | Train: [ 4/210] Step 3360/6092 Loss 5.385 Prec@(1,5) (5.9%, 16.8%)
08/01 08:01:49 PM | Train: [ 4/210] Step 3370/6092 Loss 5.385 Prec@(1,5) (5.9%, 16.7%)
08/01 08:02:05 PM | Train: [ 4/210] Step 3380/6092 Loss 5.385 Prec@(1,5) (5.9%, 16.7%)
08/01 08:02:23 PM | Train: [ 4/210] Step 3390/6092 Loss 5.384 Prec@(1,5) (5.9%, 16.8%)
08/01 08:02:41 PM | Train: [ 4/210] Step 3400/6092 Loss 5.384 Prec@(1,5) (5.9%, 16.8%)
08/01 08:02:59 PM | Train: [ 4/210] Step 3410/6092 Loss 5.384 Prec@(1,5) (5.9%, 16.7%)
08/01 08:03:16 PM | Train: [ 4/210] Step 3420/6092 Loss 5.385 Prec@(1,5) (5.9%, 16.7%)
08/01 08:03:34 PM | Train: [ 4/210] Step 3430/6092 Loss 5.386 Prec@(1,5) (5.8%, 16.7%)
08/01 08:03:52 PM | Train: [ 4/210] Step 3440/6092 Loss 5.386 Prec@(1,5) (5.8%, 16.7%)
08/01 08:04:09 PM | Train: [ 4/210] Step 3450/6092 Loss 5.384 Prec@(1,5) (5.8%, 16.7%)
08/01 08:04:26 PM | Train: [ 4/210] Step 3460/6092 Loss 5.384 Prec@(1,5) (5.8%, 16.7%)
08/01 08:04:44 PM | Train: [ 4/210] Step 3470/6092 Loss 5.386 Prec@(1,5) (5.8%, 16.6%)
08/01 08:05:01 PM | Train: [ 4/210] Step 3480/6092 Loss 5.385 Prec@(1,5) (5.8%, 16.6%)
08/01 08:05:20 PM | Train: [ 4/210] Step 3490/6092 Loss 5.385 Prec@(1,5) (5.8%, 16.7%)
08/01 08:05:38 PM | Train: [ 4/210] Step 3500/6092 Loss 5.385 Prec@(1,5) (5.8%, 16.6%)
08/01 08:05:55 PM | Train: [ 4/210] Step 3510/6092 Loss 5.386 Prec@(1,5) (5.8%, 16.6%)
08/01 08:06:13 PM | Train: [ 4/210] Step 3520/6092 Loss 5.386 Prec@(1,5) (5.8%, 16.7%)
08/01 08:06:30 PM | Train: [ 4/210] Step 3530/6092 Loss 5.385 Prec@(1,5) (5.9%, 16.7%)
08/01 08:06:48 PM | Train: [ 4/210] Step 3540/6092 Loss 5.386 Prec@(1,5) (5.9%, 16.6%)
08/01 08:07:05 PM | Train: [ 4/210] Step 3550/6092 Loss 5.386 Prec@(1,5) (5.9%, 16.7%)
08/01 08:07:23 PM | Train: [ 4/210] Step 3560/6092 Loss 5.386 Prec@(1,5) (5.9%, 16.7%)
08/01 08:07:40 PM | Train: [ 4/210] Step 3570/6092 Loss 5.387 Prec@(1,5) (5.9%, 16.7%)
08/01 08:07:58 PM | Train: [ 4/210] Step 3580/6092 Loss 5.388 Prec@(1,5) (5.9%, 16.7%)
08/01 08:08:17 PM | Train: [ 4/210] Step 3590/6092 Loss 5.388 Prec@(1,5) (5.9%, 16.7%)
08/01 08:08:35 PM | Train: [ 4/210] Step 3600/6092 Loss 5.388 Prec@(1,5) (5.9%, 16.6%)
08/01 08:08:52 PM | Train: [ 4/210] Step 3610/6092 Loss 5.388 Prec@(1,5) (5.9%, 16.6%)
08/01 08:09:10 PM | Train: [ 4/210] Step 3620/6092 Loss 5.388 Prec@(1,5) (5.9%, 16.6%)
08/01 08:09:28 PM | Train: [ 4/210] Step 3630/6092 Loss 5.388 Prec@(1,5) (5.9%, 16.6%)
08/01 08:09:46 PM | Train: [ 4/210] Step 3640/6092 Loss 5.388 Prec@(1,5) (5.9%, 16.6%)
08/01 08:10:03 PM | Train: [ 4/210] Step 3650/6092 Loss 5.386 Prec@(1,5) (5.8%, 16.7%)
08/01 08:10:22 PM | Train: [ 4/210] Step 3660/6092 Loss 5.384 Prec@(1,5) (5.9%, 16.7%)
08/01 08:10:39 PM | Train: [ 4/210] Step 3670/6092 Loss 5.384 Prec@(1,5) (5.9%, 16.7%)
08/01 08:10:56 PM | Train: [ 4/210] Step 3680/6092 Loss 5.386 Prec@(1,5) (5.9%, 16.7%)
08/01 08:11:13 PM | Train: [ 4/210] Step 3690/6092 Loss 5.387 Prec@(1,5) (5.9%, 16.7%)
08/01 08:11:30 PM | Train: [ 4/210] Step 3700/6092 Loss 5.388 Prec@(1,5) (5.8%, 16.7%)
08/01 08:11:46 PM | Train: [ 4/210] Step 3710/6092 Loss 5.386 Prec@(1,5) (5.9%, 16.7%)
08/01 08:12:04 PM | Train: [ 4/210] Step 3720/6092 Loss 5.386 Prec@(1,5) (5.8%, 16.7%)
08/01 08:12:23 PM | Train: [ 4/210] Step 3730/6092 Loss 5.387 Prec@(1,5) (5.8%, 16.7%)
08/01 08:12:40 PM | Train: [ 4/210] Step 3740/6092 Loss 5.389 Prec@(1,5) (5.8%, 16.7%)
08/01 08:12:57 PM | Train: [ 4/210] Step 3750/6092 Loss 5.389 Prec@(1,5) (5.8%, 16.7%)
08/01 08:13:15 PM | Train: [ 4/210] Step 3760/6092 Loss 5.390 Prec@(1,5) (5.8%, 16.7%)
08/01 08:13:32 PM | Train: [ 4/210] Step 3770/6092 Loss 5.390 Prec@(1,5) (5.8%, 16.7%)
08/01 08:13:49 PM | Train: [ 4/210] Step 3780/6092 Loss 5.389 Prec@(1,5) (5.8%, 16.7%)
08/01 08:14:07 PM | Train: [ 4/210] Step 3790/6092 Loss 5.390 Prec@(1,5) (5.8%, 16.7%)
08/01 08:14:24 PM | Train: [ 4/210] Step 3800/6092 Loss 5.390 Prec@(1,5) (5.8%, 16.7%)
08/01 08:14:42 PM | Train: [ 4/210] Step 3810/6092 Loss 5.391 Prec@(1,5) (5.8%, 16.7%)
08/01 08:14:59 PM | Train: [ 4/210] Step 3820/6092 Loss 5.390 Prec@(1,5) (5.8%, 16.7%)
08/01 08:15:17 PM | Train: [ 4/210] Step 3830/6092 Loss 5.391 Prec@(1,5) (5.8%, 16.7%)
08/01 08:15:34 PM | Train: [ 4/210] Step 3840/6092 Loss 5.391 Prec@(1,5) (5.8%, 16.7%)
08/01 08:15:52 PM | Train: [ 4/210] Step 3850/6092 Loss 5.391 Prec@(1,5) (5.8%, 16.7%)
08/01 08:16:09 PM | Train: [ 4/210] Step 3860/6092 Loss 5.391 Prec@(1,5) (5.8%, 16.7%)
08/01 08:16:26 PM | Train: [ 4/210] Step 3870/6092 Loss 5.391 Prec@(1,5) (5.8%, 16.7%)
08/01 08:16:43 PM | Train: [ 4/210] Step 3880/6092 Loss 5.392 Prec@(1,5) (5.8%, 16.6%)
08/01 08:17:00 PM | Train: [ 4/210] Step 3890/6092 Loss 5.391 Prec@(1,5) (5.8%, 16.6%)
08/01 08:17:17 PM | Train: [ 4/210] Step 3900/6092 Loss 5.391 Prec@(1,5) (5.8%, 16.6%)
08/01 08:17:35 PM | Train: [ 4/210] Step 3910/6092 Loss 5.390 Prec@(1,5) (5.8%, 16.6%)
08/01 08:17:52 PM | Train: [ 4/210] Step 3920/6092 Loss 5.390 Prec@(1,5) (5.8%, 16.6%)
08/01 08:18:10 PM | Train: [ 4/210] Step 3930/6092 Loss 5.389 Prec@(1,5) (5.8%, 16.6%)
08/01 08:18:28 PM | Train: [ 4/210] Step 3940/6092 Loss 5.388 Prec@(1,5) (5.8%, 16.6%)
08/01 08:18:46 PM | Train: [ 4/210] Step 3950/6092 Loss 5.390 Prec@(1,5) (5.8%, 16.6%)
08/01 08:19:05 PM | Train: [ 4/210] Step 3960/6092 Loss 5.390 Prec@(1,5) (5.8%, 16.6%)
08/01 08:19:21 PM | Train: [ 4/210] Step 3970/6092 Loss 5.390 Prec@(1,5) (5.8%, 16.6%)
08/01 08:19:39 PM | Train: [ 4/210] Step 3980/6092 Loss 5.390 Prec@(1,5) (5.8%, 16.6%)
08/01 08:19:55 PM | Train: [ 4/210] Step 3990/6092 Loss 5.390 Prec@(1,5) (5.8%, 16.6%)
08/01 08:20:14 PM | Train: [ 4/210] Step 4000/6092 Loss 5.389 Prec@(1,5) (5.8%, 16.6%)
08/01 08:20:31 PM | Train: [ 4/210] Step 4010/6092 Loss 5.389 Prec@(1,5) (5.8%, 16.6%)
08/01 08:20:48 PM | Train: [ 4/210] Step 4020/6092 Loss 5.389 Prec@(1,5) (5.8%, 16.7%)
08/01 08:21:05 PM | Train: [ 4/210] Step 4030/6092 Loss 5.389 Prec@(1,5) (5.8%, 16.7%)
08/01 08:21:22 PM | Train: [ 4/210] Step 4040/6092 Loss 5.389 Prec@(1,5) (5.8%, 16.7%)
08/01 08:21:39 PM | Train: [ 4/210] Step 4050/6092 Loss 5.389 Prec@(1,5) (5.9%, 16.7%)
08/01 08:21:56 PM | Train: [ 4/210] Step 4060/6092 Loss 5.389 Prec@(1,5) (5.8%, 16.7%)
08/01 08:22:14 PM | Train: [ 4/210] Step 4070/6092 Loss 5.388 Prec@(1,5) (5.8%, 16.7%)
08/01 08:22:31 PM | Train: [ 4/210] Step 4080/6092 Loss 5.389 Prec@(1,5) (5.8%, 16.6%)
08/01 08:22:48 PM | Train: [ 4/210] Step 4090/6092 Loss 5.389 Prec@(1,5) (5.8%, 16.6%)
08/01 08:23:05 PM | Train: [ 4/210] Step 4100/6092 Loss 5.389 Prec@(1,5) (5.8%, 16.7%)
08/01 08:23:23 PM | Train: [ 4/210] Step 4110/6092 Loss 5.390 Prec@(1,5) (5.8%, 16.7%)
08/01 08:23:40 PM | Train: [ 4/210] Step 4120/6092 Loss 5.388 Prec@(1,5) (5.9%, 16.7%)
08/01 08:23:57 PM | Train: [ 4/210] Step 4130/6092 Loss 5.389 Prec@(1,5) (5.8%, 16.6%)
08/01 08:24:14 PM | Train: [ 4/210] Step 4140/6092 Loss 5.389 Prec@(1,5) (5.8%, 16.6%)
08/01 08:24:31 PM | Train: [ 4/210] Step 4150/6092 Loss 5.389 Prec@(1,5) (5.8%, 16.6%)
08/01 08:24:48 PM | Train: [ 4/210] Step 4160/6092 Loss 5.390 Prec@(1,5) (5.8%, 16.6%)
08/01 08:25:07 PM | Train: [ 4/210] Step 4170/6092 Loss 5.389 Prec@(1,5) (5.8%, 16.6%)
08/01 08:25:24 PM | Train: [ 4/210] Step 4180/6092 Loss 5.389 Prec@(1,5) (5.8%, 16.6%)
08/01 08:25:41 PM | Train: [ 4/210] Step 4190/6092 Loss 5.389 Prec@(1,5) (5.8%, 16.6%)
08/01 08:25:57 PM | Train: [ 4/210] Step 4200/6092 Loss 5.387 Prec@(1,5) (5.8%, 16.6%)
08/01 08:26:14 PM | Train: [ 4/210] Step 4210/6092 Loss 5.388 Prec@(1,5) (5.8%, 16.6%)
08/01 08:26:31 PM | Train: [ 4/210] Step 4220/6092 Loss 5.387 Prec@(1,5) (5.8%, 16.6%)
08/01 08:26:49 PM | Train: [ 4/210] Step 4230/6092 Loss 5.386 Prec@(1,5) (5.8%, 16.6%)
08/01 08:27:08 PM | Train: [ 4/210] Step 4240/6092 Loss 5.387 Prec@(1,5) (5.9%, 16.6%)
08/01 08:27:25 PM | Train: [ 4/210] Step 4250/6092 Loss 5.386 Prec@(1,5) (5.9%, 16.6%)
08/01 08:27:42 PM | Train: [ 4/210] Step 4260/6092 Loss 5.385 Prec@(1,5) (5.9%, 16.7%)
08/01 08:28:00 PM | Train: [ 4/210] Step 4270/6092 Loss 5.385 Prec@(1,5) (5.9%, 16.7%)
08/01 08:28:17 PM | Train: [ 4/210] Step 4280/6092 Loss 5.386 Prec@(1,5) (5.9%, 16.7%)
08/01 08:28:34 PM | Train: [ 4/210] Step 4290/6092 Loss 5.386 Prec@(1,5) (5.9%, 16.7%)
08/01 08:28:51 PM | Train: [ 4/210] Step 4300/6092 Loss 5.386 Prec@(1,5) (5.9%, 16.7%)
08/01 08:29:08 PM | Train: [ 4/210] Step 4310/6092 Loss 5.386 Prec@(1,5) (5.9%, 16.7%)
08/01 08:29:27 PM | Train: [ 4/210] Step 4320/6092 Loss 5.385 Prec@(1,5) (5.9%, 16.7%)
08/01 08:29:46 PM | Train: [ 4/210] Step 4330/6092 Loss 5.386 Prec@(1,5) (5.9%, 16.6%)
08/01 08:30:03 PM | Train: [ 4/210] Step 4340/6092 Loss 5.384 Prec@(1,5) (5.9%, 16.7%)
08/01 08:30:21 PM | Train: [ 4/210] Step 4350/6092 Loss 5.384 Prec@(1,5) (5.8%, 16.7%)
08/01 08:30:38 PM | Train: [ 4/210] Step 4360/6092 Loss 5.383 Prec@(1,5) (5.8%, 16.7%)
08/01 08:30:56 PM | Train: [ 4/210] Step 4370/6092 Loss 5.383 Prec@(1,5) (5.8%, 16.7%)
08/01 08:31:13 PM | Train: [ 4/210] Step 4380/6092 Loss 5.382 Prec@(1,5) (5.8%, 16.7%)
08/01 08:31:30 PM | Train: [ 4/210] Step 4390/6092 Loss 5.382 Prec@(1,5) (5.8%, 16.7%)
08/01 08:31:48 PM | Train: [ 4/210] Step 4400/6092 Loss 5.381 Prec@(1,5) (5.8%, 16.7%)
08/01 08:32:05 PM | Train: [ 4/210] Step 4410/6092 Loss 5.381 Prec@(1,5) (5.8%, 16.7%)
08/01 08:32:22 PM | Train: [ 4/210] Step 4420/6092 Loss 5.381 Prec@(1,5) (5.8%, 16.7%)
08/01 08:32:40 PM | Train: [ 4/210] Step 4430/6092 Loss 5.382 Prec@(1,5) (5.8%, 16.7%)
08/01 08:32:57 PM | Train: [ 4/210] Step 4440/6092 Loss 5.381 Prec@(1,5) (5.8%, 16.7%)
08/01 08:33:14 PM | Train: [ 4/210] Step 4450/6092 Loss 5.381 Prec@(1,5) (5.8%, 16.7%)
08/01 08:33:32 PM | Train: [ 4/210] Step 4460/6092 Loss 5.381 Prec@(1,5) (5.8%, 16.7%)
08/01 08:33:51 PM | Train: [ 4/210] Step 4470/6092 Loss 5.381 Prec@(1,5) (5.8%, 16.7%)
08/01 08:34:08 PM | Train: [ 4/210] Step 4480/6092 Loss 5.381 Prec@(1,5) (5.8%, 16.7%)
08/01 08:34:25 PM | Train: [ 4/210] Step 4490/6092 Loss 5.381 Prec@(1,5) (5.8%, 16.7%)
08/01 08:34:42 PM | Train: [ 4/210] Step 4500/6092 Loss 5.380 Prec@(1,5) (5.8%, 16.7%)
08/01 08:34:59 PM | Train: [ 4/210] Step 4510/6092 Loss 5.378 Prec@(1,5) (5.9%, 16.8%)
08/01 08:35:18 PM | Train: [ 4/210] Step 4520/6092 Loss 5.378 Prec@(1,5) (5.9%, 16.8%)
08/01 08:35:35 PM | Train: [ 4/210] Step 4530/6092 Loss 5.378 Prec@(1,5) (5.9%, 16.8%)
08/01 08:35:52 PM | Train: [ 4/210] Step 4540/6092 Loss 5.378 Prec@(1,5) (5.8%, 16.8%)
08/01 08:36:10 PM | Train: [ 4/210] Step 4550/6092 Loss 5.378 Prec@(1,5) (5.9%, 16.8%)
08/01 08:36:27 PM | Train: [ 4/210] Step 4560/6092 Loss 5.378 Prec@(1,5) (5.9%, 16.8%)
08/01 08:36:44 PM | Train: [ 4/210] Step 4570/6092 Loss 5.377 Prec@(1,5) (5.9%, 16.8%)
08/01 08:37:01 PM | Train: [ 4/210] Step 4580/6092 Loss 5.378 Prec@(1,5) (5.9%, 16.8%)
08/01 08:37:19 PM | Train: [ 4/210] Step 4590/6092 Loss 5.378 Prec@(1,5) (5.9%, 16.8%)
08/01 08:37:36 PM | Train: [ 4/210] Step 4600/6092 Loss 5.378 Prec@(1,5) (5.9%, 16.8%)
08/01 08:37:53 PM | Train: [ 4/210] Step 4610/6092 Loss 5.378 Prec@(1,5) (5.9%, 16.8%)
08/01 08:38:10 PM | Train: [ 4/210] Step 4620/6092 Loss 5.377 Prec@(1,5) (5.9%, 16.8%)
08/01 08:38:27 PM | Train: [ 4/210] Step 4630/6092 Loss 5.377 Prec@(1,5) (5.9%, 16.8%)
08/01 08:38:45 PM | Train: [ 4/210] Step 4640/6092 Loss 5.376 Prec@(1,5) (5.9%, 16.8%)
08/01 08:39:03 PM | Train: [ 4/210] Step 4650/6092 Loss 5.375 Prec@(1,5) (5.9%, 16.8%)
08/01 08:39:20 PM | Train: [ 4/210] Step 4660/6092 Loss 5.376 Prec@(1,5) (5.9%, 16.8%)
08/01 08:39:38 PM | Train: [ 4/210] Step 4670/6092 Loss 5.376 Prec@(1,5) (5.9%, 16.8%)
08/01 08:39:57 PM | Train: [ 4/210] Step 4680/6092 Loss 5.377 Prec@(1,5) (5.9%, 16.8%)
08/01 08:40:15 PM | Train: [ 4/210] Step 4690/6092 Loss 5.377 Prec@(1,5) (5.9%, 16.8%)
08/01 08:40:33 PM | Train: [ 4/210] Step 4700/6092 Loss 5.376 Prec@(1,5) (5.9%, 16.8%)
08/01 08:40:51 PM | Train: [ 4/210] Step 4710/6092 Loss 5.376 Prec@(1,5) (5.9%, 16.8%)
08/01 08:41:07 PM | Train: [ 4/210] Step 4720/6092 Loss 5.376 Prec@(1,5) (5.9%, 16.8%)
08/01 08:41:25 PM | Train: [ 4/210] Step 4730/6092 Loss 5.377 Prec@(1,5) (5.9%, 16.8%)
08/01 08:41:43 PM | Train: [ 4/210] Step 4740/6092 Loss 5.378 Prec@(1,5) (5.9%, 16.8%)
08/01 08:42:01 PM | Train: [ 4/210] Step 4750/6092 Loss 5.377 Prec@(1,5) (5.9%, 16.8%)
08/01 08:42:19 PM | Train: [ 4/210] Step 4760/6092 Loss 5.378 Prec@(1,5) (5.9%, 16.8%)
08/01 08:42:37 PM | Train: [ 4/210] Step 4770/6092 Loss 5.377 Prec@(1,5) (5.9%, 16.8%)
08/01 08:42:54 PM | Train: [ 4/210] Step 4780/6092 Loss 5.377 Prec@(1,5) (5.9%, 16.8%)
08/01 08:43:12 PM | Train: [ 4/210] Step 4790/6092 Loss 5.376 Prec@(1,5) (5.9%, 16.8%)
08/01 08:43:28 PM | Train: [ 4/210] Step 4800/6092 Loss 5.376 Prec@(1,5) (5.9%, 16.8%)
08/01 08:43:46 PM | Train: [ 4/210] Step 4810/6092 Loss 5.376 Prec@(1,5) (5.9%, 16.8%)
08/01 08:44:02 PM | Train: [ 4/210] Step 4820/6092 Loss 5.376 Prec@(1,5) (5.9%, 16.8%)
08/01 08:44:19 PM | Train: [ 4/210] Step 4830/6092 Loss 5.375 Prec@(1,5) (5.9%, 16.8%)
08/01 08:44:36 PM | Train: [ 4/210] Step 4840/6092 Loss 5.376 Prec@(1,5) (5.9%, 16.8%)
08/01 08:44:54 PM | Train: [ 4/210] Step 4850/6092 Loss 5.375 Prec@(1,5) (5.9%, 16.8%)
08/01 08:45:13 PM | Train: [ 4/210] Step 4860/6092 Loss 5.376 Prec@(1,5) (5.9%, 16.8%)
08/01 08:45:32 PM | Train: [ 4/210] Step 4870/6092 Loss 5.376 Prec@(1,5) (5.9%, 16.8%)
08/01 08:45:51 PM | Train: [ 4/210] Step 4880/6092 Loss 5.375 Prec@(1,5) (5.9%, 16.8%)
08/01 08:46:08 PM | Train: [ 4/210] Step 4890/6092 Loss 5.375 Prec@(1,5) (5.9%, 16.8%)
08/01 08:46:26 PM | Train: [ 4/210] Step 4900/6092 Loss 5.375 Prec@(1,5) (5.9%, 16.8%)
08/01 08:46:44 PM | Train: [ 4/210] Step 4910/6092 Loss 5.375 Prec@(1,5) (5.9%, 16.8%)
08/01 08:47:01 PM | Train: [ 4/210] Step 4920/6092 Loss 5.376 Prec@(1,5) (5.9%, 16.7%)
08/01 08:47:17 PM | Train: [ 4/210] Step 4930/6092 Loss 5.376 Prec@(1,5) (5.9%, 16.7%)
08/01 08:47:34 PM | Train: [ 4/210] Step 4940/6092 Loss 5.375 Prec@(1,5) (5.9%, 16.7%)
08/01 08:47:52 PM | Train: [ 4/210] Step 4950/6092 Loss 5.374 Prec@(1,5) (5.9%, 16.7%)
08/01 08:48:09 PM | Train: [ 4/210] Step 4960/6092 Loss 5.375 Prec@(1,5) (5.9%, 16.7%)
08/01 08:48:25 PM | Train: [ 4/210] Step 4970/6092 Loss 5.373 Prec@(1,5) (5.9%, 16.8%)
08/01 08:48:43 PM | Train: [ 4/210] Step 4980/6092 Loss 5.372 Prec@(1,5) (5.9%, 16.8%)
08/01 08:49:00 PM | Train: [ 4/210] Step 4990/6092 Loss 5.372 Prec@(1,5) (5.9%, 16.8%)
08/01 08:49:18 PM | Train: [ 4/210] Step 5000/6092 Loss 5.371 Prec@(1,5) (5.9%, 16.8%)
08/01 08:49:35 PM | Train: [ 4/210] Step 5010/6092 Loss 5.371 Prec@(1,5) (5.9%, 16.8%)
08/01 08:49:52 PM | Train: [ 4/210] Step 5020/6092 Loss 5.370 Prec@(1,5) (5.9%, 16.8%)
08/01 08:50:13 PM | Train: [ 4/210] Step 5030/6092 Loss 5.372 Prec@(1,5) (5.9%, 16.8%)
08/01 08:50:30 PM | Train: [ 4/210] Step 5040/6092 Loss 5.372 Prec@(1,5) (5.9%, 16.8%)
08/01 08:50:47 PM | Train: [ 4/210] Step 5050/6092 Loss 5.371 Prec@(1,5) (5.9%, 16.8%)
08/01 08:51:04 PM | Train: [ 4/210] Step 5060/6092 Loss 5.370 Prec@(1,5) (5.9%, 16.8%)
08/01 08:51:21 PM | Train: [ 4/210] Step 5070/6092 Loss 5.371 Prec@(1,5) (5.9%, 16.8%)
08/01 08:51:38 PM | Train: [ 4/210] Step 5080/6092 Loss 5.372 Prec@(1,5) (5.9%, 16.8%)
08/01 08:51:56 PM | Train: [ 4/210] Step 5090/6092 Loss 5.371 Prec@(1,5) (5.9%, 16.8%)
08/01 08:52:13 PM | Train: [ 4/210] Step 5100/6092 Loss 5.371 Prec@(1,5) (5.9%, 16.8%)
08/01 08:52:30 PM | Train: [ 4/210] Step 5110/6092 Loss 5.371 Prec@(1,5) (5.9%, 16.8%)
08/01 08:52:47 PM | Train: [ 4/210] Step 5120/6092 Loss 5.372 Prec@(1,5) (5.9%, 16.8%)
08/01 08:53:04 PM | Train: [ 4/210] Step 5130/6092 Loss 5.371 Prec@(1,5) (5.9%, 16.8%)
08/01 08:53:21 PM | Train: [ 4/210] Step 5140/6092 Loss 5.372 Prec@(1,5) (5.9%, 16.8%)
08/01 08:53:38 PM | Train: [ 4/210] Step 5150/6092 Loss 5.371 Prec@(1,5) (5.9%, 16.8%)
08/01 08:53:56 PM | Train: [ 4/210] Step 5160/6092 Loss 5.372 Prec@(1,5) (5.9%, 16.8%)
08/01 08:54:14 PM | Train: [ 4/210] Step 5170/6092 Loss 5.371 Prec@(1,5) (6.0%, 16.9%)
08/01 08:54:31 PM | Train: [ 4/210] Step 5180/6092 Loss 5.371 Prec@(1,5) (6.0%, 16.9%)
08/01 08:54:48 PM | Train: [ 4/210] Step 5190/6092 Loss 5.372 Prec@(1,5) (6.0%, 16.8%)
08/01 08:55:07 PM | Train: [ 4/210] Step 5200/6092 Loss 5.372 Prec@(1,5) (5.9%, 16.8%)
08/01 08:55:26 PM | Train: [ 4/210] Step 5210/6092 Loss 5.371 Prec@(1,5) (5.9%, 16.8%)
08/01 08:55:45 PM | Train: [ 4/210] Step 5220/6092 Loss 5.370 Prec@(1,5) (5.9%, 16.9%)
08/01 08:56:03 PM | Train: [ 4/210] Step 5230/6092 Loss 5.371 Prec@(1,5) (5.9%, 16.8%)
08/01 08:56:22 PM | Train: [ 4/210] Step 5240/6092 Loss 5.371 Prec@(1,5) (5.9%, 16.8%)
08/01 08:56:40 PM | Train: [ 4/210] Step 5250/6092 Loss 5.371 Prec@(1,5) (6.0%, 16.9%)
08/01 08:56:58 PM | Train: [ 4/210] Step 5260/6092 Loss 5.372 Prec@(1,5) (5.9%, 16.8%)
08/01 08:57:16 PM | Train: [ 4/210] Step 5270/6092 Loss 5.372 Prec@(1,5) (5.9%, 16.8%)
08/01 08:57:33 PM | Train: [ 4/210] Step 5280/6092 Loss 5.372 Prec@(1,5) (5.9%, 16.8%)
08/01 08:57:51 PM | Train: [ 4/210] Step 5290/6092 Loss 5.372 Prec@(1,5) (6.0%, 16.8%)
08/01 08:58:08 PM | Train: [ 4/210] Step 5300/6092 Loss 5.371 Prec@(1,5) (6.0%, 16.9%)
08/01 08:58:25 PM | Train: [ 4/210] Step 5310/6092 Loss 5.370 Prec@(1,5) (6.0%, 16.9%)
08/01 08:58:42 PM | Train: [ 4/210] Step 5320/6092 Loss 5.369 Prec@(1,5) (6.0%, 16.9%)
08/01 08:58:59 PM | Train: [ 4/210] Step 5330/6092 Loss 5.370 Prec@(1,5) (6.0%, 16.9%)
08/01 08:59:16 PM | Train: [ 4/210] Step 5340/6092 Loss 5.369 Prec@(1,5) (6.0%, 16.9%)
08/01 08:59:34 PM | Train: [ 4/210] Step 5350/6092 Loss 5.369 Prec@(1,5) (6.0%, 16.9%)
08/01 08:59:51 PM | Train: [ 4/210] Step 5360/6092 Loss 5.369 Prec@(1,5) (6.0%, 16.9%)
08/01 09:00:08 PM | Train: [ 4/210] Step 5370/6092 Loss 5.369 Prec@(1,5) (6.0%, 16.9%)
08/01 09:00:26 PM | Train: [ 4/210] Step 5380/6092 Loss 5.368 Prec@(1,5) (6.0%, 17.0%)
08/01 09:00:45 PM | Train: [ 4/210] Step 5390/6092 Loss 5.367 Prec@(1,5) (6.0%, 17.0%)
08/01 09:01:03 PM | Train: [ 4/210] Step 5400/6092 Loss 5.368 Prec@(1,5) (6.0%, 17.0%)
08/01 09:01:20 PM | Train: [ 4/210] Step 5410/6092 Loss 5.368 Prec@(1,5) (6.0%, 17.0%)
08/01 09:01:37 PM | Train: [ 4/210] Step 5420/6092 Loss 5.367 Prec@(1,5) (6.0%, 17.0%)
08/01 09:01:55 PM | Train: [ 4/210] Step 5430/6092 Loss 5.367 Prec@(1,5) (6.0%, 17.0%)
08/01 09:02:12 PM | Train: [ 4/210] Step 5440/6092 Loss 5.367 Prec@(1,5) (6.0%, 17.0%)
08/01 09:02:29 PM | Train: [ 4/210] Step 5450/6092 Loss 5.365 Prec@(1,5) (6.0%, 17.0%)
08/01 09:02:46 PM | Train: [ 4/210] Step 5460/6092 Loss 5.364 Prec@(1,5) (6.0%, 17.0%)
08/01 09:03:03 PM | Train: [ 4/210] Step 5470/6092 Loss 5.364 Prec@(1,5) (6.0%, 17.0%)
08/01 09:03:22 PM | Train: [ 4/210] Step 5480/6092 Loss 5.363 Prec@(1,5) (6.0%, 17.0%)
08/01 09:03:40 PM | Train: [ 4/210] Step 5490/6092 Loss 5.362 Prec@(1,5) (6.0%, 17.0%)
08/01 09:03:57 PM | Train: [ 4/210] Step 5500/6092 Loss 5.363 Prec@(1,5) (6.0%, 17.0%)
08/01 09:04:14 PM | Train: [ 4/210] Step 5510/6092 Loss 5.364 Prec@(1,5) (6.0%, 17.0%)
08/01 09:04:32 PM | Train: [ 4/210] Step 5520/6092 Loss 5.364 Prec@(1,5) (6.0%, 17.0%)
08/01 09:04:50 PM | Train: [ 4/210] Step 5530/6092 Loss 5.364 Prec@(1,5) (6.0%, 17.0%)
08/01 09:05:10 PM | Train: [ 4/210] Step 5540/6092 Loss 5.365 Prec@(1,5) (6.0%, 17.0%)
08/01 09:05:27 PM | Train: [ 4/210] Step 5550/6092 Loss 5.365 Prec@(1,5) (6.0%, 17.0%)
08/01 09:05:44 PM | Train: [ 4/210] Step 5560/6092 Loss 5.365 Prec@(1,5) (6.0%, 17.0%)
08/01 09:06:02 PM | Train: [ 4/210] Step 5570/6092 Loss 5.365 Prec@(1,5) (6.0%, 17.0%)
08/01 09:06:19 PM | Train: [ 4/210] Step 5580/6092 Loss 5.365 Prec@(1,5) (6.0%, 17.0%)
08/01 09:06:36 PM | Train: [ 4/210] Step 5590/6092 Loss 5.366 Prec@(1,5) (6.0%, 17.0%)
08/01 09:06:54 PM | Train: [ 4/210] Step 5600/6092 Loss 5.366 Prec@(1,5) (6.0%, 17.0%)
08/01 09:07:11 PM | Train: [ 4/210] Step 5610/6092 Loss 5.365 Prec@(1,5) (6.0%, 17.0%)
08/01 09:07:28 PM | Train: [ 4/210] Step 5620/6092 Loss 5.364 Prec@(1,5) (6.0%, 17.0%)
08/01 09:07:45 PM | Train: [ 4/210] Step 5630/6092 Loss 5.364 Prec@(1,5) (6.0%, 17.0%)
08/01 09:08:03 PM | Train: [ 4/210] Step 5640/6092 Loss 5.364 Prec@(1,5) (6.0%, 17.0%)
08/01 09:08:20 PM | Train: [ 4/210] Step 5650/6092 Loss 5.364 Prec@(1,5) (6.0%, 17.0%)
08/01 09:08:37 PM | Train: [ 4/210] Step 5660/6092 Loss 5.364 Prec@(1,5) (6.0%, 17.0%)
08/01 09:08:55 PM | Train: [ 4/210] Step 5670/6092 Loss 5.364 Prec@(1,5) (6.0%, 17.0%)
08/01 09:09:12 PM | Train: [ 4/210] Step 5680/6092 Loss 5.363 Prec@(1,5) (6.0%, 17.0%)
08/01 09:09:29 PM | Train: [ 4/210] Step 5690/6092 Loss 5.363 Prec@(1,5) (6.0%, 17.0%)
08/01 09:09:46 PM | Train: [ 4/210] Step 5700/6092 Loss 5.363 Prec@(1,5) (6.0%, 17.0%)
08/01 09:10:03 PM | Train: [ 4/210] Step 5710/6092 Loss 5.363 Prec@(1,5) (6.0%, 17.0%)
08/01 09:10:22 PM | Train: [ 4/210] Step 5720/6092 Loss 5.363 Prec@(1,5) (6.0%, 17.0%)
08/01 09:10:39 PM | Train: [ 4/210] Step 5730/6092 Loss 5.363 Prec@(1,5) (6.0%, 17.0%)
08/01 09:10:56 PM | Train: [ 4/210] Step 5740/6092 Loss 5.360 Prec@(1,5) (6.0%, 17.0%)
08/01 09:11:14 PM | Train: [ 4/210] Step 5750/6092 Loss 5.360 Prec@(1,5) (6.0%, 17.0%)
08/01 09:11:31 PM | Train: [ 4/210] Step 5760/6092 Loss 5.359 Prec@(1,5) (6.0%, 17.1%)
08/01 09:11:48 PM | Train: [ 4/210] Step 5770/6092 Loss 5.359 Prec@(1,5) (6.0%, 17.1%)
08/01 09:12:05 PM | Train: [ 4/210] Step 5780/6092 Loss 5.360 Prec@(1,5) (6.0%, 17.0%)
08/01 09:12:22 PM | Train: [ 4/210] Step 5790/6092 Loss 5.360 Prec@(1,5) (6.0%, 17.1%)
08/01 09:12:40 PM | Train: [ 4/210] Step 5800/6092 Loss 5.359 Prec@(1,5) (6.0%, 17.1%)
08/01 09:12:57 PM | Train: [ 4/210] Step 5810/6092 Loss 5.359 Prec@(1,5) (6.0%, 17.1%)
08/01 09:13:15 PM | Train: [ 4/210] Step 5820/6092 Loss 5.359 Prec@(1,5) (6.0%, 17.1%)
08/01 09:13:32 PM | Train: [ 4/210] Step 5830/6092 Loss 5.360 Prec@(1,5) (6.0%, 17.0%)
08/01 09:13:50 PM | Train: [ 4/210] Step 5840/6092 Loss 5.361 Prec@(1,5) (6.0%, 17.0%)
08/01 09:14:06 PM | Train: [ 4/210] Step 5850/6092 Loss 5.360 Prec@(1,5) (6.0%, 17.0%)
08/01 09:14:24 PM | Train: [ 4/210] Step 5860/6092 Loss 5.361 Prec@(1,5) (6.0%, 17.0%)
08/01 09:14:41 PM | Train: [ 4/210] Step 5870/6092 Loss 5.361 Prec@(1,5) (6.0%, 17.0%)
08/01 09:14:58 PM | Train: [ 4/210] Step 5880/6092 Loss 5.361 Prec@(1,5) (6.0%, 17.0%)
08/01 09:15:16 PM | Train: [ 4/210] Step 5890/6092 Loss 5.362 Prec@(1,5) (6.0%, 17.0%)
08/01 09:15:34 PM | Train: [ 4/210] Step 5900/6092 Loss 5.361 Prec@(1,5) (6.0%, 17.0%)
08/01 09:15:52 PM | Train: [ 4/210] Step 5910/6092 Loss 5.361 Prec@(1,5) (6.0%, 17.0%)
08/01 09:16:09 PM | Train: [ 4/210] Step 5920/6092 Loss 5.361 Prec@(1,5) (6.0%, 17.1%)
08/01 09:16:26 PM | Train: [ 4/210] Step 5930/6092 Loss 5.361 Prec@(1,5) (6.0%, 17.1%)
08/01 09:16:43 PM | Train: [ 4/210] Step 5940/6092 Loss 5.360 Prec@(1,5) (6.0%, 17.1%)
08/01 09:17:01 PM | Train: [ 4/210] Step 5950/6092 Loss 5.360 Prec@(1,5) (6.0%, 17.1%)
08/01 09:17:18 PM | Train: [ 4/210] Step 5960/6092 Loss 5.360 Prec@(1,5) (6.0%, 17.1%)
08/01 09:17:35 PM | Train: [ 4/210] Step 5970/6092 Loss 5.360 Prec@(1,5) (6.0%, 17.1%)
08/01 09:17:53 PM | Train: [ 4/210] Step 5980/6092 Loss 5.362 Prec@(1,5) (6.0%, 17.1%)
08/01 09:18:10 PM | Train: [ 4/210] Step 5990/6092 Loss 5.361 Prec@(1,5) (6.0%, 17.1%)
08/01 09:18:28 PM | Train: [ 4/210] Step 6000/6092 Loss 5.361 Prec@(1,5) (6.0%, 17.1%)
08/01 09:18:45 PM | Train: [ 4/210] Step 6010/6092 Loss 5.361 Prec@(1,5) (6.0%, 17.0%)
08/01 09:19:02 PM | Train: [ 4/210] Step 6020/6092 Loss 5.360 Prec@(1,5) (6.0%, 17.1%)
08/01 09:19:19 PM | Train: [ 4/210] Step 6030/6092 Loss 5.359 Prec@(1,5) (6.0%, 17.1%)
08/01 09:19:36 PM | Train: [ 4/210] Step 6040/6092 Loss 5.359 Prec@(1,5) (6.0%, 17.1%)
08/01 09:19:54 PM | Train: [ 4/210] Step 6050/6092 Loss 5.359 Prec@(1,5) (6.0%, 17.1%)
08/01 09:20:12 PM | Train: [ 4/210] Step 6060/6092 Loss 5.360 Prec@(1,5) (6.0%, 17.1%)
08/01 09:20:29 PM | Train: [ 4/210] Step 6070/6092 Loss 5.360 Prec@(1,5) (6.0%, 17.1%)
08/01 09:20:47 PM | Train: [ 4/210] Step 6080/6092 Loss 5.360 Prec@(1,5) (6.0%, 17.1%)
08/01 09:21:04 PM | Train: [ 4/210] Step 6090/6092 Loss 5.360 Prec@(1,5) (6.0%, 17.1%)
08/01 09:21:07 PM | Train: [ 4/210] Step 6092/6092 Loss 5.360 Prec@(1,5) (6.0%, 17.1%)
08/01 09:21:09 PM | Train: [ 4/210] Final Prec@1 6.0074%
