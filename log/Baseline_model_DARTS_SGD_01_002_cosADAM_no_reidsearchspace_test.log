08/01 09:46:44 PM | Logger is set 
08/01 09:46:44 PM | Logger without distribution
08/01 09:47:13 PM | Initializing dataset used 29.350975036621094 basic time unit
08/01 09:47:13 PM | The training classes labels length :  751
08/01 09:48:22 PM | batch loading time example is 68.63068771362305
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1248, 0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251],
        [0.1249, 0.1250, 0.1247, 0.1250, 0.1252, 0.1251, 0.1250, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1251],
        [0.1251, 0.1250, 0.1248, 0.1249, 0.1251, 0.1252, 0.1251, 0.1249],
        [0.1252, 0.1250, 0.1251, 0.1251, 0.1250, 0.1246, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1252, 0.1249, 0.1250, 0.1249, 0.1251, 0.1250, 0.1249, 0.1249],
        [0.1248, 0.1249, 0.1251, 0.1252, 0.1250, 0.1248, 0.1250, 0.1252],
        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1251, 0.1250, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1250, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1248, 0.1250, 0.1248, 0.1251, 0.1251, 0.1249, 0.1252],
        [0.1250, 0.1251, 0.1251, 0.1250, 0.1251, 0.1249, 0.1251, 0.1248],
        [0.1248, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251, 0.1252, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1249, 0.1250, 0.1250, 0.1251, 0.1250],
        [0.1251, 0.1251, 0.1250, 0.1249, 0.1250, 0.1252, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1252, 0.1252, 0.1248, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249],
        [0.1251, 0.1251, 0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1251, 0.1249, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250],
        [0.1251, 0.1251, 0.1249, 0.1252, 0.1250, 0.1250, 0.1248, 0.1249],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1248, 0.1251, 0.1251, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1251, 0.1249, 0.1250, 0.1249, 0.1250, 0.1251, 0.1250],
        [0.1251, 0.1249, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],
        [0.1251, 0.1249, 0.1250, 0.1252, 0.1251, 0.1247, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1251, 0.1248, 0.1250, 0.1251, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1251, 0.1249, 0.1251, 0.1248, 0.1251, 0.1252, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1248, 0.1251, 0.1252, 0.1250],
        [0.1251, 0.1251, 0.1250, 0.1252, 0.1250, 0.1248, 0.1250, 0.1248],
        [0.1253, 0.1251, 0.1250, 0.1249, 0.1250, 0.1249, 0.1248, 0.1249],
        [0.1251, 0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
08/01 09:50:35 PM | Train: [ 1/210] Step 000/6092 Loss 6.589 Prec@(1,5) (0.0%, 0.0%)
08/01 09:50:53 PM | Train: [ 1/210] Step 010/6092 Loss 6.647 Prec@(1,5) (0.0%, 0.0%)
08/01 09:51:09 PM | Train: [ 1/210] Step 020/6092 Loss 6.613 Prec@(1,5) (0.0%, 4.8%)
08/01 09:51:25 PM | Train: [ 1/210] Step 030/6092 Loss 6.809 Prec@(1,5) (0.0%, 3.2%)
08/01 09:51:42 PM | Train: [ 1/210] Step 040/6092 Loss 6.804 Prec@(1,5) (0.0%, 2.4%)
08/01 09:51:58 PM | Train: [ 1/210] Step 050/6092 Loss 6.780 Prec@(1,5) (0.0%, 2.0%)
08/01 09:52:14 PM | Train: [ 1/210] Step 060/6092 Loss 6.755 Prec@(1,5) (0.0%, 1.6%)
08/01 09:52:31 PM | Train: [ 1/210] Step 070/6092 Loss 6.772 Prec@(1,5) (0.0%, 1.4%)
08/01 09:52:47 PM | Train: [ 1/210] Step 080/6092 Loss 6.771 Prec@(1,5) (0.0%, 1.2%)
08/01 09:53:04 PM | Train: [ 1/210] Step 090/6092 Loss 6.768 Prec@(1,5) (0.0%, 1.1%)
08/01 09:53:21 PM | Train: [ 1/210] Step 100/6092 Loss 6.768 Prec@(1,5) (0.0%, 1.0%)
08/01 09:53:37 PM | Train: [ 1/210] Step 110/6092 Loss 6.772 Prec@(1,5) (0.0%, 0.9%)
08/01 09:53:54 PM | Train: [ 1/210] Step 120/6092 Loss 6.751 Prec@(1,5) (0.4%, 1.2%)
08/01 09:54:11 PM | Train: [ 1/210] Step 130/6092 Loss 6.761 Prec@(1,5) (0.4%, 1.1%)
08/01 09:54:27 PM | Train: [ 1/210] Step 140/6092 Loss 6.754 Prec@(1,5) (0.4%, 1.4%)
08/01 09:54:44 PM | Train: [ 1/210] Step 150/6092 Loss 6.759 Prec@(1,5) (0.3%, 1.3%)
08/01 09:55:00 PM | Train: [ 1/210] Step 160/6092 Loss 6.756 Prec@(1,5) (0.3%, 1.6%)
08/01 09:55:18 PM | Train: [ 1/210] Step 170/6092 Loss 6.752 Prec@(1,5) (0.3%, 1.5%)
08/01 09:55:35 PM | Train: [ 1/210] Step 180/6092 Loss 6.747 Prec@(1,5) (0.3%, 1.4%)
08/01 09:55:51 PM | Train: [ 1/210] Step 190/6092 Loss 6.732 Prec@(1,5) (0.3%, 1.6%)
08/01 09:56:08 PM | Train: [ 1/210] Step 200/6092 Loss 6.728 Prec@(1,5) (0.2%, 1.5%)
08/01 09:56:24 PM | Train: [ 1/210] Step 210/6092 Loss 6.737 Prec@(1,5) (0.2%, 1.4%)
08/01 09:56:41 PM | Train: [ 1/210] Step 220/6092 Loss 6.740 Prec@(1,5) (0.2%, 1.4%)
08/01 09:56:58 PM | Train: [ 1/210] Step 230/6092 Loss 6.738 Prec@(1,5) (0.2%, 1.3%)
08/01 09:57:15 PM | Train: [ 1/210] Step 240/6092 Loss 6.737 Prec@(1,5) (0.2%, 1.2%)
08/01 09:57:31 PM | Train: [ 1/210] Step 250/6092 Loss 6.731 Prec@(1,5) (0.2%, 1.2%)
08/01 09:57:48 PM | Train: [ 1/210] Step 260/6092 Loss 6.732 Prec@(1,5) (0.2%, 1.1%)
08/01 09:58:04 PM | Train: [ 1/210] Step 270/6092 Loss 6.730 Prec@(1,5) (0.2%, 1.1%)
08/01 09:58:21 PM | Train: [ 1/210] Step 280/6092 Loss 6.733 Prec@(1,5) (0.2%, 1.1%)
08/01 09:58:38 PM | Train: [ 1/210] Step 290/6092 Loss 6.729 Prec@(1,5) (0.2%, 1.0%)
08/01 09:58:55 PM | Train: [ 1/210] Step 300/6092 Loss 6.725 Prec@(1,5) (0.2%, 1.0%)
08/01 09:59:12 PM | Train: [ 1/210] Step 310/6092 Loss 6.717 Prec@(1,5) (0.2%, 1.0%)
08/01 09:59:28 PM | Train: [ 1/210] Step 320/6092 Loss 6.714 Prec@(1,5) (0.2%, 0.9%)
08/01 09:59:45 PM | Train: [ 1/210] Step 330/6092 Loss 6.707 Prec@(1,5) (0.3%, 1.1%)
08/01 10:00:01 PM | Train: [ 1/210] Step 340/6092 Loss 6.705 Prec@(1,5) (0.3%, 1.0%)
08/01 10:00:20 PM | Train: [ 1/210] Step 350/6092 Loss 6.700 Prec@(1,5) (0.3%, 1.1%)
08/01 10:00:37 PM | Train: [ 1/210] Step 360/6092 Loss 6.699 Prec@(1,5) (0.3%, 1.1%)
08/02 09:46:54 AM | Logger is set 
08/02 09:46:54 AM | Logger without distribution
08/02 09:47:14 AM | Initializing dataset used 19.458070039749146 basic time unit
08/02 09:47:14 AM | The training classes labels length :  751
08/02 09:48:26 AM | batch loading time example is 72.7122893333435
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1248, 0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251],
        [0.1249, 0.1250, 0.1247, 0.1250, 0.1252, 0.1251, 0.1250, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1251],
        [0.1251, 0.1250, 0.1248, 0.1249, 0.1251, 0.1252, 0.1251, 0.1249],
        [0.1252, 0.1250, 0.1251, 0.1251, 0.1250, 0.1246, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1252, 0.1249, 0.1250, 0.1249, 0.1251, 0.1250, 0.1249, 0.1249],
        [0.1248, 0.1249, 0.1251, 0.1252, 0.1250, 0.1248, 0.1250, 0.1252],
        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1251, 0.1250, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1250, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1248, 0.1250, 0.1248, 0.1251, 0.1251, 0.1249, 0.1252],
        [0.1250, 0.1251, 0.1251, 0.1250, 0.1251, 0.1249, 0.1251, 0.1248],
        [0.1248, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251, 0.1252, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1249, 0.1250, 0.1250, 0.1251, 0.1250],
        [0.1251, 0.1251, 0.1250, 0.1249, 0.1250, 0.1252, 0.1249, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1252, 0.1252, 0.1248, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249],
        [0.1251, 0.1251, 0.1251, 0.1250, 0.1250, 0.1249, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1251, 0.1249, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250],
        [0.1251, 0.1251, 0.1249, 0.1252, 0.1250, 0.1250, 0.1248, 0.1249],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1248, 0.1251, 0.1251, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1251, 0.1249, 0.1250, 0.1249, 0.1250, 0.1251, 0.1250],
        [0.1251, 0.1249, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],
        [0.1251, 0.1249, 0.1250, 0.1252, 0.1251, 0.1247, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1251, 0.1248, 0.1250, 0.1251, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1251, 0.1249, 0.1251, 0.1248, 0.1251, 0.1252, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1248, 0.1251, 0.1252, 0.1250],
        [0.1251, 0.1251, 0.1250, 0.1252, 0.1250, 0.1248, 0.1250, 0.1248],
        [0.1253, 0.1251, 0.1250, 0.1249, 0.1250, 0.1249, 0.1248, 0.1249],
        [0.1251, 0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
08/02 09:51:10 AM | Train: [ 1/210] Step 000/6092 Loss 6.589 Prec@(1,5) (0.0%, 0.0%)
08/02 09:51:27 AM | Train: [ 1/210] Step 010/6092 Loss 6.643 Prec@(1,5) (0.0%, 0.0%)
08/02 09:51:43 AM | Train: [ 1/210] Step 020/6092 Loss 6.627 Prec@(1,5) (0.0%, 2.4%)
08/02 09:51:59 AM | Train: [ 1/210] Step 030/6092 Loss 6.793 Prec@(1,5) (0.0%, 1.6%)
08/02 09:52:15 AM | Train: [ 1/210] Step 040/6092 Loss 6.799 Prec@(1,5) (0.0%, 1.2%)
08/02 09:52:31 AM | Train: [ 1/210] Step 050/6092 Loss 6.743 Prec@(1,5) (0.0%, 2.0%)
08/02 09:52:48 AM | Train: [ 1/210] Step 060/6092 Loss 6.765 Prec@(1,5) (0.0%, 2.5%)
08/02 09:53:04 AM | Train: [ 1/210] Step 070/6092 Loss 6.769 Prec@(1,5) (0.0%, 2.1%)
08/02 09:53:20 AM | Train: [ 1/210] Step 080/6092 Loss 6.764 Prec@(1,5) (0.0%, 1.9%)
08/02 09:53:37 AM | Train: [ 1/210] Step 090/6092 Loss 6.758 Prec@(1,5) (0.0%, 1.6%)
08/02 09:53:53 AM | Train: [ 1/210] Step 100/6092 Loss 6.762 Prec@(1,5) (0.0%, 1.5%)
08/02 09:54:09 AM | Train: [ 1/210] Step 110/6092 Loss 6.757 Prec@(1,5) (0.5%, 1.8%)
08/02 09:54:25 AM | Train: [ 1/210] Step 120/6092 Loss 6.753 Prec@(1,5) (0.4%, 2.1%)
08/02 09:54:42 AM | Train: [ 1/210] Step 130/6092 Loss 6.767 Prec@(1,5) (0.4%, 1.9%)
08/02 09:54:58 AM | Train: [ 1/210] Step 140/6092 Loss 6.759 Prec@(1,5) (0.4%, 1.8%)
08/02 09:55:17 AM | Train: [ 1/210] Step 150/6092 Loss 6.759 Prec@(1,5) (0.3%, 1.7%)
08/02 09:55:33 AM | Train: [ 1/210] Step 160/6092 Loss 6.757 Prec@(1,5) (0.3%, 1.6%)
08/02 09:55:49 AM | Train: [ 1/210] Step 170/6092 Loss 6.751 Prec@(1,5) (0.3%, 1.5%)
08/02 09:56:05 AM | Train: [ 1/210] Step 180/6092 Loss 6.748 Prec@(1,5) (0.3%, 1.7%)
08/02 09:56:21 AM | Train: [ 1/210] Step 190/6092 Loss 6.744 Prec@(1,5) (0.3%, 1.6%)
08/02 09:56:38 AM | Train: [ 1/210] Step 200/6092 Loss 6.739 Prec@(1,5) (0.2%, 1.5%)
08/02 09:56:54 AM | Train: [ 1/210] Step 210/6092 Loss 6.744 Prec@(1,5) (0.2%, 1.4%)
08/02 09:57:10 AM | Train: [ 1/210] Step 220/6092 Loss 6.741 Prec@(1,5) (0.2%, 1.4%)
08/02 09:57:27 AM | Train: [ 1/210] Step 230/6092 Loss 6.739 Prec@(1,5) (0.2%, 1.5%)
08/02 09:57:43 AM | Train: [ 1/210] Step 240/6092 Loss 6.735 Prec@(1,5) (0.2%, 1.5%)
08/02 09:57:59 AM | Train: [ 1/210] Step 250/6092 Loss 6.731 Prec@(1,5) (0.2%, 1.4%)
08/02 09:58:15 AM | Train: [ 1/210] Step 260/6092 Loss 6.733 Prec@(1,5) (0.2%, 1.3%)
08/02 09:58:31 AM | Train: [ 1/210] Step 270/6092 Loss 6.731 Prec@(1,5) (0.2%, 1.3%)
08/02 09:58:47 AM | Train: [ 1/210] Step 280/6092 Loss 6.732 Prec@(1,5) (0.2%, 1.2%)
08/02 09:59:03 AM | Train: [ 1/210] Step 290/6092 Loss 6.728 Prec@(1,5) (0.2%, 1.2%)
08/02 09:59:20 AM | Train: [ 1/210] Step 300/6092 Loss 6.722 Prec@(1,5) (0.2%, 1.3%)
08/02 09:59:36 AM | Train: [ 1/210] Step 310/6092 Loss 6.713 Prec@(1,5) (0.2%, 1.3%)
08/02 09:59:52 AM | Train: [ 1/210] Step 320/6092 Loss 6.711 Prec@(1,5) (0.2%, 1.2%)
08/02 10:00:14 AM | Train: [ 1/210] Step 330/6092 Loss 6.705 Prec@(1,5) (0.3%, 1.4%)
08/02 10:00:30 AM | Train: [ 1/210] Step 340/6092 Loss 6.704 Prec@(1,5) (0.3%, 1.3%)
08/02 10:00:46 AM | Train: [ 1/210] Step 350/6092 Loss 6.703 Prec@(1,5) (0.3%, 1.4%)
08/02 10:01:03 AM | Train: [ 1/210] Step 360/6092 Loss 6.700 Prec@(1,5) (0.3%, 1.4%)
08/02 10:01:19 AM | Train: [ 1/210] Step 370/6092 Loss 6.693 Prec@(1,5) (0.3%, 1.3%)
08/02 10:01:42 AM | Train: [ 1/210] Final Prec@1 0.2660%
08/02 10:02:45 AM | Valid: [ 1/210] Step 000/375 Loss 5.882 Prec@(1,5) (0.0%, 0.0%)
08/02 10:02:46 AM | Valid: [ 1/210] Step 010/375 Loss 6.566 Prec@(1,5) (0.0%, 0.0%)
08/02 10:02:47 AM | Valid: [ 1/210] Step 020/375 Loss 6.728 Prec@(1,5) (0.0%, 0.0%)
08/02 10:02:47 AM | Valid: [ 1/210] Step 030/375 Loss 6.783 Prec@(1,5) (0.0%, 0.0%)
08/02 10:02:48 AM | Valid: [ 1/210] Step 040/375 Loss 6.752 Prec@(1,5) (0.0%, 1.2%)
08/02 10:02:49 AM | Valid: [ 1/210] Step 050/375 Loss 6.735 Prec@(1,5) (0.0%, 2.0%)
08/02 10:02:50 AM | Valid: [ 1/210] Step 060/375 Loss 6.729 Prec@(1,5) (0.0%, 2.5%)
08/02 10:02:51 AM | Valid: [ 1/210] Step 070/375 Loss 6.733 Prec@(1,5) (0.0%, 2.1%)
08/02 10:02:51 AM | Valid: [ 1/210] Step 080/375 Loss 6.719 Prec@(1,5) (0.0%, 1.9%)
08/02 10:02:52 AM | Valid: [ 1/210] Step 090/375 Loss 6.712 Prec@(1,5) (0.0%, 1.6%)
08/02 10:02:53 AM | Valid: [ 1/210] Step 100/375 Loss 6.726 Prec@(1,5) (0.0%, 1.5%)
08/02 10:02:54 AM | Valid: [ 1/210] Step 110/375 Loss 6.735 Prec@(1,5) (0.0%, 1.4%)
08/02 10:02:55 AM | Valid: [ 1/210] Step 120/375 Loss 6.742 Prec@(1,5) (0.0%, 1.2%)
08/02 10:02:56 AM | Valid: [ 1/210] Step 130/375 Loss 6.751 Prec@(1,5) (0.0%, 1.1%)
08/02 10:02:56 AM | Valid: [ 1/210] Step 140/375 Loss 6.732 Prec@(1,5) (0.0%, 1.1%)
08/02 10:02:57 AM | Valid: [ 1/210] Step 150/375 Loss 6.737 Prec@(1,5) (0.0%, 1.0%)
08/02 10:02:58 AM | Valid: [ 1/210] Step 160/375 Loss 6.734 Prec@(1,5) (0.0%, 0.9%)
08/02 10:02:59 AM | Valid: [ 1/210] Step 170/375 Loss 6.740 Prec@(1,5) (0.0%, 0.9%)
08/02 10:03:00 AM | Valid: [ 1/210] Step 180/375 Loss 6.724 Prec@(1,5) (0.0%, 0.8%)
08/02 10:03:01 AM | Valid: [ 1/210] Step 190/375 Loss 6.731 Prec@(1,5) (0.0%, 0.8%)
08/02 10:03:02 AM | Valid: [ 1/210] Step 200/375 Loss 6.731 Prec@(1,5) (0.0%, 0.7%)
08/02 10:03:03 AM | Valid: [ 1/210] Step 210/375 Loss 6.734 Prec@(1,5) (0.0%, 0.7%)
08/02 10:03:03 AM | Valid: [ 1/210] Step 220/375 Loss 6.729 Prec@(1,5) (0.0%, 0.7%)
08/02 10:03:04 AM | Valid: [ 1/210] Step 230/375 Loss 6.726 Prec@(1,5) (0.0%, 0.6%)
08/02 10:03:05 AM | Valid: [ 1/210] Step 240/375 Loss 6.719 Prec@(1,5) (0.0%, 0.6%)
08/02 10:03:06 AM | Valid: [ 1/210] Step 250/375 Loss 6.727 Prec@(1,5) (0.0%, 0.6%)
08/02 10:03:07 AM | Valid: [ 1/210] Step 260/375 Loss 6.714 Prec@(1,5) (0.0%, 0.6%)
08/02 10:03:08 AM | Valid: [ 1/210] Step 270/375 Loss 6.711 Prec@(1,5) (0.0%, 0.6%)
08/02 10:03:08 AM | Valid: [ 1/210] Step 280/375 Loss 6.703 Prec@(1,5) (0.2%, 0.7%)
08/02 10:03:09 AM | Valid: [ 1/210] Step 290/375 Loss 6.707 Prec@(1,5) (0.2%, 0.7%)
08/02 10:03:10 AM | Valid: [ 1/210] Step 300/375 Loss 6.709 Prec@(1,5) (0.2%, 0.7%)
08/02 10:03:11 AM | Valid: [ 1/210] Step 310/375 Loss 6.712 Prec@(1,5) (0.3%, 0.8%)
08/02 10:03:12 AM | Valid: [ 1/210] Step 320/375 Loss 6.716 Prec@(1,5) (0.3%, 0.8%)
08/02 10:03:13 AM | Valid: [ 1/210] Step 330/375 Loss 6.720 Prec@(1,5) (0.3%, 0.8%)
08/02 10:03:14 AM | Valid: [ 1/210] Step 340/375 Loss 6.719 Prec@(1,5) (0.3%, 0.7%)
08/02 10:03:15 AM | Valid: [ 1/210] Step 350/375 Loss 6.724 Prec@(1,5) (0.3%, 0.7%)
08/02 10:03:16 AM | Valid: [ 1/210] Step 360/375 Loss 6.723 Prec@(1,5) (0.3%, 0.8%)
08/02 10:03:17 AM | Valid: [ 1/210] Step 370/375 Loss 6.725 Prec@(1,5) (0.3%, 0.9%)
08/02 10:03:20 AM | Valid: [ 1/210] Final Prec@1 0.2667%, Prec@5 0.9333%, Prec@10 1.7333%
08/02 10:03:20 AM | Final best Prec@1 = 0.2667%
08/02 10:03:20 AM | genotype = Genotype(normal=[[('dil_conv_5x5', 0), ('sep_conv_5x5', 1)], [('sep_conv_3x3', 2), ('dil_conv_3x3', 0)], [('dil_conv_3x3', 0), ('dil_conv_5x5', 2)], [('sep_conv_3x3', 2), ('dil_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_5x5', 1)], [('sep_conv_5x5', 3), ('skip_connect', 0)], [('dil_conv_3x3', 4), ('dil_conv_5x5', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.0782, 0.0727, 0.0742, 0.0636, 0.1304, 0.1576, 0.2918, 0.1315],
        [0.0885, 0.0720, 0.0745, 0.1106, 0.2164, 0.0970, 0.1378, 0.2033]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1104, 0.1027, 0.1051, 0.0691, 0.1080, 0.2071, 0.1146, 0.1828],
        [0.1101, 0.1009, 0.1039, 0.1307, 0.1822, 0.1260, 0.0915, 0.1546],
        [0.1250, 0.0942, 0.0644, 0.2725, 0.1531, 0.0821, 0.1101, 0.0987]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0702, 0.0660, 0.0673, 0.0900, 0.0610, 0.3228, 0.0806, 0.2422],
        [0.0947, 0.0870, 0.0890, 0.0770, 0.0912, 0.0974, 0.1531, 0.3106],
        [0.0787, 0.0627, 0.0443, 0.1197, 0.1093, 0.0805, 0.3555, 0.1494],
        [0.0557, 0.0487, 0.0420, 0.3032, 0.1210, 0.1713, 0.2145, 0.0436]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0575, 0.0540, 0.0546, 0.0968, 0.1036, 0.1853, 0.1028, 0.3453],
        [0.0804, 0.0713, 0.0723, 0.0939, 0.1309, 0.1932, 0.1920, 0.1659],
        [0.0592, 0.0491, 0.0370, 0.4716, 0.1903, 0.0528, 0.0868, 0.0532],
        [0.1474, 0.1179, 0.0602, 0.0935, 0.2533, 0.1028, 0.0902, 0.1347],
        [0.0860, 0.0649, 0.0664, 0.1100, 0.1891, 0.0732, 0.3612, 0.0492]],
       grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.0311, 0.0291, 0.0268, 0.0410, 0.0269, 0.0240, 0.0288, 0.7924],
        [0.0378, 0.0355, 0.0706, 0.1574, 0.1461, 0.0929, 0.0613, 0.3985]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0416, 0.0372, 0.0425, 0.0327, 0.0248, 0.0305, 0.0334, 0.7573],
        [0.0327, 0.0316, 0.0384, 0.0364, 0.0516, 0.0401, 0.0362, 0.7330],
        [0.0221, 0.0204, 0.0261, 0.0623, 0.0285, 0.0331, 0.0416, 0.7661]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0279, 0.0268, 0.0543, 0.0325, 0.0400, 0.0540, 0.0273, 0.7371],
        [0.0225, 0.0238, 0.0416, 0.0382, 0.0425, 0.0358, 0.0489, 0.7467],
        [0.0144, 0.0133, 0.0203, 0.0343, 0.0271, 0.0237, 0.0371, 0.8297],
        [0.0169, 0.0153, 0.0245, 0.0762, 0.1212, 0.0589, 0.0565, 0.6305]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0268, 0.0240, 0.0398, 0.0326, 0.0228, 0.0239, 0.0273, 0.8028],
        [0.0264, 0.0288, 0.0352, 0.0331, 0.0326, 0.0390, 0.0352, 0.7697],
        [0.0210, 0.0210, 0.0350, 0.0466, 0.0420, 0.0402, 0.0363, 0.7578],
        [0.0234, 0.0210, 0.0369, 0.0760, 0.0735, 0.0707, 0.0815, 0.6169],
        [0.0216, 0.0222, 0.0301, 0.0955, 0.0778, 0.1017, 0.0775, 0.5736]],
       grad_fn=<SoftmaxBackward>)
#####################
08/02 10:05:17 AM | Train: [ 2/210] Step 000/6092 Loss 6.825 Prec@(1,5) (0.0%, 0.0%)
08/02 10:05:33 AM | Train: [ 2/210] Step 010/6092 Loss 6.513 Prec@(1,5) (0.0%, 0.0%)
08/02 10:05:49 AM | Train: [ 2/210] Step 020/6092 Loss 6.668 Prec@(1,5) (0.0%, 0.0%)
08/02 10:06:05 AM | Train: [ 2/210] Step 030/6092 Loss 6.615 Prec@(1,5) (0.0%, 0.0%)
08/02 10:06:22 AM | Train: [ 2/210] Step 040/6092 Loss 6.588 Prec@(1,5) (0.0%, 0.0%)
08/02 10:06:38 AM | Train: [ 2/210] Step 050/6092 Loss 6.612 Prec@(1,5) (0.0%, 0.0%)
