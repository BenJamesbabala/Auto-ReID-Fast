08/02 10:50:03 PM | Logger is set 
08/02 10:50:03 PM | Logger with distribution
08/02 10:51:08 PM | Initializing dataset used 65.28579068183899 basic time unit
08/02 10:51:08 PM | Initializing dataset used 65.26591110229492 basic time unit
08/02 10:51:08 PM | Initializing dataset used 65.28593015670776 basic time unit
08/02 10:51:08 PM | Initializing dataset used 64.60776424407959 basic time unit
08/02 10:51:08 PM | The training classes labels length :  751
08/02 10:51:08 PM | The training classes labels length :  751
08/02 10:51:08 PM | The training classes labels length :  751
08/02 10:51:08 PM | The training classes labels length :  751
08/02 10:51:08 PM | Initializing dataset used 65.39442205429077 basic time unit
08/02 10:51:08 PM | The training classes labels length :  751
08/02 10:51:08 PM | Initializing dataset used 65.98064112663269 basic time unit
08/02 10:51:08 PM | The training classes labels length :  751
08/02 10:51:09 PM | Initializing dataset used 66.24137663841248 basic time unit
08/02 10:51:09 PM | The training classes labels length :  751
08/02 10:51:09 PM | Initializing dataset used 66.26533508300781 basic time unit
08/02 10:51:09 PM | The training classes labels length :  751
08/02 10:53:09 PM | batch loading time example is 120.91697716712952
08/02 10:53:09 PM | batch loading time example is 120.98877835273743
08/02 10:53:09 PM | batch loading time example is 120.98892021179199
08/02 10:53:09 PM | batch loading time example is 120.30281209945679
08/02 10:53:09 PM | batch loading time example is 120.9909176826477
08/02 10:53:09 PM | batch loading time example is 120.98510074615479
08/02 10:53:09 PM | batch loading time example is 120.04738187789917
08/02 10:53:09 PM | batch loading time example is 120.05491018295288
####### ALPHA #######
# Alpha - normal
tensor([[0.1428, 0.1428, 0.1430, 0.1426, 0.1429, 0.1430, 0.1428],
        [0.1429, 0.1428, 0.1429, 0.1430, 0.1429, 0.1429, 0.1427]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1430, 0.1428, 0.1429, 0.1427, 0.1426, 0.1428, 0.1431],
        [0.1430, 0.1429, 0.1430, 0.1431, 0.1426, 0.1427, 0.1428],
        [0.1427, 0.1428, 0.1431, 0.1428, 0.1428, 0.1429, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1428, 0.1428, 0.1429, 0.1428, 0.1428, 0.1428, 0.1431],
        [0.1431, 0.1428, 0.1428, 0.1428, 0.1427, 0.1429, 0.1429],
        [0.1428, 0.1427, 0.1428, 0.1428, 0.1429, 0.1429, 0.1431],
        [0.1427, 0.1429, 0.1427, 0.1430, 0.1428, 0.1429, 0.1429]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1428, 0.1429, 0.1427, 0.1429, 0.1429, 0.1428, 0.1429],
        [0.1426, 0.1427, 0.1431, 0.1430, 0.1428, 0.1430, 0.1428],
        [0.1428, 0.1427, 0.1432, 0.1430, 0.1427, 0.1428, 0.1427],
        [0.1429, 0.1427, 0.1429, 0.1428, 0.1428, 0.1431, 0.1429],
        [0.1429, 0.1426, 0.1429, 0.1429, 0.1428, 0.1430, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1428, 0.1428, 0.1429, 0.1428, 0.1429, 0.1426, 0.1432],
        [0.1428, 0.1427, 0.1430, 0.1429, 0.1431, 0.1429, 0.1427]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1430, 0.1429, 0.1429, 0.1428, 0.1428, 0.1429, 0.1428],
        [0.1430, 0.1428, 0.1429, 0.1428, 0.1430, 0.1428, 0.1428],
        [0.1431, 0.1427, 0.1429, 0.1427, 0.1428, 0.1429, 0.1429]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1429, 0.1427, 0.1427, 0.1429, 0.1426, 0.1430, 0.1431],
        [0.1430, 0.1430, 0.1431, 0.1426, 0.1427, 0.1427, 0.1430],
        [0.1431, 0.1428, 0.1426, 0.1429, 0.1429, 0.1430, 0.1427],
        [0.1426, 0.1430, 0.1428, 0.1426, 0.1432, 0.1428, 0.1430]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1428, 0.1427, 0.1427, 0.1431, 0.1429, 0.1428, 0.1429],
        [0.1432, 0.1430, 0.1427, 0.1428, 0.1427, 0.1430, 0.1427],
        [0.1426, 0.1428, 0.1430, 0.1427, 0.1429, 0.1429, 0.1430],
        [0.1427, 0.1430, 0.1430, 0.1427, 0.1427, 0.1428, 0.1430],
        [0.1429, 0.1429, 0.1428, 0.1428, 0.1428, 0.1429, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
08/02 10:57:05 PM | Train: [ 1/210] Step 000/404 Loss 3.313 Prec@(1,5) (0.0%, 0.0%)
08/02 10:57:50 PM | Train: [ 1/210] Step 010/404 Loss 3.290 Prec@(1,5) (0.3%, 2.8%)
08/02 10:58:37 PM | Train: [ 1/210] Step 020/404 Loss 3.324 Prec@(1,5) (0.4%, 3.0%)
08/02 10:59:04 PM | Train: [ 1/210] Final Prec@1 0.5208%
08/02 11:00:52 PM | Valid: [ 1/210] Step 000/023 Loss 3.338 Prec@(1,5) (0.0%, 3.1%)
08/02 11:00:55 PM | Valid: [ 1/210] Step 010/023 Loss 3.407 Prec@(1,5) (0.0%, 1.1%)
08/02 11:00:58 PM | Valid: [ 1/210] Step 020/023 Loss 3.405 Prec@(1,5) (0.1%, 1.8%)
08/02 11:01:03 PM | Valid: [ 1/210] Final Prec@1 0.1359%, Prec@5 1.9022%, Prec@10 3.2609%
08/02 11:01:04 PM | Final best Prec@1 = 0.1359%
08/02 11:01:04 PM | genotype = Genotype(normal=[[('dil_conv_3x3', 0), ('dil_conv_3x3', 1)], [('dil_conv_3x3', 0), ('sep_conv_3x3', 1)], [('partial_aware_1x1', 0), ('partial_aware_1x1', 3)], [('dil_conv_3x3', 0), ('partial_aware_1x1', 4)]], normal_concat=range(2, 6), reduce=[[('partial_aware_1x1', 1), ('partial_aware_1x1', 0)], [('partial_aware_1x1', 2), ('skip_connect', 1)], [('partial_aware_1x1', 3), ('partial_aware_1x1', 2)], [('sep_conv_3x3', 4), ('partial_aware_1x1', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1329, 0.1287, 0.1330, 0.1403, 0.1221, 0.1729, 0.1701],
        [0.1492, 0.1352, 0.1420, 0.1374, 0.1446, 0.1520, 0.1397]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1248, 0.1174, 0.1225, 0.1474, 0.1452, 0.1616, 0.1810],
        [0.1561, 0.1432, 0.1525, 0.1616, 0.1090, 0.1593, 0.1183],
        [0.1442, 0.1205, 0.1375, 0.1515, 0.1491, 0.1594, 0.1378]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1165, 0.1135, 0.1178, 0.1477, 0.1605, 0.1572, 0.1867],
        [0.1526, 0.1476, 0.1525, 0.1428, 0.1428, 0.1279, 0.1338],
        [0.1282, 0.1233, 0.1323, 0.1531, 0.1445, 0.1493, 0.1694],
        [0.1326, 0.1273, 0.1295, 0.1533, 0.1678, 0.1541, 0.1354]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1275, 0.1243, 0.1277, 0.1485, 0.1342, 0.1867, 0.1511],
        [0.1431, 0.1338, 0.1417, 0.1582, 0.1284, 0.1732, 0.1217],
        [0.1326, 0.1253, 0.1240, 0.1790, 0.1446, 0.1665, 0.1281],
        [0.1502, 0.1366, 0.1342, 0.1401, 0.1376, 0.1643, 0.1370],
        [0.1339, 0.1308, 0.1338, 0.1493, 0.1808, 0.1319, 0.1396]],
       grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1175, 0.1172, 0.1360, 0.1418, 0.1497, 0.1371, 0.2007],
        [0.1024, 0.0991, 0.1317, 0.1302, 0.1777, 0.1551, 0.2038]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1051, 0.1036, 0.1319, 0.1450, 0.1674, 0.1469, 0.2002],
        [0.1095, 0.1074, 0.1759, 0.1244, 0.1668, 0.1280, 0.1880],
        [0.1014, 0.0942, 0.0896, 0.1762, 0.1828, 0.1532, 0.2026]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1143, 0.1105, 0.1256, 0.1577, 0.1499, 0.1402, 0.2019],
        [0.1098, 0.1076, 0.1324, 0.1210, 0.1568, 0.1610, 0.2114],
        [0.1094, 0.1030, 0.1056, 0.1574, 0.1764, 0.1477, 0.2004],
        [0.0952, 0.0900, 0.0891, 0.1788, 0.1967, 0.1613, 0.1889]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1129, 0.1090, 0.1318, 0.1146, 0.1774, 0.1651, 0.1892],
        [0.0981, 0.0920, 0.1517, 0.1538, 0.1731, 0.1283, 0.2030],
        [0.1000, 0.0977, 0.0900, 0.1624, 0.1891, 0.1571, 0.2036],
        [0.0872, 0.0834, 0.0823, 0.1843, 0.1879, 0.1874, 0.1875],
        [0.0868, 0.0854, 0.0845, 0.1942, 0.1786, 0.1828, 0.1878]],
       grad_fn=<SoftmaxBackward>)
#####################
08/02 11:04:27 PM | Train: [ 2/210] Step 000/404 Loss 3.011 Prec@(1,5) (3.1%, 9.4%)
08/02 11:05:13 PM | Train: [ 2/210] Step 010/404 Loss 2.858 Prec@(1,5) (1.7%, 10.2%)
08/02 11:05:58 PM | Train: [ 2/210] Step 020/404 Loss 2.903 Prec@(1,5) (1.3%, 8.6%)
08/02 11:06:43 PM | Train: [ 2/210] Step 030/404 Loss 2.977 Prec@(1,5) (1.5%, 8.3%)
08/02 11:07:27 PM | Train: [ 2/210] Step 040/404 Loss 3.040 Prec@(1,5) (1.3%, 7.1%)
08/02 11:08:12 PM | Train: [ 2/210] Step 050/404 Loss 3.069 Prec@(1,5) (1.3%, 6.7%)
08/02 11:08:56 PM | Train: [ 2/210] Step 060/404 Loss 3.083 Prec@(1,5) (1.4%, 6.2%)
08/02 11:09:40 PM | Train: [ 2/210] Step 070/404 Loss 3.105 Prec@(1,5) (1.5%, 5.9%)
08/02 11:10:27 PM | Train: [ 2/210] Step 080/404 Loss 3.111 Prec@(1,5) (1.4%, 5.9%)
08/02 11:11:11 PM | Train: [ 2/210] Step 090/404 Loss 3.109 Prec@(1,5) (1.3%, 6.0%)
08/02 11:11:57 PM | Train: [ 2/210] Step 100/404 Loss 3.105 Prec@(1,5) (1.3%, 6.1%)
08/02 11:12:41 PM | Train: [ 2/210] Step 110/404 Loss 3.100 Prec@(1,5) (1.4%, 6.2%)
08/02 11:13:26 PM | Train: [ 2/210] Step 120/404 Loss 3.099 Prec@(1,5) (1.4%, 6.1%)
08/02 11:14:10 PM | Train: [ 2/210] Step 130/404 Loss 3.092 Prec@(1,5) (1.6%, 6.2%)
08/02 11:14:56 PM | Train: [ 2/210] Step 140/404 Loss 3.087 Prec@(1,5) (1.6%, 6.2%)
08/02 11:15:42 PM | Train: [ 2/210] Step 150/404 Loss 3.077 Prec@(1,5) (1.7%, 6.4%)
08/02 11:16:27 PM | Train: [ 2/210] Step 160/404 Loss 3.073 Prec@(1,5) (1.6%, 6.4%)
08/02 11:17:12 PM | Train: [ 2/210] Step 170/404 Loss 3.068 Prec@(1,5) (1.7%, 6.5%)
08/02 11:17:57 PM | Train: [ 2/210] Step 180/404 Loss 3.064 Prec@(1,5) (1.7%, 6.5%)
08/02 11:18:41 PM | Train: [ 2/210] Step 190/404 Loss 3.060 Prec@(1,5) (1.9%, 6.6%)
08/02 11:19:26 PM | Train: [ 2/210] Step 200/404 Loss 3.054 Prec@(1,5) (1.9%, 6.7%)
08/02 11:20:10 PM | Train: [ 2/210] Step 210/404 Loss 3.050 Prec@(1,5) (1.9%, 6.7%)
08/02 11:20:55 PM | Train: [ 2/210] Step 220/404 Loss 3.042 Prec@(1,5) (2.0%, 6.9%)
08/02 11:21:40 PM | Train: [ 2/210] Step 230/404 Loss 3.037 Prec@(1,5) (2.0%, 7.0%)
08/02 11:22:24 PM | Train: [ 2/210] Step 240/404 Loss 3.030 Prec@(1,5) (2.0%, 7.2%)
08/02 11:23:08 PM | Train: [ 2/210] Step 250/404 Loss 3.023 Prec@(1,5) (2.0%, 7.2%)
08/02 11:23:52 PM | Train: [ 2/210] Step 260/404 Loss 3.019 Prec@(1,5) (2.1%, 7.2%)
08/02 11:24:36 PM | Train: [ 2/210] Step 270/404 Loss 3.014 Prec@(1,5) (2.1%, 7.4%)
08/02 11:25:22 PM | Train: [ 2/210] Step 280/404 Loss 3.011 Prec@(1,5) (2.1%, 7.4%)
08/02 11:26:06 PM | Train: [ 2/210] Step 290/404 Loss 3.005 Prec@(1,5) (2.1%, 7.5%)
08/02 11:26:51 PM | Train: [ 2/210] Step 300/404 Loss 3.001 Prec@(1,5) (2.1%, 7.6%)
08/02 11:27:36 PM | Train: [ 2/210] Step 310/404 Loss 2.997 Prec@(1,5) (2.2%, 7.6%)
08/02 11:28:20 PM | Train: [ 2/210] Step 320/404 Loss 2.991 Prec@(1,5) (2.2%, 7.8%)
08/02 11:29:05 PM | Train: [ 2/210] Step 330/404 Loss 2.985 Prec@(1,5) (2.2%, 7.9%)
08/02 11:29:49 PM | Train: [ 2/210] Step 340/404 Loss 2.979 Prec@(1,5) (2.3%, 8.0%)
08/02 11:30:37 PM | Train: [ 2/210] Step 350/404 Loss 2.975 Prec@(1,5) (2.3%, 8.0%)
08/02 11:31:21 PM | Train: [ 2/210] Step 360/404 Loss 2.967 Prec@(1,5) (2.4%, 8.2%)
08/02 11:32:05 PM | Train: [ 2/210] Step 370/404 Loss 2.962 Prec@(1,5) (2.5%, 8.3%)
08/02 11:32:50 PM | Train: [ 2/210] Step 380/404 Loss 2.956 Prec@(1,5) (2.5%, 8.4%)
08/02 11:33:35 PM | Train: [ 2/210] Step 390/404 Loss 2.950 Prec@(1,5) (2.6%, 8.6%)
08/02 11:34:20 PM | Train: [ 2/210] Step 400/404 Loss 2.944 Prec@(1,5) (2.6%, 8.8%)
08/02 11:34:42 PM | Train: [ 2/210] Final Prec@1 2.6609%
08/02 11:36:35 PM | Valid: [ 2/210] Step 000/023 Loss 2.967 Prec@(1,5) (3.1%, 15.6%)
08/02 11:36:37 PM | Valid: [ 2/210] Step 010/023 Loss 2.885 Prec@(1,5) (3.4%, 12.2%)
08/02 11:36:41 PM | Valid: [ 2/210] Step 020/023 Loss 2.887 Prec@(1,5) (2.8%, 11.6%)
08/02 11:36:46 PM | Valid: [ 2/210] Final Prec@1 2.5815%, Prec@5 11.6848%, Prec@10 16.7120%
08/02 11:36:46 PM | Final best Prec@1 = 2.5815%
08/02 11:36:46 PM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('partial_aware_1x1', 0), ('max_pool_3x3', 1)], [('partial_aware_1x1', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('sep_conv_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('partial_aware_1x1', 2), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 3), ('max_pool_3x3', 0)], [('sep_conv_3x3', 4), ('max_pool_3x3', 0)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.2818, 0.1052, 0.1405, 0.1008, 0.1220, 0.0917, 0.1581],
        [0.2925, 0.1184, 0.1448, 0.0927, 0.1183, 0.1339, 0.0994]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1367, 0.0735, 0.0871, 0.1599, 0.3214, 0.1077, 0.1137],
        [0.3036, 0.1415, 0.1817, 0.1154, 0.0636, 0.0774, 0.1168],
        [0.2349, 0.0959, 0.1164, 0.1827, 0.0990, 0.1486, 0.1224]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1980, 0.1079, 0.1302, 0.1312, 0.1220, 0.1458, 0.1649],
        [0.2233, 0.1413, 0.1646, 0.0720, 0.1442, 0.1252, 0.1293],
        [0.2289, 0.1048, 0.1301, 0.1427, 0.1195, 0.1330, 0.1410],
        [0.2443, 0.1065, 0.0949, 0.0774, 0.2512, 0.1155, 0.1101]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.2138, 0.1297, 0.1546, 0.1197, 0.1068, 0.1299, 0.1455],
        [0.1640, 0.1314, 0.1494, 0.1510, 0.1376, 0.1403, 0.1263],
        [0.1885, 0.1154, 0.1265, 0.1356, 0.1502, 0.1240, 0.1598],
        [0.1730, 0.1180, 0.0993, 0.2304, 0.1497, 0.1121, 0.1174],
        [0.3095, 0.1347, 0.1100, 0.1065, 0.1153, 0.0930, 0.1310]],
       grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1626, 0.0535, 0.1151, 0.1420, 0.0974, 0.0590, 0.3704],
        [0.1247, 0.0477, 0.1190, 0.0813, 0.0687, 0.0629, 0.4956]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1397, 0.0527, 0.0948, 0.1437, 0.0816, 0.0904, 0.3971],
        [0.0853, 0.0433, 0.0648, 0.0919, 0.1101, 0.0782, 0.5264],
        [0.0739, 0.0336, 0.0400, 0.1103, 0.1230, 0.1141, 0.5050]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1652, 0.0487, 0.0939, 0.1564, 0.0855, 0.0734, 0.3768],
        [0.0795, 0.0396, 0.0859, 0.0621, 0.0680, 0.0781, 0.5868],
        [0.1047, 0.0411, 0.0474, 0.1017, 0.0921, 0.0933, 0.5197],
        [0.0680, 0.0349, 0.0380, 0.1541, 0.1117, 0.1261, 0.4672]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1871, 0.0522, 0.0911, 0.0792, 0.0716, 0.0902, 0.4285],
        [0.1127, 0.0421, 0.0718, 0.0763, 0.0625, 0.0607, 0.5740],
        [0.1207, 0.0437, 0.0477, 0.1146, 0.0706, 0.0818, 0.5209],
        [0.0762, 0.0360, 0.0393, 0.1455, 0.0881, 0.0785, 0.5364],
        [0.0666, 0.0375, 0.0363, 0.2221, 0.1064, 0.0985, 0.4326]],
       grad_fn=<SoftmaxBackward>)
#####################
08/02 11:40:25 PM | Train: [ 3/210] Step 000/404 Loss 2.626 Prec@(1,5) (0.0%, 18.8%)
08/02 11:41:09 PM | Train: [ 3/210] Step 010/404 Loss 2.638 Prec@(1,5) (3.4%, 14.8%)
08/02 11:41:53 PM | Train: [ 3/210] Step 020/404 Loss 2.720 Prec@(1,5) (3.6%, 13.2%)
08/02 11:42:36 PM | Train: [ 3/210] Step 030/404 Loss 2.703 Prec@(1,5) (4.4%, 14.0%)
08/02 11:43:19 PM | Train: [ 3/210] Step 040/404 Loss 2.704 Prec@(1,5) (4.7%, 13.6%)
08/02 11:44:02 PM | Train: [ 3/210] Step 050/404 Loss 2.704 Prec@(1,5) (4.7%, 13.7%)
08/02 11:44:46 PM | Train: [ 3/210] Step 060/404 Loss 2.695 Prec@(1,5) (4.9%, 14.1%)
08/02 11:45:33 PM | Train: [ 3/210] Step 070/404 Loss 2.705 Prec@(1,5) (4.8%, 14.2%)
08/02 11:46:16 PM | Train: [ 3/210] Step 080/404 Loss 2.705 Prec@(1,5) (4.8%, 14.2%)
08/02 11:47:00 PM | Train: [ 3/210] Step 090/404 Loss 2.703 Prec@(1,5) (4.8%, 14.1%)
08/02 11:47:44 PM | Train: [ 3/210] Step 100/404 Loss 2.700 Prec@(1,5) (4.6%, 14.2%)
08/02 11:48:29 PM | Train: [ 3/210] Step 110/404 Loss 2.698 Prec@(1,5) (4.7%, 14.0%)
08/02 11:49:13 PM | Train: [ 3/210] Step 120/404 Loss 2.690 Prec@(1,5) (4.9%, 14.4%)
08/02 11:49:56 PM | Train: [ 3/210] Step 130/404 Loss 2.690 Prec@(1,5) (4.8%, 14.4%)
08/02 11:50:43 PM | Train: [ 3/210] Step 140/404 Loss 2.685 Prec@(1,5) (4.7%, 14.4%)
08/02 11:51:27 PM | Train: [ 3/210] Step 150/404 Loss 2.677 Prec@(1,5) (4.9%, 14.7%)
08/02 11:52:11 PM | Train: [ 3/210] Step 160/404 Loss 2.674 Prec@(1,5) (4.9%, 14.6%)
08/02 11:52:56 PM | Train: [ 3/210] Step 170/404 Loss 2.671 Prec@(1,5) (5.0%, 14.8%)
08/02 11:53:41 PM | Train: [ 3/210] Step 180/404 Loss 2.667 Prec@(1,5) (5.0%, 14.8%)
08/02 11:54:26 PM | Train: [ 3/210] Step 190/404 Loss 2.662 Prec@(1,5) (5.1%, 15.2%)
08/02 11:55:12 PM | Train: [ 3/210] Step 200/404 Loss 2.658 Prec@(1,5) (5.1%, 15.1%)
08/02 11:55:59 PM | Train: [ 3/210] Step 210/404 Loss 2.656 Prec@(1,5) (5.0%, 15.0%)
08/02 11:56:45 PM | Train: [ 3/210] Step 220/404 Loss 2.648 Prec@(1,5) (5.1%, 15.2%)
08/02 11:57:29 PM | Train: [ 3/210] Step 230/404 Loss 2.646 Prec@(1,5) (5.2%, 15.3%)
08/02 11:58:13 PM | Train: [ 3/210] Step 240/404 Loss 2.641 Prec@(1,5) (5.2%, 15.4%)
08/02 11:58:57 PM | Train: [ 3/210] Step 250/404 Loss 2.635 Prec@(1,5) (5.3%, 15.5%)
08/02 11:59:43 PM | Train: [ 3/210] Step 260/404 Loss 2.631 Prec@(1,5) (5.4%, 15.5%)
08/03 12:00:31 AM | Train: [ 3/210] Step 270/404 Loss 2.626 Prec@(1,5) (5.5%, 15.8%)
08/03 12:01:15 AM | Train: [ 3/210] Step 280/404 Loss 2.625 Prec@(1,5) (5.6%, 15.9%)
08/03 12:01:59 AM | Train: [ 3/210] Step 290/404 Loss 2.621 Prec@(1,5) (5.7%, 16.1%)
08/03 12:02:42 AM | Train: [ 3/210] Step 300/404 Loss 2.616 Prec@(1,5) (5.7%, 16.2%)
08/03 12:03:26 AM | Train: [ 3/210] Step 310/404 Loss 2.616 Prec@(1,5) (5.7%, 16.3%)
08/03 12:04:11 AM | Train: [ 3/210] Step 320/404 Loss 2.613 Prec@(1,5) (5.7%, 16.4%)
08/03 12:04:55 AM | Train: [ 3/210] Step 330/404 Loss 2.609 Prec@(1,5) (5.8%, 16.5%)
08/03 12:05:39 AM | Train: [ 3/210] Step 340/404 Loss 2.604 Prec@(1,5) (5.8%, 16.7%)
08/03 12:06:23 AM | Train: [ 3/210] Step 350/404 Loss 2.600 Prec@(1,5) (5.9%, 16.9%)
08/03 12:07:08 AM | Train: [ 3/210] Step 360/404 Loss 2.594 Prec@(1,5) (6.0%, 17.1%)
08/03 12:09:46 AM | Logger is set 
08/03 12:09:46 AM | Logger with distribution
08/03 12:10:39 AM | Initializing dataset used 53.46789073944092 basic time unit
08/03 12:10:39 AM | Initializing dataset used 53.47087597846985 basic time unit
08/03 12:10:39 AM | Initializing dataset used 52.89311170578003 basic time unit
08/03 12:10:39 AM | Initializing dataset used 53.47082734107971 basic time unit
08/03 12:10:39 AM | Initializing dataset used 53.47080326080322 basic time unit
08/03 12:10:39 AM | Initializing dataset used 53.46115827560425 basic time unit
08/03 12:10:39 AM | Initializing dataset used 53.45458793640137 basic time unit
08/03 12:10:39 AM | Initializing dataset used 53.470884799957275 basic time unit
08/03 12:10:39 AM | The training classes labels length :  751
08/03 12:10:39 AM | The training classes labels length :  751
08/03 12:10:39 AM | The training classes labels length :  751
08/03 12:10:39 AM | The training classes labels length :  751
08/03 12:10:39 AM | The training classes labels length :  751
08/03 12:10:39 AM | The training classes labels length :  751
08/03 12:10:39 AM | The training classes labels length :  751
08/03 12:10:39 AM | The training classes labels length :  751
08/03 12:12:53 AM | batch loading time example is 134.5307514667511
08/03 12:12:53 AM | batch loading time example is 134.53134560585022
08/03 12:12:53 AM | batch loading time example is 134.53295612335205
08/03 12:12:53 AM | batch loading time example is 134.5334153175354
08/03 12:12:53 AM | batch loading time example is 134.5314552783966
08/03 12:12:53 AM | batch loading time example is 134.53439664840698
08/03 12:12:53 AM | batch loading time example is 134.53451013565063
08/03 12:12:53 AM | batch loading time example is 134.5361659526825
####### ALPHA #######
# Alpha - normal
tensor([[0.1428, 0.1428, 0.1430, 0.1426, 0.1429, 0.1430, 0.1428],
        [0.1429, 0.1428, 0.1429, 0.1430, 0.1429, 0.1429, 0.1427]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1430, 0.1428, 0.1429, 0.1427, 0.1426, 0.1428, 0.1431],
        [0.1430, 0.1429, 0.1430, 0.1431, 0.1426, 0.1427, 0.1428],
        [0.1427, 0.1428, 0.1431, 0.1428, 0.1428, 0.1429, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1428, 0.1428, 0.1429, 0.1428, 0.1428, 0.1428, 0.1431],
        [0.1431, 0.1428, 0.1428, 0.1428, 0.1427, 0.1429, 0.1429],
        [0.1428, 0.1427, 0.1428, 0.1428, 0.1429, 0.1429, 0.1431],
        [0.1427, 0.1429, 0.1427, 0.1430, 0.1428, 0.1429, 0.1429]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1428, 0.1429, 0.1427, 0.1429, 0.1429, 0.1428, 0.1429],
        [0.1426, 0.1427, 0.1431, 0.1430, 0.1428, 0.1430, 0.1428],
        [0.1428, 0.1427, 0.1432, 0.1430, 0.1427, 0.1428, 0.1427],
        [0.1429, 0.1427, 0.1429, 0.1428, 0.1428, 0.1431, 0.1429],
        [0.1429, 0.1426, 0.1429, 0.1429, 0.1428, 0.1430, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1428, 0.1428, 0.1429, 0.1428, 0.1429, 0.1426, 0.1432],
        [0.1428, 0.1427, 0.1430, 0.1429, 0.1431, 0.1429, 0.1427]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1430, 0.1429, 0.1429, 0.1428, 0.1428, 0.1429, 0.1428],
        [0.1430, 0.1428, 0.1429, 0.1428, 0.1430, 0.1428, 0.1428],
        [0.1431, 0.1427, 0.1429, 0.1427, 0.1428, 0.1429, 0.1429]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1429, 0.1427, 0.1427, 0.1429, 0.1426, 0.1430, 0.1431],
        [0.1430, 0.1430, 0.1431, 0.1426, 0.1427, 0.1427, 0.1430],
        [0.1431, 0.1428, 0.1426, 0.1429, 0.1429, 0.1430, 0.1427],
        [0.1426, 0.1430, 0.1428, 0.1426, 0.1432, 0.1428, 0.1430]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1428, 0.1427, 0.1427, 0.1431, 0.1429, 0.1428, 0.1429],
        [0.1432, 0.1430, 0.1427, 0.1428, 0.1427, 0.1430, 0.1427],
        [0.1426, 0.1428, 0.1430, 0.1427, 0.1429, 0.1429, 0.1430],
        [0.1427, 0.1430, 0.1430, 0.1427, 0.1427, 0.1428, 0.1430],
        [0.1429, 0.1429, 0.1428, 0.1428, 0.1428, 0.1429, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
08/03 12:16:59 AM | Train: [ 1/210] Step 000/404 Loss 3.621 Prec@(1,5) (0.0%, 0.0%)
08/03 12:17:44 AM | Train: [ 1/210] Step 010/404 Loss 3.825 Prec@(1,5) (0.0%, 1.7%)
08/03 12:18:29 AM | Train: [ 1/210] Step 020/404 Loss 3.863 Prec@(1,5) (0.0%, 1.2%)
08/03 12:18:54 AM | Train: [ 1/210] Final Prec@1 0.0000%
08/03 12:20:42 AM | Valid: [ 1/210] Step 000/023 Loss 6.792 Prec@(1,5) (0.0%, 0.0%)
08/03 12:20:45 AM | Valid: [ 1/210] Step 010/023 Loss 6.503 Prec@(1,5) (0.0%, 0.3%)
08/03 12:20:48 AM | Valid: [ 1/210] Step 020/023 Loss 6.521 Prec@(1,5) (0.0%, 0.4%)
08/03 12:20:52 AM | Valid: [ 1/210] Final Prec@1 0.1359%, Prec@5 0.5435%, Prec@10 1.3587%
08/03 12:20:53 AM | Final best Prec@1 = 0.1359%
08/03 12:20:53 AM | genotype = Genotype(normal=[[('partial_aware_1x1', 1), ('dil_conv_3x3', 0)], [('dil_conv_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('dil_conv_3x3', 3)], [('partial_aware_1x1', 4), ('skip_connect', 2)]], normal_concat=range(2, 6), reduce=[[('partial_aware_1x1', 0), ('skip_connect', 1)], [('partial_aware_1x1', 2), ('partial_aware_1x1', 0)], [('partial_aware_1x1', 3), ('partial_aware_1x1', 2)], [('dil_conv_3x3', 4), ('partial_aware_1x1', 3)]], reduce_concat=range(2, 6))
