08/02 04:16:32 PM | Logger is set 
08/02 04:16:32 PM | Logger with distribution
08/02 04:17:32 PM | Initializing dataset used 61.19995927810669 basic time unit
08/02 04:17:32 PM | Initializing dataset used 61.17988443374634 basic time unit
08/02 04:17:32 PM | Initializing dataset used 61.19323945045471 basic time unit
08/02 04:17:32 PM | Initializing dataset used 61.20660996437073 basic time unit
08/02 04:17:32 PM | Initializing dataset used 61.186615228652954 basic time unit
08/02 04:17:32 PM | Initializing dataset used 61.20878505706787 basic time unit
08/02 04:17:32 PM | Initializing dataset used 60.5251247882843 basic time unit
08/02 04:17:32 PM | Initializing dataset used 61.20933198928833 basic time unit
08/02 04:17:32 PM | The training classes labels length :  751
08/02 04:17:32 PM | The training classes labels length :  751
08/02 04:17:32 PM | The training classes labels length :  751
08/02 04:17:32 PM | The training classes labels length :  751
08/02 04:17:32 PM | The training classes labels length :  751
08/02 04:17:32 PM | The training classes labels length :  751
08/02 04:17:32 PM | The training classes labels length :  751
08/02 04:17:32 PM | The training classes labels length :  751
08/02 04:19:40 PM | batch loading time example is 127.09595513343811
08/02 04:19:40 PM | batch loading time example is 127.09588956832886
08/02 04:19:40 PM | batch loading time example is 127.09619379043579
08/02 04:19:40 PM | batch loading time example is 127.09419894218445
08/02 04:19:40 PM | batch loading time example is 127.09611105918884
08/02 04:19:40 PM | batch loading time example is 127.0966534614563
08/02 04:19:40 PM | batch loading time example is 127.09739661216736
08/02 04:19:40 PM | batch loading time example is 127.09832406044006
####### ALPHA #######
# Alpha - normal
tensor([[0.1428, 0.1428, 0.1430, 0.1426, 0.1429, 0.1430, 0.1428],
        [0.1429, 0.1428, 0.1429, 0.1430, 0.1429, 0.1429, 0.1427]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1430, 0.1428, 0.1429, 0.1427, 0.1426, 0.1428, 0.1431],
        [0.1430, 0.1429, 0.1430, 0.1431, 0.1426, 0.1427, 0.1428],
        [0.1427, 0.1428, 0.1431, 0.1428, 0.1428, 0.1429, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1428, 0.1428, 0.1429, 0.1428, 0.1428, 0.1428, 0.1431],
        [0.1431, 0.1428, 0.1428, 0.1428, 0.1427, 0.1429, 0.1429],
        [0.1428, 0.1427, 0.1428, 0.1428, 0.1429, 0.1429, 0.1431],
        [0.1427, 0.1429, 0.1427, 0.1430, 0.1428, 0.1429, 0.1429]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1428, 0.1429, 0.1427, 0.1429, 0.1429, 0.1428, 0.1429],
        [0.1426, 0.1427, 0.1431, 0.1430, 0.1428, 0.1430, 0.1428],
        [0.1428, 0.1427, 0.1432, 0.1430, 0.1427, 0.1428, 0.1427],
        [0.1429, 0.1427, 0.1429, 0.1428, 0.1428, 0.1431, 0.1429],
        [0.1429, 0.1426, 0.1429, 0.1429, 0.1428, 0.1430, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1428, 0.1428, 0.1429, 0.1428, 0.1429, 0.1426, 0.1432],
        [0.1428, 0.1427, 0.1430, 0.1429, 0.1431, 0.1429, 0.1427]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1430, 0.1429, 0.1429, 0.1428, 0.1428, 0.1429, 0.1428],
        [0.1430, 0.1428, 0.1429, 0.1428, 0.1430, 0.1428, 0.1428],
        [0.1431, 0.1427, 0.1429, 0.1427, 0.1428, 0.1429, 0.1429]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1429, 0.1427, 0.1427, 0.1429, 0.1426, 0.1430, 0.1431],
        [0.1430, 0.1430, 0.1431, 0.1426, 0.1427, 0.1427, 0.1430],
        [0.1431, 0.1428, 0.1426, 0.1429, 0.1429, 0.1430, 0.1427],
        [0.1426, 0.1430, 0.1428, 0.1426, 0.1432, 0.1428, 0.1430]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1428, 0.1427, 0.1427, 0.1431, 0.1429, 0.1428, 0.1429],
        [0.1432, 0.1430, 0.1427, 0.1428, 0.1427, 0.1430, 0.1427],
        [0.1426, 0.1428, 0.1430, 0.1427, 0.1429, 0.1429, 0.1430],
        [0.1427, 0.1430, 0.1430, 0.1427, 0.1427, 0.1428, 0.1430],
        [0.1429, 0.1429, 0.1428, 0.1428, 0.1428, 0.1429, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
08/02 04:54:52 PM | Logger is set 
08/02 04:54:52 PM | Logger with distribution
08/02 04:55:45 PM | Initializing dataset used 53.21863651275635 basic time unit
08/02 04:55:45 PM | Initializing dataset used 53.2119574546814 basic time unit
08/02 04:55:45 PM | Initializing dataset used 53.222365379333496 basic time unit
08/02 04:55:45 PM | Initializing dataset used 53.19857335090637 basic time unit
08/02 04:55:45 PM | Initializing dataset used 53.1917507648468 basic time unit
08/02 04:55:45 PM | Initializing dataset used 53.222416639328 basic time unit
08/02 04:55:45 PM | Initializing dataset used 52.647337913513184 basic time unit
08/02 04:55:45 PM | Initializing dataset used 53.205199241638184 basic time unit
08/02 04:55:45 PM | The training classes labels length :  751
08/02 04:55:45 PM | The training classes labels length :  751
08/02 04:55:45 PM | The training classes labels length :  751
08/02 04:55:45 PM | The training classes labels length :  751
08/02 04:55:45 PM | The training classes labels length :  751
08/02 04:55:45 PM | The training classes labels length :  751
08/02 04:55:45 PM | The training classes labels length :  751
08/02 04:55:45 PM | The training classes labels length :  751
08/02 04:57:41 PM | batch loading time example is 116.11153721809387
08/02 04:57:41 PM | batch loading time example is 116.11153960227966
08/02 04:57:41 PM | batch loading time example is 116.11188125610352
08/02 04:57:41 PM | batch loading time example is 116.11163640022278
08/02 04:57:41 PM | batch loading time example is 116.11214756965637
08/02 04:57:41 PM | batch loading time example is 116.11197638511658
08/02 04:57:41 PM | batch loading time example is 116.11309933662415
08/02 04:57:41 PM | batch loading time example is 116.11178088188171
####### ALPHA #######
# Alpha - normal
tensor([[0.1428, 0.1428, 0.1430, 0.1426, 0.1429, 0.1430, 0.1428],
        [0.1429, 0.1428, 0.1429, 0.1430, 0.1429, 0.1429, 0.1427]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1430, 0.1428, 0.1429, 0.1427, 0.1426, 0.1428, 0.1431],
        [0.1430, 0.1429, 0.1430, 0.1431, 0.1426, 0.1427, 0.1428],
        [0.1427, 0.1428, 0.1431, 0.1428, 0.1428, 0.1429, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1428, 0.1428, 0.1429, 0.1428, 0.1428, 0.1428, 0.1431],
        [0.1431, 0.1428, 0.1428, 0.1428, 0.1427, 0.1429, 0.1429],
        [0.1428, 0.1427, 0.1428, 0.1428, 0.1429, 0.1429, 0.1431],
        [0.1427, 0.1429, 0.1427, 0.1430, 0.1428, 0.1429, 0.1429]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1428, 0.1429, 0.1427, 0.1429, 0.1429, 0.1428, 0.1429],
        [0.1426, 0.1427, 0.1431, 0.1430, 0.1428, 0.1430, 0.1428],
        [0.1428, 0.1427, 0.1432, 0.1430, 0.1427, 0.1428, 0.1427],
        [0.1429, 0.1427, 0.1429, 0.1428, 0.1428, 0.1431, 0.1429],
        [0.1429, 0.1426, 0.1429, 0.1429, 0.1428, 0.1430, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1428, 0.1428, 0.1429, 0.1428, 0.1429, 0.1426, 0.1432],
        [0.1428, 0.1427, 0.1430, 0.1429, 0.1431, 0.1429, 0.1427]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1430, 0.1429, 0.1429, 0.1428, 0.1428, 0.1429, 0.1428],
        [0.1430, 0.1428, 0.1429, 0.1428, 0.1430, 0.1428, 0.1428],
        [0.1431, 0.1427, 0.1429, 0.1427, 0.1428, 0.1429, 0.1429]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1429, 0.1427, 0.1427, 0.1429, 0.1426, 0.1430, 0.1431],
        [0.1430, 0.1430, 0.1431, 0.1426, 0.1427, 0.1427, 0.1430],
        [0.1431, 0.1428, 0.1426, 0.1429, 0.1429, 0.1430, 0.1427],
        [0.1426, 0.1430, 0.1428, 0.1426, 0.1432, 0.1428, 0.1430]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1428, 0.1427, 0.1427, 0.1431, 0.1429, 0.1428, 0.1429],
        [0.1432, 0.1430, 0.1427, 0.1428, 0.1427, 0.1430, 0.1427],
        [0.1426, 0.1428, 0.1430, 0.1427, 0.1429, 0.1429, 0.1430],
        [0.1427, 0.1430, 0.1430, 0.1427, 0.1427, 0.1428, 0.1430],
        [0.1429, 0.1429, 0.1428, 0.1428, 0.1428, 0.1429, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
08/02 05:01:31 PM | Train: [ 1/210] Step 000/404 Loss 6.626 Prec@(1,5) (0.0%, 0.0%)
08/02 05:02:17 PM | Train: [ 1/210] Step 010/404 Loss 6.590 Prec@(1,5) (0.9%, 2.3%)
08/02 05:03:03 PM | Train: [ 1/210] Step 020/404 Loss 6.636 Prec@(1,5) (0.4%, 2.7%)
08/02 05:03:31 PM | Train: [ 1/210] Final Prec@1 0.5208%
08/02 05:05:26 PM | Valid: [ 1/210] Step 000/023 Loss 11.298 Prec@(1,5) (3.1%, 6.2%)
08/02 05:05:28 PM | Valid: [ 1/210] Step 010/023 Loss 13.164 Prec@(1,5) (0.3%, 1.4%)
08/02 05:05:31 PM | Valid: [ 1/210] Step 020/023 Loss 13.330 Prec@(1,5) (0.1%, 1.6%)
08/02 05:05:44 PM | Valid: [ 1/210] Final Prec@1 0.1359%, Prec@5 1.9022%, Prec@10 2.8533%
08/02 05:05:44 PM | Final best Prec@1 = 0.1359%
08/02 05:05:44 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('max_pool_3x3', 1)], [('sep_conv_3x3', 1), ('max_pool_3x3', 2)], [('dil_conv_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 1), ('partial_aware_1x1', 0)], [('partial_aware_1x1', 2), ('partial_aware_1x1', 0)], [('partial_aware_1x1', 3), ('partial_aware_1x1', 2)], [('dil_conv_3x3', 4), ('partial_aware_1x1', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1367, 0.1249, 0.1257, 0.1628, 0.1552, 0.1231, 0.1716],
        [0.1550, 0.1420, 0.1476, 0.1437, 0.1497, 0.1426, 0.1192]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1474, 0.1256, 0.1297, 0.1324, 0.1400, 0.1431, 0.1819],
        [0.1482, 0.1283, 0.1364, 0.1677, 0.1491, 0.1300, 0.1404],
        [0.1621, 0.1400, 0.1358, 0.1378, 0.1376, 0.1393, 0.1476]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1213, 0.1165, 0.1185, 0.1525, 0.1371, 0.1847, 0.1695],
        [0.1660, 0.1506, 0.1612, 0.1362, 0.1126, 0.1488, 0.1246],
        [0.1620, 0.1530, 0.1529, 0.1292, 0.1436, 0.1194, 0.1399],
        [0.1454, 0.1358, 0.1421, 0.1543, 0.1454, 0.1419, 0.1352]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1695, 0.1561, 0.1602, 0.1252, 0.1193, 0.1275, 0.1422],
        [0.1458, 0.1338, 0.1393, 0.1383, 0.1458, 0.1537, 0.1434],
        [0.1748, 0.1663, 0.1500, 0.1346, 0.1249, 0.1186, 0.1308],
        [0.1700, 0.1585, 0.1505, 0.1279, 0.1376, 0.1258, 0.1297],
        [0.1670, 0.1588, 0.1554, 0.1162, 0.1521, 0.1264, 0.1241]],
       grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.0981, 0.0952, 0.1489, 0.1662, 0.1776, 0.1296, 0.1845],
        [0.0962, 0.0931, 0.1408, 0.1841, 0.1473, 0.1698, 0.1688]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1006, 0.0995, 0.1639, 0.1413, 0.1723, 0.1182, 0.2043],
        [0.1041, 0.1024, 0.1648, 0.1520, 0.1624, 0.1364, 0.1780],
        [0.1067, 0.1017, 0.0980, 0.1678, 0.1795, 0.1572, 0.1891]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1057, 0.1051, 0.1492, 0.1538, 0.1666, 0.1295, 0.1901],
        [0.1101, 0.1081, 0.1311, 0.1606, 0.1642, 0.1380, 0.1878],
        [0.1062, 0.1016, 0.1101, 0.1557, 0.1798, 0.1594, 0.1870],
        [0.0933, 0.0895, 0.0912, 0.1802, 0.1956, 0.1650, 0.1852]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1161, 0.1142, 0.1528, 0.1347, 0.1623, 0.1368, 0.1832],
        [0.1096, 0.1083, 0.1548, 0.1349, 0.1655, 0.1373, 0.1895],
        [0.1094, 0.1076, 0.1062, 0.1598, 0.1713, 0.1606, 0.1849],
        [0.0930, 0.0920, 0.0932, 0.1720, 0.1859, 0.1793, 0.1844],
        [0.0974, 0.0947, 0.0984, 0.1687, 0.1789, 0.1823, 0.1794]],
       grad_fn=<SoftmaxBackward>)
#####################
08/02 05:09:37 PM | Train: [ 2/210] Step 000/404 Loss 5.698 Prec@(1,5) (6.2%, 12.5%)
08/02 05:10:23 PM | Train: [ 2/210] Step 010/404 Loss 5.878 Prec@(1,5) (3.1%, 9.4%)
08/02 05:11:07 PM | Train: [ 2/210] Step 020/404 Loss 5.832 Prec@(1,5) (2.4%, 8.0%)
08/02 05:11:51 PM | Train: [ 2/210] Step 030/404 Loss 5.992 Prec@(1,5) (1.8%, 7.5%)
08/02 05:12:37 PM | Train: [ 2/210] Step 040/404 Loss 6.119 Prec@(1,5) (1.7%, 6.4%)
08/02 05:13:20 PM | Train: [ 2/210] Step 050/404 Loss 6.169 Prec@(1,5) (1.5%, 6.1%)
08/02 05:14:08 PM | Train: [ 2/210] Step 060/404 Loss 6.194 Prec@(1,5) (1.5%, 5.8%)
08/02 05:14:51 PM | Train: [ 2/210] Step 070/404 Loss 6.220 Prec@(1,5) (1.4%, 5.7%)
08/02 05:15:37 PM | Train: [ 2/210] Step 080/404 Loss 6.224 Prec@(1,5) (1.4%, 5.7%)
08/02 05:16:22 PM | Train: [ 2/210] Step 090/404 Loss 6.218 Prec@(1,5) (1.6%, 5.8%)
08/02 05:17:07 PM | Train: [ 2/210] Step 100/404 Loss 6.205 Prec@(1,5) (1.7%, 5.9%)
08/02 05:17:49 PM | Train: [ 2/210] Step 110/404 Loss 6.196 Prec@(1,5) (1.6%, 5.9%)
08/02 05:18:32 PM | Train: [ 2/210] Step 120/404 Loss 6.180 Prec@(1,5) (1.6%, 6.0%)
08/02 05:19:14 PM | Train: [ 2/210] Step 130/404 Loss 6.166 Prec@(1,5) (1.8%, 6.1%)
08/02 05:19:58 PM | Train: [ 2/210] Step 140/404 Loss 6.156 Prec@(1,5) (1.8%, 6.1%)
08/02 05:20:45 PM | Train: [ 2/210] Step 150/404 Loss 6.135 Prec@(1,5) (1.8%, 6.2%)
08/02 05:21:28 PM | Train: [ 2/210] Step 160/404 Loss 6.123 Prec@(1,5) (1.8%, 6.3%)
08/02 05:22:10 PM | Train: [ 2/210] Step 170/404 Loss 6.109 Prec@(1,5) (1.9%, 6.5%)
08/02 05:22:54 PM | Train: [ 2/210] Step 180/404 Loss 6.101 Prec@(1,5) (2.0%, 6.5%)
08/02 05:23:38 PM | Train: [ 2/210] Step 190/404 Loss 6.091 Prec@(1,5) (2.0%, 6.5%)
08/02 05:24:22 PM | Train: [ 2/210] Step 200/404 Loss 6.078 Prec@(1,5) (2.0%, 6.6%)
08/02 05:25:09 PM | Train: [ 2/210] Step 210/404 Loss 6.069 Prec@(1,5) (2.0%, 6.6%)
08/02 05:25:54 PM | Train: [ 2/210] Step 220/404 Loss 6.055 Prec@(1,5) (2.0%, 6.7%)
08/02 05:26:41 PM | Train: [ 2/210] Step 230/404 Loss 6.042 Prec@(1,5) (2.0%, 6.8%)
08/02 05:27:28 PM | Train: [ 2/210] Step 240/404 Loss 6.028 Prec@(1,5) (2.0%, 7.0%)
08/02 05:28:10 PM | Train: [ 2/210] Step 250/404 Loss 6.016 Prec@(1,5) (2.1%, 7.1%)
08/02 05:28:59 PM | Train: [ 2/210] Step 260/404 Loss 6.008 Prec@(1,5) (2.1%, 7.3%)
08/02 05:29:42 PM | Train: [ 2/210] Step 270/404 Loss 5.998 Prec@(1,5) (2.1%, 7.4%)
08/02 05:30:35 PM | Train: [ 2/210] Step 280/404 Loss 5.990 Prec@(1,5) (2.2%, 7.5%)
08/02 05:31:21 PM | Train: [ 2/210] Step 290/404 Loss 5.979 Prec@(1,5) (2.2%, 7.5%)
08/02 05:32:04 PM | Train: [ 2/210] Step 300/404 Loss 5.965 Prec@(1,5) (2.2%, 7.6%)
08/02 05:32:50 PM | Train: [ 2/210] Step 310/404 Loss 5.959 Prec@(1,5) (2.2%, 7.6%)
08/02 05:33:34 PM | Train: [ 2/210] Step 320/404 Loss 5.947 Prec@(1,5) (2.3%, 7.8%)
08/02 05:34:18 PM | Train: [ 2/210] Step 330/404 Loss 5.937 Prec@(1,5) (2.3%, 7.9%)
08/02 05:35:04 PM | Train: [ 2/210] Step 340/404 Loss 5.924 Prec@(1,5) (2.3%, 8.1%)
08/02 05:35:49 PM | Train: [ 2/210] Step 350/404 Loss 5.918 Prec@(1,5) (2.3%, 8.2%)
08/02 05:36:34 PM | Train: [ 2/210] Step 360/404 Loss 5.904 Prec@(1,5) (2.4%, 8.4%)
08/02 05:37:18 PM | Train: [ 2/210] Step 370/404 Loss 5.895 Prec@(1,5) (2.5%, 8.6%)
08/02 05:38:03 PM | Train: [ 2/210] Step 380/404 Loss 5.885 Prec@(1,5) (2.5%, 8.7%)
08/02 05:38:48 PM | Train: [ 2/210] Step 390/404 Loss 5.871 Prec@(1,5) (2.6%, 8.8%)
08/02 05:39:32 PM | Train: [ 2/210] Step 400/404 Loss 5.861 Prec@(1,5) (2.6%, 8.9%)
08/02 06:11:39 PM | Logger is set 
08/02 06:11:39 PM | Logger with distribution
08/02 06:12:33 PM | Initializing dataset used 54.719242095947266 basic time unit
08/02 06:12:33 PM | Initializing dataset used 53.613872051239014 basic time unit
08/02 06:12:33 PM | Initializing dataset used 54.688536405563354 basic time unit
08/02 06:12:33 PM | Initializing dataset used 56.159977197647095 basic time unit
08/02 06:12:33 PM | Initializing dataset used 56.15936779975891 basic time unit
08/02 06:12:33 PM | Initializing dataset used 54.678624629974365 basic time unit
08/02 06:12:33 PM | Initializing dataset used 54.71927237510681 basic time unit
08/02 06:12:33 PM | Initializing dataset used 54.70356297492981 basic time unit
08/02 06:12:33 PM | The training classes labels length :  751
08/02 06:12:33 PM | The training classes labels length :  751
08/02 06:12:33 PM | The training classes labels length :  751
08/02 06:12:33 PM | The training classes labels length :  751
08/02 06:12:33 PM | The training classes labels length :  751
08/02 06:12:33 PM | The training classes labels length :  751
08/02 06:12:33 PM | The training classes labels length :  751
08/02 06:12:33 PM | The training classes labels length :  751
08/02 06:14:30 PM | batch loading time example is 117.6997663974762
08/02 06:14:30 PM | batch loading time example is 117.69980263710022
08/02 06:14:30 PM | batch loading time example is 117.69681191444397
08/02 06:14:30 PM | batch loading time example is 117.70052146911621
08/02 06:14:30 PM | batch loading time example is 117.70045948028564
08/02 06:14:30 PM | batch loading time example is 117.70059013366699
08/02 06:14:30 PM | batch loading time example is 117.70062279701233
08/02 06:14:30 PM | batch loading time example is 117.70184540748596
####### ALPHA #######
# Alpha - normal
tensor([[0.1428, 0.1428, 0.1430, 0.1426, 0.1429, 0.1430, 0.1428],
        [0.1429, 0.1428, 0.1429, 0.1430, 0.1429, 0.1429, 0.1427]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1430, 0.1428, 0.1429, 0.1427, 0.1426, 0.1428, 0.1431],
        [0.1430, 0.1429, 0.1430, 0.1431, 0.1426, 0.1427, 0.1428],
        [0.1427, 0.1428, 0.1431, 0.1428, 0.1428, 0.1429, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1428, 0.1428, 0.1429, 0.1428, 0.1428, 0.1428, 0.1431],
        [0.1431, 0.1428, 0.1428, 0.1428, 0.1427, 0.1429, 0.1429],
        [0.1428, 0.1427, 0.1428, 0.1428, 0.1429, 0.1429, 0.1431],
        [0.1427, 0.1429, 0.1427, 0.1430, 0.1428, 0.1429, 0.1429]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1428, 0.1429, 0.1427, 0.1429, 0.1429, 0.1428, 0.1429],
        [0.1426, 0.1427, 0.1431, 0.1430, 0.1428, 0.1430, 0.1428],
        [0.1428, 0.1427, 0.1432, 0.1430, 0.1427, 0.1428, 0.1427],
        [0.1429, 0.1427, 0.1429, 0.1428, 0.1428, 0.1431, 0.1429],
        [0.1429, 0.1426, 0.1429, 0.1429, 0.1428, 0.1430, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1428, 0.1428, 0.1429, 0.1428, 0.1429, 0.1426, 0.1432],
        [0.1428, 0.1427, 0.1430, 0.1429, 0.1431, 0.1429, 0.1427]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1430, 0.1429, 0.1429, 0.1428, 0.1428, 0.1429, 0.1428],
        [0.1430, 0.1428, 0.1429, 0.1428, 0.1430, 0.1428, 0.1428],
        [0.1431, 0.1427, 0.1429, 0.1427, 0.1428, 0.1429, 0.1429]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1429, 0.1427, 0.1427, 0.1429, 0.1426, 0.1430, 0.1431],
        [0.1430, 0.1430, 0.1431, 0.1426, 0.1427, 0.1427, 0.1430],
        [0.1431, 0.1428, 0.1426, 0.1429, 0.1429, 0.1430, 0.1427],
        [0.1426, 0.1430, 0.1428, 0.1426, 0.1432, 0.1428, 0.1430]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1428, 0.1427, 0.1427, 0.1431, 0.1429, 0.1428, 0.1429],
        [0.1432, 0.1430, 0.1427, 0.1428, 0.1427, 0.1430, 0.1427],
        [0.1426, 0.1428, 0.1430, 0.1427, 0.1429, 0.1429, 0.1430],
        [0.1427, 0.1430, 0.1430, 0.1427, 0.1427, 0.1428, 0.1430],
        [0.1429, 0.1429, 0.1428, 0.1428, 0.1428, 0.1429, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
08/02 06:18:54 PM | Train: [ 1/210] Step 000/404 Loss 6.626 Prec@(1,5) (0.0%, 0.0%)
08/02 06:19:39 PM | Train: [ 1/210] Step 010/404 Loss 6.590 Prec@(1,5) (0.9%, 2.0%)
08/02 06:20:29 PM | Train: [ 1/210] Step 020/404 Loss 6.632 Prec@(1,5) (0.6%, 2.7%)
08/02 06:21:02 PM | Train: [ 1/210] Final Prec@1 0.6510%
08/02 06:22:54 PM | Valid: [ 1/210] Step 000/023 Loss 9.198 Prec@(1,5) (0.0%, 0.0%)
08/02 06:22:57 PM | Valid: [ 1/210] Step 010/023 Loss 10.437 Prec@(1,5) (0.3%, 0.3%)
08/02 06:23:01 PM | Valid: [ 1/210] Step 020/023 Loss 10.624 Prec@(1,5) (0.1%, 0.9%)
08/02 06:23:06 PM | Valid: [ 1/210] Final Prec@1 0.1359%, Prec@5 1.0870%, Prec@10 1.9022%
08/02 06:23:06 PM | Final best Prec@1 = 0.1359%
08/02 06:23:06 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('max_pool_3x3', 2), ('sep_conv_3x3', 1)], [('dil_conv_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('partial_aware_1x1', 2), ('partial_aware_1x1', 0)], [('partial_aware_1x1', 3), ('partial_aware_1x1', 2)], [('partial_aware_1x1', 3), ('dil_conv_3x3', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1540, 0.1345, 0.1363, 0.1649, 0.1424, 0.1112, 0.1567],
        [0.1529, 0.1381, 0.1425, 0.1530, 0.1526, 0.1343, 0.1265]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1581, 0.1329, 0.1377, 0.1309, 0.1288, 0.1428, 0.1688],
        [0.1521, 0.1306, 0.1369, 0.1685, 0.1352, 0.1319, 0.1447],
        [0.1639, 0.1430, 0.1393, 0.1307, 0.1391, 0.1393, 0.1448]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1284, 0.1204, 0.1231, 0.1453, 0.1387, 0.1866, 0.1575],
        [0.1621, 0.1438, 0.1531, 0.1464, 0.1166, 0.1472, 0.1308],
        [0.1611, 0.1515, 0.1538, 0.1272, 0.1526, 0.1176, 0.1363],
        [0.1521, 0.1396, 0.1417, 0.1408, 0.1391, 0.1477, 0.1390]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1695, 0.1540, 0.1586, 0.1235, 0.1211, 0.1354, 0.1379],
        [0.1428, 0.1309, 0.1352, 0.1551, 0.1377, 0.1531, 0.1452],
        [0.1724, 0.1650, 0.1495, 0.1321, 0.1237, 0.1254, 0.1318],
        [0.1683, 0.1558, 0.1435, 0.1240, 0.1445, 0.1315, 0.1325],
        [0.1685, 0.1617, 0.1555, 0.1078, 0.1517, 0.1279, 0.1268]],
       grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.0956, 0.0929, 0.1514, 0.1768, 0.1605, 0.1418, 0.1809],
        [0.1016, 0.0980, 0.1539, 0.1622, 0.1528, 0.1585, 0.1729]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0997, 0.0989, 0.1659, 0.1454, 0.1719, 0.1198, 0.1984],
        [0.1077, 0.1059, 0.1576, 0.1439, 0.1599, 0.1427, 0.1823],
        [0.1054, 0.1006, 0.0970, 0.1662, 0.1819, 0.1571, 0.1919]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1059, 0.1055, 0.1530, 0.1613, 0.1630, 0.1243, 0.1870],
        [0.1119, 0.1097, 0.1289, 0.1586, 0.1633, 0.1406, 0.1870],
        [0.1057, 0.1011, 0.1086, 0.1658, 0.1769, 0.1569, 0.1850],
        [0.0934, 0.0892, 0.0910, 0.1891, 0.1900, 0.1634, 0.1839]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1150, 0.1132, 0.1548, 0.1383, 0.1580, 0.1421, 0.1785],
        [0.1114, 0.1099, 0.1539, 0.1392, 0.1630, 0.1380, 0.1846],
        [0.1091, 0.1074, 0.1070, 0.1554, 0.1715, 0.1631, 0.1865],
        [0.0919, 0.0907, 0.0923, 0.1784, 0.1863, 0.1783, 0.1821],
        [0.0993, 0.0965, 0.0999, 0.1662, 0.1774, 0.1825, 0.1783]],
       grad_fn=<SoftmaxBackward>)
#####################
08/02 06:26:51 PM | Train: [ 2/210] Step 000/404 Loss 5.770 Prec@(1,5) (6.2%, 12.5%)
08/02 06:27:36 PM | Train: [ 2/210] Step 010/404 Loss 5.853 Prec@(1,5) (3.4%, 8.2%)
08/02 06:28:22 PM | Train: [ 2/210] Step 020/404 Loss 5.849 Prec@(1,5) (2.5%, 7.7%)
08/02 06:29:07 PM | Train: [ 2/210] Step 030/404 Loss 6.035 Prec@(1,5) (2.2%, 7.2%)
08/02 06:29:53 PM | Train: [ 2/210] Step 040/404 Loss 6.145 Prec@(1,5) (1.8%, 6.2%)
08/02 06:30:41 PM | Train: [ 2/210] Step 050/404 Loss 6.198 Prec@(1,5) (1.6%, 5.9%)
08/02 06:31:24 PM | Train: [ 2/210] Step 060/404 Loss 6.207 Prec@(1,5) (1.7%, 5.7%)
08/02 06:32:09 PM | Train: [ 2/210] Step 070/404 Loss 6.238 Prec@(1,5) (1.6%, 5.6%)
08/02 06:32:56 PM | Train: [ 2/210] Step 080/404 Loss 6.239 Prec@(1,5) (1.5%, 5.7%)
08/02 06:33:41 PM | Train: [ 2/210] Step 090/404 Loss 6.229 Prec@(1,5) (1.6%, 5.8%)
08/02 06:34:25 PM | Train: [ 2/210] Step 100/404 Loss 6.216 Prec@(1,5) (1.7%, 6.2%)
08/02 06:35:11 PM | Train: [ 2/210] Step 110/404 Loss 6.207 Prec@(1,5) (1.7%, 6.2%)
08/02 06:35:57 PM | Train: [ 2/210] Step 120/404 Loss 6.188 Prec@(1,5) (1.8%, 6.3%)
08/02 06:36:42 PM | Train: [ 2/210] Step 130/404 Loss 6.170 Prec@(1,5) (1.9%, 6.5%)
08/02 06:37:28 PM | Train: [ 2/210] Step 140/404 Loss 6.157 Prec@(1,5) (1.8%, 6.5%)
08/02 06:38:12 PM | Train: [ 2/210] Step 150/404 Loss 6.134 Prec@(1,5) (2.0%, 6.8%)
08/02 06:38:57 PM | Train: [ 2/210] Step 160/404 Loss 6.118 Prec@(1,5) (2.0%, 6.9%)
08/02 06:39:41 PM | Train: [ 2/210] Step 170/404 Loss 6.100 Prec@(1,5) (2.1%, 7.2%)
08/02 06:40:29 PM | Train: [ 2/210] Step 180/404 Loss 6.091 Prec@(1,5) (2.2%, 7.2%)
08/02 06:41:16 PM | Train: [ 2/210] Step 190/404 Loss 6.078 Prec@(1,5) (2.2%, 7.3%)
08/02 06:42:01 PM | Train: [ 2/210] Step 200/404 Loss 6.061 Prec@(1,5) (2.2%, 7.5%)
08/02 06:42:48 PM | Train: [ 2/210] Step 210/404 Loss 6.052 Prec@(1,5) (2.2%, 7.4%)
08/02 06:43:36 PM | Train: [ 2/210] Step 220/404 Loss 6.033 Prec@(1,5) (2.2%, 7.5%)
08/02 06:44:20 PM | Train: [ 2/210] Step 230/404 Loss 6.017 Prec@(1,5) (2.2%, 7.8%)
08/02 06:45:08 PM | Train: [ 2/210] Step 240/404 Loss 6.001 Prec@(1,5) (2.3%, 8.0%)
08/02 06:45:56 PM | Train: [ 2/210] Step 250/404 Loss 5.986 Prec@(1,5) (2.3%, 8.1%)
08/02 06:46:43 PM | Train: [ 2/210] Step 260/404 Loss 5.975 Prec@(1,5) (2.4%, 8.2%)
08/02 06:47:29 PM | Train: [ 2/210] Step 270/404 Loss 5.960 Prec@(1,5) (2.5%, 8.3%)
08/02 06:48:12 PM | Train: [ 2/210] Step 280/404 Loss 5.949 Prec@(1,5) (2.5%, 8.5%)
08/02 06:48:57 PM | Train: [ 2/210] Step 290/404 Loss 5.933 Prec@(1,5) (2.5%, 8.6%)
08/02 06:49:45 PM | Train: [ 2/210] Step 300/404 Loss 5.913 Prec@(1,5) (2.6%, 8.8%)
08/02 06:50:35 PM | Train: [ 2/210] Step 310/404 Loss 5.901 Prec@(1,5) (2.7%, 8.9%)
08/02 06:51:22 PM | Train: [ 2/210] Step 320/404 Loss 5.886 Prec@(1,5) (2.7%, 9.1%)
08/02 06:52:10 PM | Train: [ 2/210] Step 330/404 Loss 5.872 Prec@(1,5) (2.8%, 9.3%)
08/02 06:52:56 PM | Train: [ 2/210] Step 340/404 Loss 5.854 Prec@(1,5) (2.9%, 9.6%)
08/02 06:53:43 PM | Train: [ 2/210] Step 350/404 Loss 5.845 Prec@(1,5) (2.9%, 9.7%)
08/02 06:54:28 PM | Train: [ 2/210] Step 360/404 Loss 5.829 Prec@(1,5) (3.0%, 9.9%)
08/02 06:55:17 PM | Train: [ 2/210] Step 370/404 Loss 5.816 Prec@(1,5) (3.2%, 10.1%)
08/02 06:56:02 PM | Train: [ 2/210] Step 380/404 Loss 5.805 Prec@(1,5) (3.2%, 10.3%)
08/02 06:56:48 PM | Train: [ 2/210] Step 390/404 Loss 5.788 Prec@(1,5) (3.2%, 10.5%)
08/02 06:57:34 PM | Train: [ 2/210] Step 400/404 Loss 5.774 Prec@(1,5) (3.3%, 10.7%)
08/02 06:57:58 PM | Train: [ 2/210] Final Prec@1 3.3106%
08/02 06:59:53 PM | Valid: [ 2/210] Step 000/023 Loss 6.351 Prec@(1,5) (3.1%, 9.4%)
08/02 06:59:56 PM | Valid: [ 2/210] Step 010/023 Loss 5.649 Prec@(1,5) (4.8%, 15.3%)
08/02 07:00:00 PM | Valid: [ 2/210] Step 020/023 Loss 5.663 Prec@(1,5) (4.8%, 14.3%)
08/02 07:00:05 PM | Valid: [ 2/210] Final Prec@1 4.8913%, Prec@5 14.2663%, Prec@10 22.9620%
08/02 07:00:06 PM | Final best Prec@1 = 4.8913%
08/02 07:00:06 PM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('partial_aware_1x1', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('partial_aware_1x1', 0), ('max_pool_3x3', 3)], [('partial_aware_1x1', 3), ('max_pool_3x3', 2)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('sep_conv_3x3', 2), ('max_pool_3x3', 0)], [('sep_conv_3x3', 3), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 4), ('sep_conv_3x3', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.2071, 0.1503, 0.1426, 0.1446, 0.1442, 0.0919, 0.1193],
        [0.1735, 0.1458, 0.1417, 0.1193, 0.1971, 0.0963, 0.1263]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.2409, 0.1804, 0.1908, 0.0922, 0.1020, 0.0828, 0.1110],
        [0.1535, 0.1286, 0.1375, 0.1797, 0.1367, 0.0945, 0.1696],
        [0.2006, 0.1629, 0.1149, 0.1738, 0.0944, 0.1088, 0.1445]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0969, 0.0780, 0.0806, 0.1556, 0.4310, 0.0912, 0.0666],
        [0.1722, 0.1287, 0.1427, 0.1278, 0.1133, 0.1172, 0.1981],
        [0.2438, 0.1842, 0.0968, 0.1084, 0.1109, 0.1223, 0.1336],
        [0.2791, 0.1951, 0.0861, 0.1395, 0.1031, 0.0823, 0.1147]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.2220, 0.1535, 0.1750, 0.0933, 0.1612, 0.0852, 0.1098],
        [0.1450, 0.1194, 0.1342, 0.1548, 0.1641, 0.1156, 0.1669],
        [0.2725, 0.1938, 0.0967, 0.1044, 0.0809, 0.0997, 0.1521],
        [0.1666, 0.1432, 0.0788, 0.0571, 0.3499, 0.1047, 0.0996],
        [0.2095, 0.1769, 0.0947, 0.1832, 0.1041, 0.1431, 0.0884]],
       grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1385, 0.0555, 0.1106, 0.1087, 0.0698, 0.0759, 0.4410],
        [0.1364, 0.0549, 0.1205, 0.0903, 0.0581, 0.1134, 0.4263]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1181, 0.0531, 0.1072, 0.0885, 0.0600, 0.0823, 0.4907],
        [0.0890, 0.0447, 0.0936, 0.0596, 0.0514, 0.0497, 0.6120],
        [0.0920, 0.0399, 0.0438, 0.1080, 0.0654, 0.0923, 0.5586]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1001, 0.0530, 0.0985, 0.1214, 0.0485, 0.0664, 0.5122],
        [0.0914, 0.0502, 0.0802, 0.0864, 0.0409, 0.1000, 0.5509],
        [0.0868, 0.0457, 0.0537, 0.1032, 0.0598, 0.1008, 0.5500],
        [0.0569, 0.0382, 0.0399, 0.1634, 0.0503, 0.0960, 0.5553]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1546, 0.0743, 0.1128, 0.0889, 0.0398, 0.0880, 0.4416],
        [0.0905, 0.0539, 0.1035, 0.0781, 0.0358, 0.0839, 0.5544],
        [0.0801, 0.0464, 0.0577, 0.0842, 0.0441, 0.0801, 0.6075],
        [0.0596, 0.0400, 0.0423, 0.1405, 0.0465, 0.1148, 0.5563],
        [0.0466, 0.0353, 0.0382, 0.2370, 0.0582, 0.1562, 0.4285]],
       grad_fn=<SoftmaxBackward>)
#####################
08/02 07:03:43 PM | Train: [ 3/210] Step 000/404 Loss 5.081 Prec@(1,5) (9.4%, 21.9%)
08/02 07:04:28 PM | Train: [ 3/210] Step 010/404 Loss 5.127 Prec@(1,5) (4.8%, 19.0%)
08/02 07:05:17 PM | Train: [ 3/210] Step 020/404 Loss 5.176 Prec@(1,5) (4.6%, 18.2%)
08/02 07:06:03 PM | Train: [ 3/210] Step 030/404 Loss 5.159 Prec@(1,5) (4.7%, 17.7%)
08/02 07:06:49 PM | Train: [ 3/210] Step 040/404 Loss 5.168 Prec@(1,5) (5.5%, 17.8%)
08/02 07:07:34 PM | Train: [ 3/210] Step 050/404 Loss 5.171 Prec@(1,5) (5.9%, 18.3%)
08/02 07:08:21 PM | Train: [ 3/210] Step 060/404 Loss 5.159 Prec@(1,5) (6.1%, 18.0%)
08/02 07:09:06 PM | Train: [ 3/210] Step 070/404 Loss 5.172 Prec@(1,5) (6.2%, 18.0%)
08/02 07:09:51 PM | Train: [ 3/210] Step 080/404 Loss 5.176 Prec@(1,5) (6.3%, 17.9%)
08/02 07:10:40 PM | Train: [ 3/210] Step 090/404 Loss 5.159 Prec@(1,5) (6.2%, 18.2%)
08/02 07:11:25 PM | Train: [ 3/210] Step 100/404 Loss 5.154 Prec@(1,5) (6.3%, 18.4%)
08/02 07:12:14 PM | Train: [ 3/210] Step 110/404 Loss 5.151 Prec@(1,5) (6.2%, 18.3%)
08/02 07:12:59 PM | Train: [ 3/210] Step 120/404 Loss 5.124 Prec@(1,5) (6.4%, 18.6%)
08/02 07:13:42 PM | Train: [ 3/210] Step 130/404 Loss 5.115 Prec@(1,5) (6.5%, 18.6%)
08/02 07:14:26 PM | Train: [ 3/210] Step 140/404 Loss 5.106 Prec@(1,5) (6.4%, 18.8%)
08/02 07:15:12 PM | Train: [ 3/210] Step 150/404 Loss 5.089 Prec@(1,5) (6.6%, 19.0%)
08/02 07:15:55 PM | Train: [ 3/210] Step 160/404 Loss 5.075 Prec@(1,5) (6.6%, 19.1%)
08/02 07:16:40 PM | Train: [ 3/210] Step 170/404 Loss 5.066 Prec@(1,5) (6.9%, 19.4%)
08/02 07:17:25 PM | Train: [ 3/210] Step 180/404 Loss 5.060 Prec@(1,5) (6.9%, 19.3%)
08/02 07:18:08 PM | Train: [ 3/210] Step 190/404 Loss 5.050 Prec@(1,5) (7.1%, 19.5%)
08/02 07:18:51 PM | Train: [ 3/210] Step 200/404 Loss 5.044 Prec@(1,5) (7.1%, 19.6%)
08/02 07:19:34 PM | Train: [ 3/210] Step 210/404 Loss 5.044 Prec@(1,5) (7.0%, 19.4%)
08/02 07:20:21 PM | Train: [ 3/210] Step 220/404 Loss 5.032 Prec@(1,5) (7.1%, 19.5%)
08/02 07:21:04 PM | Train: [ 3/210] Step 230/404 Loss 5.027 Prec@(1,5) (7.2%, 19.5%)
08/02 07:21:52 PM | Train: [ 3/210] Step 240/404 Loss 5.021 Prec@(1,5) (7.2%, 19.6%)
08/02 07:22:37 PM | Train: [ 3/210] Step 250/404 Loss 5.012 Prec@(1,5) (7.2%, 19.8%)
08/02 07:23:22 PM | Train: [ 3/210] Step 260/404 Loss 5.009 Prec@(1,5) (7.3%, 19.9%)
08/02 07:24:07 PM | Train: [ 3/210] Step 270/404 Loss 5.001 Prec@(1,5) (7.4%, 20.1%)
08/02 07:24:52 PM | Train: [ 3/210] Step 280/404 Loss 4.996 Prec@(1,5) (7.4%, 20.2%)
08/02 07:25:39 PM | Train: [ 3/210] Step 290/404 Loss 4.985 Prec@(1,5) (7.4%, 20.4%)
08/02 07:26:24 PM | Train: [ 3/210] Step 300/404 Loss 4.973 Prec@(1,5) (7.5%, 20.6%)
08/02 07:27:09 PM | Train: [ 3/210] Step 310/404 Loss 4.968 Prec@(1,5) (7.5%, 20.6%)
08/02 07:27:52 PM | Train: [ 3/210] Step 320/404 Loss 4.960 Prec@(1,5) (7.6%, 20.7%)
08/02 07:28:39 PM | Train: [ 3/210] Step 330/404 Loss 4.952 Prec@(1,5) (7.7%, 20.9%)
08/02 07:29:23 PM | Train: [ 3/210] Step 340/404 Loss 4.939 Prec@(1,5) (7.8%, 21.2%)
08/02 07:30:06 PM | Train: [ 3/210] Step 350/404 Loss 4.935 Prec@(1,5) (7.8%, 21.2%)
08/02 07:30:54 PM | Train: [ 3/210] Step 360/404 Loss 4.924 Prec@(1,5) (8.0%, 21.5%)
08/02 07:31:42 PM | Train: [ 3/210] Step 370/404 Loss 4.917 Prec@(1,5) (8.1%, 21.6%)
08/02 07:32:28 PM | Train: [ 3/210] Step 380/404 Loss 4.912 Prec@(1,5) (8.1%, 21.7%)
08/02 07:33:12 PM | Train: [ 3/210] Step 390/404 Loss 4.900 Prec@(1,5) (8.2%, 21.9%)
08/02 07:33:55 PM | Train: [ 3/210] Step 400/404 Loss 4.892 Prec@(1,5) (8.2%, 22.0%)
08/02 07:34:17 PM | Train: [ 3/210] Final Prec@1 8.2534%
08/02 07:36:09 PM | Valid: [ 3/210] Step 000/023 Loss 5.641 Prec@(1,5) (3.1%, 12.5%)
08/02 07:36:12 PM | Valid: [ 3/210] Step 010/023 Loss 5.058 Prec@(1,5) (9.1%, 25.3%)
08/02 07:36:15 PM | Valid: [ 3/210] Step 020/023 Loss 5.091 Prec@(1,5) (9.1%, 24.1%)
08/02 07:36:20 PM | Valid: [ 3/210] Final Prec@1 9.1033%, Prec@5 24.1848%, Prec@10 34.6467%
08/02 07:36:21 PM | Final best Prec@1 = 9.1033%
08/02 07:36:21 PM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('partial_aware_1x1', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 2), ('partial_aware_1x1', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('sep_conv_3x3', 3), ('max_pool_3x3', 2)], [('sep_conv_3x3', 4), ('sep_conv_3x3', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.2375, 0.2195, 0.1598, 0.0610, 0.1294, 0.1065, 0.0862],
        [0.1442, 0.1440, 0.1311, 0.1420, 0.1998, 0.1203, 0.1186]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.2386, 0.2067, 0.1710, 0.1143, 0.1279, 0.0613, 0.0801],
        [0.1080, 0.0995, 0.1039, 0.1334, 0.1907, 0.0623, 0.3022],
        [0.2510, 0.1439, 0.0912, 0.1656, 0.0939, 0.1092, 0.1451]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1358, 0.1128, 0.1137, 0.1320, 0.3354, 0.0961, 0.0742],
        [0.2090, 0.1612, 0.1781, 0.0999, 0.0879, 0.0971, 0.1669],
        [0.4557, 0.1659, 0.0704, 0.0996, 0.0559, 0.0675, 0.0850],
        [0.4792, 0.1438, 0.0562, 0.1504, 0.0619, 0.0379, 0.0705]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.3292, 0.1758, 0.2013, 0.0760, 0.1084, 0.0447, 0.0645],
        [0.1461, 0.1068, 0.1223, 0.0716, 0.2301, 0.0601, 0.2630],
        [0.5809, 0.1187, 0.0630, 0.0670, 0.0508, 0.0430, 0.0766],
        [0.2031, 0.1296, 0.0754, 0.0455, 0.3813, 0.0674, 0.0978],
        [0.2323, 0.1205, 0.0829, 0.2102, 0.1827, 0.1255, 0.0459]],
       grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1833, 0.0370, 0.1257, 0.1094, 0.0472, 0.0943, 0.4031],
        [0.1691, 0.0378, 0.0903, 0.0935, 0.0405, 0.1106, 0.4581]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1552, 0.0404, 0.0904, 0.1003, 0.0341, 0.0682, 0.5114],
        [0.0860, 0.0289, 0.0821, 0.0589, 0.0316, 0.0559, 0.6567],
        [0.1513, 0.0281, 0.0278, 0.1113, 0.0407, 0.1045, 0.5364]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1038, 0.0318, 0.0979, 0.1286, 0.0313, 0.0632, 0.5433],
        [0.0981, 0.0324, 0.0766, 0.0629, 0.0255, 0.0977, 0.6068],
        [0.1587, 0.0331, 0.0333, 0.1351, 0.0359, 0.1259, 0.4780],
        [0.0748, 0.0334, 0.0328, 0.1951, 0.0391, 0.1634, 0.4615]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1907, 0.0506, 0.1171, 0.0973, 0.0211, 0.0791, 0.4441],
        [0.0983, 0.0376, 0.1108, 0.0836, 0.0223, 0.0916, 0.5557],
        [0.1024, 0.0413, 0.0426, 0.1030, 0.0266, 0.0744, 0.6097],
        [0.0494, 0.0299, 0.0301, 0.1724, 0.0260, 0.1195, 0.5727],
        [0.0421, 0.0299, 0.0291, 0.2874, 0.0377, 0.2359, 0.3379]],
       grad_fn=<SoftmaxBackward>)
#####################
08/02 07:39:59 PM | Train: [ 4/210] Step 000/404 Loss 4.609 Prec@(1,5) (9.4%, 25.0%)
08/02 07:40:48 PM | Train: [ 4/210] Step 010/404 Loss 4.499 Prec@(1,5) (11.1%, 27.3%)
08/02 07:41:34 PM | Train: [ 4/210] Step 020/404 Loss 4.531 Prec@(1,5) (11.3%, 28.1%)
08/02 07:42:19 PM | Train: [ 4/210] Step 030/404 Loss 4.539 Prec@(1,5) (11.4%, 27.3%)
08/02 07:43:03 PM | Train: [ 4/210] Step 040/404 Loss 4.556 Prec@(1,5) (11.1%, 27.7%)
08/02 07:43:47 PM | Train: [ 4/210] Step 050/404 Loss 4.555 Prec@(1,5) (11.2%, 27.5%)
08/02 07:44:31 PM | Train: [ 4/210] Step 060/404 Loss 4.553 Prec@(1,5) (10.7%, 27.3%)
08/02 07:45:18 PM | Train: [ 4/210] Step 070/404 Loss 4.560 Prec@(1,5) (11.0%, 27.6%)
08/02 07:46:03 PM | Train: [ 4/210] Step 080/404 Loss 4.554 Prec@(1,5) (11.1%, 27.6%)
08/02 07:46:50 PM | Train: [ 4/210] Step 090/404 Loss 4.533 Prec@(1,5) (11.3%, 28.1%)
08/02 07:47:34 PM | Train: [ 4/210] Step 100/404 Loss 4.529 Prec@(1,5) (11.3%, 28.4%)
08/02 07:48:18 PM | Train: [ 4/210] Step 110/404 Loss 4.534 Prec@(1,5) (11.1%, 28.4%)
08/02 07:49:01 PM | Train: [ 4/210] Step 120/404 Loss 4.515 Prec@(1,5) (11.1%, 28.8%)
08/02 07:49:46 PM | Train: [ 4/210] Step 130/404 Loss 4.513 Prec@(1,5) (11.2%, 28.7%)
08/02 07:50:33 PM | Train: [ 4/210] Step 140/404 Loss 4.507 Prec@(1,5) (11.2%, 28.6%)
08/02 07:51:17 PM | Train: [ 4/210] Step 150/404 Loss 4.495 Prec@(1,5) (11.5%, 28.8%)
08/02 07:52:03 PM | Train: [ 4/210] Step 160/404 Loss 4.482 Prec@(1,5) (11.5%, 29.1%)
08/02 07:52:47 PM | Train: [ 4/210] Step 170/404 Loss 4.474 Prec@(1,5) (11.8%, 29.2%)
08/02 07:53:31 PM | Train: [ 4/210] Step 180/404 Loss 4.472 Prec@(1,5) (11.8%, 29.2%)
08/02 07:54:22 PM | Train: [ 4/210] Step 190/404 Loss 4.466 Prec@(1,5) (12.0%, 29.3%)
08/02 07:55:15 PM | Train: [ 4/210] Step 200/404 Loss 4.464 Prec@(1,5) (12.0%, 29.4%)
08/02 07:55:59 PM | Train: [ 4/210] Step 210/404 Loss 4.470 Prec@(1,5) (11.8%, 29.2%)
08/02 07:56:47 PM | Train: [ 4/210] Step 220/404 Loss 4.461 Prec@(1,5) (11.9%, 29.5%)
08/02 07:57:33 PM | Train: [ 4/210] Step 230/404 Loss 4.455 Prec@(1,5) (11.9%, 29.6%)
08/02 07:58:19 PM | Train: [ 4/210] Step 240/404 Loss 4.450 Prec@(1,5) (11.9%, 29.7%)
08/02 07:59:06 PM | Train: [ 4/210] Step 250/404 Loss 4.441 Prec@(1,5) (12.1%, 29.9%)
08/02 07:59:52 PM | Train: [ 4/210] Step 260/404 Loss 4.439 Prec@(1,5) (12.2%, 29.9%)
08/02 08:00:43 PM | Train: [ 4/210] Step 270/404 Loss 4.434 Prec@(1,5) (12.2%, 30.1%)
08/02 08:01:30 PM | Train: [ 4/210] Step 280/404 Loss 4.432 Prec@(1,5) (12.3%, 30.1%)
08/02 08:02:15 PM | Train: [ 4/210] Step 290/404 Loss 4.421 Prec@(1,5) (12.3%, 30.3%)
08/02 08:03:02 PM | Train: [ 4/210] Step 300/404 Loss 4.411 Prec@(1,5) (12.4%, 30.5%)
08/02 08:03:48 PM | Train: [ 4/210] Step 310/404 Loss 4.405 Prec@(1,5) (12.4%, 30.5%)
08/02 08:04:34 PM | Train: [ 4/210] Step 320/404 Loss 4.398 Prec@(1,5) (12.5%, 30.6%)
08/02 08:05:23 PM | Train: [ 4/210] Step 330/404 Loss 4.392 Prec@(1,5) (12.6%, 30.7%)
08/02 08:06:07 PM | Train: [ 4/210] Step 340/404 Loss 4.381 Prec@(1,5) (12.7%, 31.0%)
08/02 08:06:53 PM | Train: [ 4/210] Step 350/404 Loss 4.378 Prec@(1,5) (12.7%, 31.0%)
08/02 08:07:39 PM | Train: [ 4/210] Step 360/404 Loss 4.369 Prec@(1,5) (12.8%, 31.2%)
08/02 08:08:25 PM | Train: [ 4/210] Step 370/404 Loss 4.362 Prec@(1,5) (12.9%, 31.3%)
08/02 08:09:10 PM | Train: [ 4/210] Step 380/404 Loss 4.359 Prec@(1,5) (12.9%, 31.4%)
08/02 08:09:54 PM | Train: [ 4/210] Step 390/404 Loss 4.348 Prec@(1,5) (13.0%, 31.7%)
08/02 08:10:43 PM | Train: [ 4/210] Step 400/404 Loss 4.341 Prec@(1,5) (13.1%, 31.9%)
08/02 08:11:06 PM | Train: [ 4/210] Final Prec@1 13.1343%
08/02 08:13:03 PM | Valid: [ 4/210] Step 000/023 Loss 4.669 Prec@(1,5) (9.4%, 37.5%)
08/02 08:13:06 PM | Valid: [ 4/210] Step 010/023 Loss 4.646 Prec@(1,5) (12.5%, 35.5%)
08/02 08:13:10 PM | Valid: [ 4/210] Step 020/023 Loss 4.696 Prec@(1,5) (12.4%, 34.2%)
08/02 08:13:14 PM | Valid: [ 4/210] Final Prec@1 12.3641%, Prec@5 34.2391%, Prec@10 46.7391%
08/02 08:13:15 PM | Final best Prec@1 = 12.3641%
08/02 08:13:15 PM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('partial_aware_1x1', 1)], [('max_pool_3x3', 2), ('partial_aware_1x1', 1)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 3), ('max_pool_3x3', 2)], [('sep_conv_3x3', 4), ('sep_conv_3x3', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.2591, 0.2433, 0.1521, 0.0438, 0.1104, 0.1192, 0.0721],
        [0.1340, 0.1455, 0.1321, 0.1525, 0.1784, 0.1525, 0.1051]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.2386, 0.1976, 0.1528, 0.1155, 0.1627, 0.0575, 0.0753],
        [0.0869, 0.0828, 0.0874, 0.1009, 0.2450, 0.0620, 0.3351],
        [0.3698, 0.1438, 0.0756, 0.1336, 0.0888, 0.0714, 0.1169]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1304, 0.1069, 0.1071, 0.1303, 0.3350, 0.1149, 0.0754],
        [0.2182, 0.1640, 0.1872, 0.0992, 0.0898, 0.1127, 0.1290],
        [0.5758, 0.1085, 0.0510, 0.0965, 0.0435, 0.0550, 0.0696],
        [0.5464, 0.1006, 0.0452, 0.1356, 0.0621, 0.0306, 0.0795]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.3937, 0.1810, 0.2012, 0.0569, 0.0887, 0.0322, 0.0463],
        [0.1770, 0.1277, 0.1554, 0.0576, 0.1763, 0.0434, 0.2626],
        [0.7733, 0.0651, 0.0381, 0.0370, 0.0291, 0.0229, 0.0345],
        [0.2330, 0.1320, 0.0811, 0.0446, 0.3544, 0.0490, 0.1059],
        [0.1864, 0.0920, 0.0682, 0.2328, 0.2634, 0.1172, 0.0399]],
       grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1829, 0.0264, 0.1315, 0.1068, 0.0440, 0.1298, 0.3786],
        [0.1846, 0.0278, 0.0904, 0.0960, 0.0324, 0.1170, 0.4518]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1377, 0.0301, 0.0968, 0.1377, 0.0276, 0.0651, 0.5050],
        [0.0785, 0.0210, 0.0825, 0.0594, 0.0272, 0.0648, 0.6666],
        [0.1909, 0.0181, 0.0187, 0.1136, 0.0311, 0.1308, 0.4967]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1206, 0.0236, 0.1015, 0.1563, 0.0214, 0.0585, 0.5181],
        [0.0953, 0.0240, 0.0841, 0.0651, 0.0196, 0.0919, 0.6199],
        [0.2096, 0.0216, 0.0220, 0.1463, 0.0271, 0.1200, 0.4535],
        [0.0885, 0.0303, 0.0252, 0.2265, 0.0299, 0.2254, 0.3742]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.2420, 0.0428, 0.1279, 0.1090, 0.0165, 0.0735, 0.3883],
        [0.1061, 0.0311, 0.1265, 0.0799, 0.0169, 0.0845, 0.5551],
        [0.1185, 0.0349, 0.0337, 0.1118, 0.0190, 0.0829, 0.5993],
        [0.0555, 0.0312, 0.0265, 0.2636, 0.0211, 0.1457, 0.4564],
        [0.0381, 0.0261, 0.0222, 0.3236, 0.0282, 0.2956, 0.2663]],
       grad_fn=<SoftmaxBackward>)
#####################
08/02 08:17:00 PM | Train: [ 5/210] Step 000/404 Loss 4.409 Prec@(1,5) (12.5%, 21.9%)
08/02 08:17:45 PM | Train: [ 5/210] Step 010/404 Loss 4.021 Prec@(1,5) (16.5%, 38.6%)
08/02 08:18:29 PM | Train: [ 5/210] Step 020/404 Loss 4.077 Prec@(1,5) (16.2%, 36.9%)
08/02 08:19:14 PM | Train: [ 5/210] Step 030/404 Loss 4.063 Prec@(1,5) (16.6%, 37.9%)
08/02 08:20:01 PM | Train: [ 5/210] Step 040/404 Loss 4.096 Prec@(1,5) (16.4%, 37.2%)
08/02 08:20:49 PM | Train: [ 5/210] Step 050/404 Loss 4.091 Prec@(1,5) (16.3%, 37.1%)
08/02 08:21:37 PM | Train: [ 5/210] Step 060/404 Loss 4.088 Prec@(1,5) (15.9%, 36.8%)
08/02 08:22:21 PM | Train: [ 5/210] Step 070/404 Loss 4.093 Prec@(1,5) (16.0%, 36.6%)
08/02 08:23:07 PM | Train: [ 5/210] Step 080/404 Loss 4.082 Prec@(1,5) (16.4%, 37.0%)
08/02 08:23:53 PM | Train: [ 5/210] Step 090/404 Loss 4.048 Prec@(1,5) (16.9%, 37.7%)
08/02 08:24:37 PM | Train: [ 5/210] Step 100/404 Loss 4.036 Prec@(1,5) (16.8%, 38.1%)
08/02 08:25:23 PM | Train: [ 5/210] Step 110/404 Loss 4.041 Prec@(1,5) (16.6%, 38.0%)
08/02 08:26:09 PM | Train: [ 5/210] Step 120/404 Loss 4.029 Prec@(1,5) (16.7%, 38.3%)
08/02 08:26:52 PM | Train: [ 5/210] Step 130/404 Loss 4.031 Prec@(1,5) (16.7%, 38.1%)
08/02 08:27:36 PM | Train: [ 5/210] Step 140/404 Loss 4.034 Prec@(1,5) (16.5%, 38.0%)
08/02 08:28:21 PM | Train: [ 5/210] Step 150/404 Loss 4.025 Prec@(1,5) (16.7%, 38.2%)
08/02 08:29:06 PM | Train: [ 5/210] Step 160/404 Loss 4.015 Prec@(1,5) (16.7%, 38.3%)
08/02 08:29:51 PM | Train: [ 5/210] Step 170/404 Loss 4.005 Prec@(1,5) (17.0%, 38.4%)
08/02 08:30:39 PM | Train: [ 5/210] Step 180/404 Loss 4.002 Prec@(1,5) (17.0%, 38.4%)
08/02 08:31:24 PM | Train: [ 5/210] Step 190/404 Loss 4.001 Prec@(1,5) (17.1%, 38.5%)
08/02 08:32:08 PM | Train: [ 5/210] Step 200/404 Loss 3.998 Prec@(1,5) (17.1%, 38.7%)
08/02 08:32:52 PM | Train: [ 5/210] Step 210/404 Loss 4.002 Prec@(1,5) (17.1%, 38.4%)
08/02 08:33:38 PM | Train: [ 5/210] Step 220/404 Loss 3.994 Prec@(1,5) (17.2%, 38.7%)
08/02 08:34:23 PM | Train: [ 5/210] Step 230/404 Loss 3.988 Prec@(1,5) (17.2%, 38.9%)
08/02 08:35:07 PM | Train: [ 5/210] Step 240/404 Loss 3.988 Prec@(1,5) (17.2%, 38.9%)
08/02 08:35:51 PM | Train: [ 5/210] Step 250/404 Loss 3.979 Prec@(1,5) (17.3%, 39.2%)
08/02 08:36:35 PM | Train: [ 5/210] Step 260/404 Loss 3.975 Prec@(1,5) (17.4%, 39.3%)
08/02 08:37:18 PM | Train: [ 5/210] Step 270/404 Loss 3.974 Prec@(1,5) (17.4%, 39.4%)
08/02 08:38:04 PM | Train: [ 5/210] Step 280/404 Loss 3.976 Prec@(1,5) (17.4%, 39.4%)
08/02 08:38:50 PM | Train: [ 5/210] Step 290/404 Loss 3.967 Prec@(1,5) (17.5%, 39.5%)
08/02 08:39:35 PM | Train: [ 5/210] Step 300/404 Loss 3.959 Prec@(1,5) (17.7%, 39.6%)
08/02 08:40:22 PM | Train: [ 5/210] Step 310/404 Loss 3.951 Prec@(1,5) (17.7%, 39.8%)
08/02 08:41:08 PM | Train: [ 5/210] Step 320/404 Loss 3.945 Prec@(1,5) (17.8%, 39.9%)
08/02 08:41:57 PM | Train: [ 5/210] Step 330/404 Loss 3.941 Prec@(1,5) (18.0%, 40.0%)
08/02 08:42:42 PM | Train: [ 5/210] Step 340/404 Loss 3.931 Prec@(1,5) (18.1%, 40.3%)
08/02 08:43:29 PM | Train: [ 5/210] Step 350/404 Loss 3.931 Prec@(1,5) (18.1%, 40.4%)
08/02 08:44:16 PM | Train: [ 5/210] Step 360/404 Loss 3.921 Prec@(1,5) (18.3%, 40.6%)
08/02 08:45:00 PM | Train: [ 5/210] Step 370/404 Loss 3.913 Prec@(1,5) (18.4%, 40.8%)
08/02 08:45:46 PM | Train: [ 5/210] Step 380/404 Loss 3.910 Prec@(1,5) (18.4%, 40.8%)
08/02 08:46:33 PM | Train: [ 5/210] Step 390/404 Loss 3.898 Prec@(1,5) (18.6%, 41.0%)
08/02 08:47:21 PM | Train: [ 5/210] Step 400/404 Loss 3.892 Prec@(1,5) (18.6%, 41.2%)
08/02 08:47:44 PM | Train: [ 5/210] Final Prec@1 18.6262%
08/02 08:49:37 PM | Valid: [ 5/210] Step 000/023 Loss 4.629 Prec@(1,5) (12.5%, 37.5%)
08/02 08:49:40 PM | Valid: [ 5/210] Step 010/023 Loss 4.644 Prec@(1,5) (15.9%, 39.5%)
08/02 08:49:44 PM | Valid: [ 5/210] Step 020/023 Loss 4.648 Prec@(1,5) (15.0%, 38.1%)
08/02 08:49:48 PM | Valid: [ 5/210] Final Prec@1 15.3533%, Prec@5 38.1793%, Prec@10 52.0380%
08/02 08:49:48 PM | Final best Prec@1 = 15.3533%
08/02 08:49:48 PM | genotype = Genotype(normal=[[('avg_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 2), ('partial_aware_1x1', 1)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('sep_conv_3x3', 0)], [('max_pool_3x3', 2), ('sep_conv_3x3', 3)], [('sep_conv_3x3', 4), ('sep_conv_3x3', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.2342, 0.2615, 0.1389, 0.0410, 0.1044, 0.1553, 0.0647],
        [0.1264, 0.1652, 0.1485, 0.1173, 0.1627, 0.1856, 0.0943]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.2197, 0.2257, 0.1690, 0.1404, 0.0980, 0.0538, 0.0934],
        [0.0734, 0.0777, 0.0815, 0.0837, 0.1977, 0.0573, 0.4287],
        [0.3745, 0.1422, 0.0691, 0.1411, 0.0935, 0.0833, 0.0963]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1352, 0.1176, 0.1166, 0.1165, 0.3323, 0.1051, 0.0768],
        [0.2457, 0.2104, 0.2403, 0.0681, 0.0585, 0.0848, 0.0922],
        [0.6466, 0.0914, 0.0576, 0.0833, 0.0290, 0.0415, 0.0506],
        [0.5569, 0.0913, 0.0535, 0.1159, 0.0711, 0.0277, 0.0836]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.3981, 0.2069, 0.2334, 0.0470, 0.0609, 0.0223, 0.0315],
        [0.1880, 0.1600, 0.1997, 0.0438, 0.1264, 0.0382, 0.2439],
        [0.7894, 0.0564, 0.0383, 0.0412, 0.0270, 0.0195, 0.0282],
        [0.2498, 0.1416, 0.0867, 0.0419, 0.3323, 0.0396, 0.1081],
        [0.1633, 0.0876, 0.0568, 0.2974, 0.2450, 0.1099, 0.0400]],
       grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1949, 0.0197, 0.1162, 0.1248, 0.0386, 0.1550, 0.3508],
        [0.2083, 0.0215, 0.0977, 0.0917, 0.0248, 0.1100, 0.4461]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1242, 0.0220, 0.0965, 0.1724, 0.0265, 0.0720, 0.4863],
        [0.0568, 0.0137, 0.0708, 0.0536, 0.0234, 0.0730, 0.7086],
        [0.2308, 0.0106, 0.0117, 0.1163, 0.0274, 0.1525, 0.4506]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1218, 0.0171, 0.0960, 0.1765, 0.0182, 0.0687, 0.5017],
        [0.0884, 0.0177, 0.0799, 0.0723, 0.0191, 0.0889, 0.6338],
        [0.2802, 0.0143, 0.0146, 0.1853, 0.0231, 0.1067, 0.3759],
        [0.1010, 0.0241, 0.0182, 0.2626, 0.0231, 0.2624, 0.3086]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.2244, 0.0304, 0.1480, 0.1039, 0.0163, 0.0713, 0.4057],
        [0.1130, 0.0272, 0.1364, 0.0945, 0.0143, 0.0806, 0.5342],
        [0.1467, 0.0292, 0.0265, 0.1391, 0.0164, 0.0854, 0.5566],
        [0.0598, 0.0325, 0.0239, 0.3506, 0.0191, 0.1737, 0.3405],
        [0.0373, 0.0246, 0.0182, 0.3588, 0.0235, 0.3539, 0.1836]],
       grad_fn=<SoftmaxBackward>)
#####################
08/02 08:53:30 PM | Train: [ 6/210] Step 000/404 Loss 3.937 Prec@(1,5) (12.5%, 37.5%)
08/02 08:54:16 PM | Train: [ 6/210] Step 010/404 Loss 3.519 Prec@(1,5) (23.0%, 49.7%)
08/02 08:55:00 PM | Train: [ 6/210] Step 020/404 Loss 3.578 Prec@(1,5) (22.6%, 48.1%)
08/02 08:55:47 PM | Train: [ 6/210] Step 030/404 Loss 3.594 Prec@(1,5) (22.7%, 46.7%)
08/02 08:56:33 PM | Train: [ 6/210] Step 040/404 Loss 3.634 Prec@(1,5) (21.6%, 46.7%)
08/02 08:57:17 PM | Train: [ 6/210] Step 050/404 Loss 3.637 Prec@(1,5) (21.5%, 46.4%)
08/02 08:58:00 PM | Train: [ 6/210] Step 060/404 Loss 3.639 Prec@(1,5) (21.2%, 46.6%)
08/02 08:58:48 PM | Train: [ 6/210] Step 070/404 Loss 3.655 Prec@(1,5) (21.3%, 46.0%)
08/02 08:59:31 PM | Train: [ 6/210] Step 080/404 Loss 3.648 Prec@(1,5) (21.3%, 46.3%)
08/02 09:00:14 PM | Train: [ 6/210] Step 090/404 Loss 3.620 Prec@(1,5) (21.9%, 46.7%)
08/02 09:01:03 PM | Train: [ 6/210] Step 100/404 Loss 3.604 Prec@(1,5) (22.2%, 47.0%)
08/02 09:01:48 PM | Train: [ 6/210] Step 110/404 Loss 3.606 Prec@(1,5) (22.2%, 47.0%)
08/02 09:02:30 PM | Train: [ 6/210] Step 120/404 Loss 3.596 Prec@(1,5) (22.3%, 47.2%)
08/02 09:03:13 PM | Train: [ 6/210] Step 130/404 Loss 3.597 Prec@(1,5) (22.4%, 47.4%)
08/02 09:03:57 PM | Train: [ 6/210] Step 140/404 Loss 3.600 Prec@(1,5) (22.3%, 47.5%)
08/02 09:04:41 PM | Train: [ 6/210] Step 150/404 Loss 3.592 Prec@(1,5) (22.4%, 47.5%)
08/02 09:05:27 PM | Train: [ 6/210] Step 160/404 Loss 3.587 Prec@(1,5) (22.4%, 47.7%)
08/02 09:06:12 PM | Train: [ 6/210] Step 170/404 Loss 3.579 Prec@(1,5) (22.6%, 47.6%)
08/02 09:06:56 PM | Train: [ 6/210] Step 180/404 Loss 3.576 Prec@(1,5) (22.5%, 47.6%)
08/02 09:07:41 PM | Train: [ 6/210] Step 190/404 Loss 3.576 Prec@(1,5) (22.6%, 47.4%)
08/02 09:08:25 PM | Train: [ 6/210] Step 200/404 Loss 3.575 Prec@(1,5) (22.6%, 47.4%)
08/02 09:09:09 PM | Train: [ 6/210] Step 210/404 Loss 3.581 Prec@(1,5) (22.5%, 47.3%)
08/02 09:09:52 PM | Train: [ 6/210] Step 220/404 Loss 3.578 Prec@(1,5) (22.5%, 47.5%)
08/02 09:10:37 PM | Train: [ 6/210] Step 230/404 Loss 3.574 Prec@(1,5) (22.6%, 47.7%)
08/02 09:11:21 PM | Train: [ 6/210] Step 240/404 Loss 3.578 Prec@(1,5) (22.7%, 47.7%)
08/02 09:12:06 PM | Train: [ 6/210] Step 250/404 Loss 3.573 Prec@(1,5) (22.7%, 47.8%)
08/02 09:12:52 PM | Train: [ 6/210] Step 260/404 Loss 3.570 Prec@(1,5) (22.8%, 47.8%)
08/02 09:13:39 PM | Train: [ 6/210] Step 270/404 Loss 3.568 Prec@(1,5) (22.8%, 47.9%)
08/02 09:14:24 PM | Train: [ 6/210] Step 280/404 Loss 3.571 Prec@(1,5) (22.8%, 47.8%)
08/02 09:15:11 PM | Train: [ 6/210] Step 290/404 Loss 3.562 Prec@(1,5) (22.9%, 48.0%)
08/02 09:15:55 PM | Train: [ 6/210] Step 300/404 Loss 3.553 Prec@(1,5) (23.0%, 48.1%)
08/02 09:16:41 PM | Train: [ 6/210] Step 310/404 Loss 3.546 Prec@(1,5) (23.1%, 48.3%)
08/02 09:17:26 PM | Train: [ 6/210] Step 320/404 Loss 3.541 Prec@(1,5) (23.2%, 48.5%)
08/02 09:18:08 PM | Train: [ 6/210] Step 330/404 Loss 3.536 Prec@(1,5) (23.3%, 48.5%)
08/02 09:18:53 PM | Train: [ 6/210] Step 340/404 Loss 3.529 Prec@(1,5) (23.4%, 48.7%)
08/02 09:19:38 PM | Train: [ 6/210] Step 350/404 Loss 3.530 Prec@(1,5) (23.4%, 48.6%)
08/02 09:20:25 PM | Train: [ 6/210] Step 360/404 Loss 3.520 Prec@(1,5) (23.5%, 48.8%)
08/02 09:21:14 PM | Train: [ 6/210] Step 370/404 Loss 3.514 Prec@(1,5) (23.6%, 48.9%)
08/02 09:21:58 PM | Train: [ 6/210] Step 380/404 Loss 3.513 Prec@(1,5) (23.6%, 49.0%)
08/02 09:22:41 PM | Train: [ 6/210] Step 390/404 Loss 3.502 Prec@(1,5) (23.8%, 49.2%)
08/02 09:23:28 PM | Train: [ 6/210] Step 400/404 Loss 3.496 Prec@(1,5) (23.9%, 49.3%)
08/02 09:23:52 PM | Train: [ 6/210] Final Prec@1 23.9867%
08/02 09:25:49 PM | Valid: [ 6/210] Step 000/023 Loss 4.095 Prec@(1,5) (28.1%, 53.1%)
08/02 09:25:52 PM | Valid: [ 6/210] Step 010/023 Loss 4.119 Prec@(1,5) (20.5%, 46.0%)
08/02 09:25:56 PM | Valid: [ 6/210] Step 020/023 Loss 4.101 Prec@(1,5) (20.2%, 44.9%)
08/02 09:26:06 PM | Valid: [ 6/210] Final Prec@1 20.7880%, Prec@5 45.1087%, Prec@10 61.0054%
08/02 09:26:06 PM | Final best Prec@1 = 20.7880%
08/02 09:26:07 PM | genotype = Genotype(normal=[[('avg_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 1), ('dil_conv_3x3', 0)], [('max_pool_3x3', 2), ('sep_conv_3x3', 0)], [('max_pool_3x3', 2), ('sep_conv_3x3', 3)], [('dil_conv_3x3', 4), ('sep_conv_3x3', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.2429, 0.3148, 0.1490, 0.0366, 0.0827, 0.1315, 0.0424],
        [0.0982, 0.1399, 0.1242, 0.1226, 0.1783, 0.2125, 0.1244]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.2209, 0.2505, 0.1875, 0.1303, 0.0901, 0.0467, 0.0740],
        [0.0515, 0.0558, 0.0579, 0.0765, 0.1818, 0.0583, 0.5182],
        [0.3763, 0.1485, 0.0631, 0.1318, 0.0977, 0.0893, 0.0933]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1407, 0.1223, 0.1195, 0.1132, 0.3269, 0.1038, 0.0736],
        [0.2588, 0.2213, 0.2379, 0.0611, 0.0550, 0.0799, 0.0859],
        [0.7404, 0.0756, 0.0464, 0.0515, 0.0227, 0.0261, 0.0372],
        [0.5584, 0.0898, 0.0487, 0.1219, 0.0519, 0.0288, 0.1006]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.4280, 0.1931, 0.2077, 0.0444, 0.0804, 0.0194, 0.0271],
        [0.1369, 0.1180, 0.1449, 0.0430, 0.1631, 0.0399, 0.3540],
        [0.8110, 0.0477, 0.0344, 0.0359, 0.0260, 0.0183, 0.0267],
        [0.2860, 0.1615, 0.0926, 0.0325, 0.2807, 0.0316, 0.1151],
        [0.1216, 0.0691, 0.0419, 0.4038, 0.2426, 0.0866, 0.0345]],
       grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1854, 0.0136, 0.1185, 0.1255, 0.0338, 0.1881, 0.3350],
        [0.2337, 0.0150, 0.1044, 0.0937, 0.0260, 0.0999, 0.4272]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1145, 0.0165, 0.0913, 0.1627, 0.0206, 0.0824, 0.5120],
        [0.0494, 0.0101, 0.0759, 0.0561, 0.0210, 0.0731, 0.7145],
        [0.3215, 0.0074, 0.0086, 0.1266, 0.0266, 0.1930, 0.3164]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1009, 0.0120, 0.0895, 0.2045, 0.0160, 0.0710, 0.5060],
        [0.0808, 0.0135, 0.0903, 0.0734, 0.0172, 0.1021, 0.6228],
        [0.3441, 0.0095, 0.0103, 0.2016, 0.0204, 0.1012, 0.3129],
        [0.1173, 0.0185, 0.0136, 0.2990, 0.0227, 0.2890, 0.2398]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.2146, 0.0238, 0.1909, 0.1099, 0.0146, 0.0593, 0.3869],
        [0.1443, 0.0275, 0.1310, 0.1059, 0.0126, 0.0792, 0.4995],
        [0.1942, 0.0252, 0.0237, 0.1636, 0.0144, 0.0815, 0.4975],
        [0.0627, 0.0312, 0.0221, 0.4079, 0.0166, 0.1943, 0.2652],
        [0.0358, 0.0224, 0.0150, 0.3859, 0.0193, 0.3966, 0.1250]],
       grad_fn=<SoftmaxBackward>)
#####################
08/02 09:29:45 PM | Train: [ 7/210] Step 000/404 Loss 3.290 Prec@(1,5) (34.4%, 50.0%)
08/02 09:30:37 PM | Train: [ 7/210] Step 010/404 Loss 3.140 Prec@(1,5) (30.7%, 56.0%)
08/02 09:31:23 PM | Train: [ 7/210] Step 020/404 Loss 3.172 Prec@(1,5) (29.8%, 54.5%)
08/02 09:32:09 PM | Train: [ 7/210] Step 030/404 Loss 3.173 Prec@(1,5) (29.8%, 54.7%)
08/02 09:32:54 PM | Train: [ 7/210] Step 040/404 Loss 3.231 Prec@(1,5) (28.7%, 54.5%)
08/02 09:33:36 PM | Train: [ 7/210] Step 050/404 Loss 3.240 Prec@(1,5) (28.1%, 53.9%)
08/02 09:34:21 PM | Train: [ 7/210] Step 060/404 Loss 3.246 Prec@(1,5) (27.6%, 53.5%)
08/02 09:35:04 PM | Train: [ 7/210] Step 070/404 Loss 3.274 Prec@(1,5) (27.4%, 53.2%)
08/02 09:35:49 PM | Train: [ 7/210] Step 080/404 Loss 3.287 Prec@(1,5) (27.0%, 53.0%)
08/02 09:36:34 PM | Train: [ 7/210] Step 090/404 Loss 3.263 Prec@(1,5) (27.3%, 53.8%)
08/02 09:37:19 PM | Train: [ 7/210] Step 100/404 Loss 3.253 Prec@(1,5) (27.4%, 54.1%)
08/02 09:38:03 PM | Train: [ 7/210] Step 110/404 Loss 3.252 Prec@(1,5) (27.5%, 53.9%)
08/02 09:38:50 PM | Train: [ 7/210] Step 120/404 Loss 3.238 Prec@(1,5) (27.9%, 54.2%)
08/02 09:39:34 PM | Train: [ 7/210] Step 130/404 Loss 3.237 Prec@(1,5) (28.0%, 54.5%)
08/02 09:40:19 PM | Train: [ 7/210] Step 140/404 Loss 3.243 Prec@(1,5) (28.0%, 54.5%)
08/02 09:41:06 PM | Train: [ 7/210] Step 150/404 Loss 3.234 Prec@(1,5) (28.3%, 54.7%)
08/02 09:41:50 PM | Train: [ 7/210] Step 160/404 Loss 3.227 Prec@(1,5) (28.2%, 54.9%)
08/02 09:42:35 PM | Train: [ 7/210] Step 170/404 Loss 3.219 Prec@(1,5) (28.4%, 55.0%)
08/02 09:43:21 PM | Train: [ 7/210] Step 180/404 Loss 3.218 Prec@(1,5) (28.4%, 54.9%)
08/02 09:44:05 PM | Train: [ 7/210] Step 190/404 Loss 3.216 Prec@(1,5) (28.5%, 54.9%)
08/02 09:44:49 PM | Train: [ 7/210] Step 200/404 Loss 3.212 Prec@(1,5) (28.6%, 55.1%)
08/02 09:45:34 PM | Train: [ 7/210] Step 210/404 Loss 3.217 Prec@(1,5) (28.5%, 55.1%)
08/02 09:46:17 PM | Train: [ 7/210] Step 220/404 Loss 3.215 Prec@(1,5) (28.5%, 55.2%)
08/02 09:47:01 PM | Train: [ 7/210] Step 230/404 Loss 3.212 Prec@(1,5) (28.4%, 55.3%)
08/02 09:47:46 PM | Train: [ 7/210] Step 240/404 Loss 3.219 Prec@(1,5) (28.4%, 55.2%)
08/02 09:48:33 PM | Train: [ 7/210] Step 250/404 Loss 3.219 Prec@(1,5) (28.3%, 55.2%)
08/02 09:49:19 PM | Train: [ 7/210] Step 260/404 Loss 3.215 Prec@(1,5) (28.3%, 55.3%)
08/02 09:50:06 PM | Train: [ 7/210] Step 270/404 Loss 3.211 Prec@(1,5) (28.4%, 55.4%)
08/02 09:50:58 PM | Train: [ 7/210] Step 280/404 Loss 3.215 Prec@(1,5) (28.3%, 55.2%)
08/02 09:51:44 PM | Train: [ 7/210] Step 290/404 Loss 3.205 Prec@(1,5) (28.4%, 55.4%)
08/02 09:52:27 PM | Train: [ 7/210] Step 300/404 Loss 3.198 Prec@(1,5) (28.6%, 55.6%)
08/02 09:53:14 PM | Train: [ 7/210] Step 310/404 Loss 3.192 Prec@(1,5) (28.5%, 55.7%)
08/02 09:53:58 PM | Train: [ 7/210] Step 320/404 Loss 3.188 Prec@(1,5) (28.7%, 55.8%)
08/02 09:54:44 PM | Train: [ 7/210] Step 330/404 Loss 3.184 Prec@(1,5) (28.7%, 55.9%)
08/02 09:55:31 PM | Train: [ 7/210] Step 340/404 Loss 3.178 Prec@(1,5) (28.8%, 55.9%)
08/02 09:56:18 PM | Train: [ 7/210] Step 350/404 Loss 3.179 Prec@(1,5) (28.8%, 55.9%)
08/02 09:57:00 PM | Train: [ 7/210] Step 360/404 Loss 3.170 Prec@(1,5) (29.1%, 56.2%)
08/02 09:57:43 PM | Train: [ 7/210] Step 370/404 Loss 3.164 Prec@(1,5) (29.2%, 56.3%)
08/02 09:58:27 PM | Train: [ 7/210] Step 380/404 Loss 3.162 Prec@(1,5) (29.2%, 56.3%)
08/02 09:59:11 PM | Train: [ 7/210] Step 390/404 Loss 3.151 Prec@(1,5) (29.4%, 56.5%)
08/02 09:59:56 PM | Train: [ 7/210] Step 400/404 Loss 3.146 Prec@(1,5) (29.6%, 56.7%)
08/02 10:00:16 PM | Train: [ 7/210] Final Prec@1 29.6024%
08/02 10:02:13 PM | Valid: [ 7/210] Step 000/023 Loss 4.036 Prec@(1,5) (25.0%, 59.4%)
08/02 10:02:16 PM | Valid: [ 7/210] Step 010/023 Loss 3.962 Prec@(1,5) (21.6%, 51.7%)
08/02 10:02:19 PM | Valid: [ 7/210] Step 020/023 Loss 4.041 Prec@(1,5) (21.3%, 50.9%)
08/02 10:02:24 PM | Valid: [ 7/210] Final Prec@1 21.4674%, Prec@5 51.0870%, Prec@10 66.4402%
08/02 10:02:25 PM | Final best Prec@1 = 21.4674%
08/02 10:02:25 PM | genotype = Genotype(normal=[[('avg_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 1), ('dil_conv_3x3', 0)], [('max_pool_3x3', 2), ('sep_conv_3x3', 0)], [('max_pool_3x3', 2), ('sep_conv_3x3', 3)], [('sep_conv_3x3', 4), ('sep_conv_3x3', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.2235, 0.2763, 0.1233, 0.0433, 0.1123, 0.1696, 0.0517],
        [0.0990, 0.1479, 0.1331, 0.1346, 0.1404, 0.2460, 0.0991]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.2658, 0.2391, 0.1760, 0.0979, 0.0895, 0.0524, 0.0794],
        [0.0583, 0.0613, 0.0631, 0.0833, 0.1592, 0.0642, 0.5106],
        [0.4290, 0.1402, 0.0630, 0.0920, 0.0841, 0.0875, 0.1044]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1364, 0.1090, 0.1060, 0.1023, 0.3242, 0.1306, 0.0915],
        [0.3122, 0.2140, 0.2245, 0.0496, 0.0482, 0.0919, 0.0594],
        [0.7609, 0.0568, 0.0426, 0.0577, 0.0233, 0.0272, 0.0315],
        [0.5458, 0.0754, 0.0522, 0.1300, 0.0563, 0.0322, 0.1081]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.4620, 0.1949, 0.2028, 0.0372, 0.0602, 0.0164, 0.0264],
        [0.1381, 0.1303, 0.1599, 0.0425, 0.1719, 0.0400, 0.3173],
        [0.8019, 0.0468, 0.0369, 0.0413, 0.0288, 0.0163, 0.0280],
        [0.2882, 0.1645, 0.0890, 0.0340, 0.2700, 0.0322, 0.1221],
        [0.0988, 0.0593, 0.0361, 0.4297, 0.2497, 0.0958, 0.0307]],
       grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1896, 0.0106, 0.1126, 0.1243, 0.0362, 0.2012, 0.3255],
        [0.2738, 0.0109, 0.1022, 0.1015, 0.0251, 0.0988, 0.3876]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1040, 0.0133, 0.0904, 0.1889, 0.0201, 0.0803, 0.5030],
        [0.0450, 0.0083, 0.0809, 0.0662, 0.0191, 0.0921, 0.6883],
        [0.3901, 0.0052, 0.0062, 0.1402, 0.0273, 0.2447, 0.1863]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0843, 0.0098, 0.0854, 0.2059, 0.0159, 0.0683, 0.5305],
        [0.0670, 0.0102, 0.0976, 0.0750, 0.0150, 0.1020, 0.6332],
        [0.3660, 0.0069, 0.0080, 0.2376, 0.0212, 0.1105, 0.2498],
        [0.1487, 0.0156, 0.0109, 0.3424, 0.0223, 0.2940, 0.1661]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.2136, 0.0213, 0.2245, 0.1479, 0.0130, 0.0523, 0.3274],
        [0.1557, 0.0254, 0.1260, 0.1293, 0.0115, 0.0892, 0.4629],
        [0.2494, 0.0225, 0.0213, 0.2105, 0.0136, 0.0757, 0.4069],
        [0.0593, 0.0282, 0.0195, 0.4575, 0.0150, 0.2385, 0.1821],
        [0.0280, 0.0180, 0.0114, 0.4464, 0.0161, 0.4049, 0.0751]],
       grad_fn=<SoftmaxBackward>)
#####################
08/02 10:06:01 PM | Train: [ 8/210] Step 000/404 Loss 2.810 Prec@(1,5) (40.6%, 65.6%)
08/02 10:06:45 PM | Train: [ 8/210] Step 010/404 Loss 2.834 Prec@(1,5) (36.4%, 62.8%)
08/02 10:07:29 PM | Train: [ 8/210] Step 020/404 Loss 2.884 Prec@(1,5) (34.1%, 61.2%)
08/02 10:08:14 PM | Train: [ 8/210] Step 030/404 Loss 2.884 Prec@(1,5) (34.1%, 61.3%)
08/02 10:08:59 PM | Train: [ 8/210] Step 040/404 Loss 2.896 Prec@(1,5) (33.5%, 61.4%)
08/02 10:09:42 PM | Train: [ 8/210] Step 050/404 Loss 2.891 Prec@(1,5) (33.8%, 61.3%)
08/02 10:10:28 PM | Train: [ 8/210] Step 060/404 Loss 2.899 Prec@(1,5) (33.2%, 61.6%)
08/02 10:11:12 PM | Train: [ 8/210] Step 070/404 Loss 2.918 Prec@(1,5) (33.2%, 61.0%)
08/02 10:12:00 PM | Train: [ 8/210] Step 080/404 Loss 2.937 Prec@(1,5) (32.8%, 60.6%)
08/02 10:12:48 PM | Train: [ 8/210] Step 090/404 Loss 2.927 Prec@(1,5) (33.0%, 61.0%)
08/02 10:13:34 PM | Train: [ 8/210] Step 100/404 Loss 2.922 Prec@(1,5) (33.3%, 61.1%)
08/02 10:14:19 PM | Train: [ 8/210] Step 110/404 Loss 2.923 Prec@(1,5) (33.4%, 60.9%)
08/02 10:15:04 PM | Train: [ 8/210] Step 120/404 Loss 2.901 Prec@(1,5) (34.0%, 61.4%)
08/02 10:15:49 PM | Train: [ 8/210] Step 130/404 Loss 2.901 Prec@(1,5) (34.0%, 61.6%)
08/02 10:16:32 PM | Train: [ 8/210] Step 140/404 Loss 2.907 Prec@(1,5) (34.0%, 61.5%)
08/02 10:17:18 PM | Train: [ 8/210] Step 150/404 Loss 2.902 Prec@(1,5) (34.4%, 61.5%)
08/02 10:18:02 PM | Train: [ 8/210] Step 160/404 Loss 2.897 Prec@(1,5) (34.4%, 61.6%)
08/02 10:18:47 PM | Train: [ 8/210] Step 170/404 Loss 2.895 Prec@(1,5) (34.4%, 61.6%)
08/02 10:19:33 PM | Train: [ 8/210] Step 180/404 Loss 2.894 Prec@(1,5) (34.2%, 61.7%)
08/02 10:20:20 PM | Train: [ 8/210] Step 190/404 Loss 2.890 Prec@(1,5) (34.2%, 61.8%)
08/02 10:21:05 PM | Train: [ 8/210] Step 200/404 Loss 2.888 Prec@(1,5) (34.2%, 61.8%)
08/02 10:21:50 PM | Train: [ 8/210] Step 210/404 Loss 2.893 Prec@(1,5) (34.0%, 61.8%)
08/02 10:22:36 PM | Train: [ 8/210] Step 220/404 Loss 2.889 Prec@(1,5) (34.1%, 62.0%)
08/02 10:23:21 PM | Train: [ 8/210] Step 230/404 Loss 2.885 Prec@(1,5) (34.1%, 62.0%)
08/02 10:24:07 PM | Train: [ 8/210] Step 240/404 Loss 2.893 Prec@(1,5) (34.0%, 62.0%)
08/02 10:24:54 PM | Train: [ 8/210] Step 250/404 Loss 2.895 Prec@(1,5) (34.0%, 61.9%)
08/02 10:25:46 PM | Train: [ 8/210] Step 260/404 Loss 2.894 Prec@(1,5) (34.1%, 61.8%)
08/02 10:26:30 PM | Train: [ 8/210] Step 270/404 Loss 2.889 Prec@(1,5) (34.2%, 62.0%)
08/02 10:27:18 PM | Train: [ 8/210] Step 280/404 Loss 2.891 Prec@(1,5) (34.2%, 62.0%)
08/02 10:28:01 PM | Train: [ 8/210] Step 290/404 Loss 2.883 Prec@(1,5) (34.3%, 62.1%)
08/02 10:28:48 PM | Train: [ 8/210] Step 300/404 Loss 2.878 Prec@(1,5) (34.4%, 62.2%)
08/02 10:29:33 PM | Train: [ 8/210] Step 310/404 Loss 2.870 Prec@(1,5) (34.5%, 62.4%)
08/02 10:30:18 PM | Train: [ 8/210] Step 320/404 Loss 2.868 Prec@(1,5) (34.5%, 62.5%)
08/02 10:31:06 PM | Train: [ 8/210] Step 330/404 Loss 2.862 Prec@(1,5) (34.6%, 62.6%)
08/02 10:31:50 PM | Train: [ 8/210] Step 340/404 Loss 2.855 Prec@(1,5) (34.8%, 62.7%)
08/02 10:32:37 PM | Train: [ 8/210] Step 350/404 Loss 2.855 Prec@(1,5) (34.7%, 62.7%)
08/02 10:33:20 PM | Train: [ 8/210] Step 360/404 Loss 2.848 Prec@(1,5) (34.8%, 62.9%)
08/02 10:34:05 PM | Train: [ 8/210] Step 370/404 Loss 2.842 Prec@(1,5) (34.9%, 62.9%)
08/02 10:34:50 PM | Train: [ 8/210] Step 380/404 Loss 2.840 Prec@(1,5) (35.0%, 63.0%)
08/02 10:35:36 PM | Train: [ 8/210] Step 390/404 Loss 2.831 Prec@(1,5) (35.2%, 63.1%)
08/02 10:36:23 PM | Train: [ 8/210] Step 400/404 Loss 2.826 Prec@(1,5) (35.3%, 63.3%)
08/02 10:36:48 PM | Train: [ 8/210] Final Prec@1 35.2645%
08/02 10:38:35 PM | Valid: [ 8/210] Step 000/023 Loss 3.764 Prec@(1,5) (28.1%, 59.4%)
08/02 10:38:38 PM | Valid: [ 8/210] Step 010/023 Loss 3.864 Prec@(1,5) (25.0%, 58.0%)
08/02 10:38:41 PM | Valid: [ 8/210] Step 020/023 Loss 3.941 Prec@(1,5) (23.5%, 53.4%)
08/02 10:38:48 PM | Valid: [ 8/210] Final Prec@1 23.5054%, Prec@5 53.3967%, Prec@10 66.4402%
08/02 10:38:48 PM | Final best Prec@1 = 23.5054%
08/02 10:38:48 PM | genotype = Genotype(normal=[[('avg_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('sep_conv_3x3', 0)], [('max_pool_3x3', 2), ('sep_conv_3x3', 3)], [('sep_conv_3x3', 4), ('sep_conv_3x3', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.2280, 0.3319, 0.1276, 0.0340, 0.0872, 0.1489, 0.0423],
        [0.1026, 0.1668, 0.1529, 0.1228, 0.1357, 0.2170, 0.1023]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.2788, 0.2374, 0.1686, 0.0989, 0.0979, 0.0449, 0.0734],
        [0.0453, 0.0468, 0.0484, 0.0661, 0.1300, 0.0527, 0.6107],
        [0.4311, 0.1352, 0.0569, 0.0943, 0.0950, 0.0801, 0.1074]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1616, 0.1273, 0.1222, 0.1115, 0.2584, 0.1264, 0.0927],
        [0.3234, 0.2174, 0.2357, 0.0442, 0.0424, 0.0850, 0.0520],
        [0.7274, 0.0575, 0.0474, 0.0855, 0.0232, 0.0268, 0.0321],
        [0.5559, 0.0766, 0.0444, 0.1240, 0.0483, 0.0345, 0.1163]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.4786, 0.2002, 0.1893, 0.0344, 0.0567, 0.0156, 0.0254],
        [0.1394, 0.1388, 0.1754, 0.0420, 0.1650, 0.0357, 0.3036],
        [0.7748, 0.0505, 0.0422, 0.0513, 0.0289, 0.0185, 0.0337],
        [0.3038, 0.1774, 0.0888, 0.0323, 0.2392, 0.0331, 0.1254],
        [0.0793, 0.0501, 0.0304, 0.5041, 0.2282, 0.0810, 0.0269]],
       grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.2083, 0.0093, 0.1173, 0.1248, 0.0364, 0.1947, 0.3092],
        [0.3168, 0.0079, 0.0952, 0.1256, 0.0261, 0.0918, 0.3366]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0845, 0.0104, 0.0910, 0.1889, 0.0195, 0.0784, 0.5273],
        [0.0437, 0.0072, 0.0753, 0.0675, 0.0184, 0.0999, 0.6881],
        [0.4202, 0.0041, 0.0048, 0.1400, 0.0219, 0.3020, 0.1071]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0723, 0.0086, 0.0954, 0.2205, 0.0152, 0.0649, 0.5230],
        [0.0579, 0.0084, 0.0948, 0.0755, 0.0130, 0.1033, 0.6472],
        [0.3953, 0.0055, 0.0065, 0.2643, 0.0189, 0.1446, 0.1649],
        [0.1588, 0.0119, 0.0084, 0.3741, 0.0206, 0.3166, 0.1097]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.2068, 0.0189, 0.2377, 0.1724, 0.0111, 0.0620, 0.2911],
        [0.1497, 0.0215, 0.1266, 0.1377, 0.0105, 0.1017, 0.4524],
        [0.3023, 0.0198, 0.0171, 0.2746, 0.0118, 0.0771, 0.2974],
        [0.0573, 0.0271, 0.0176, 0.5033, 0.0127, 0.2561, 0.1260],
        [0.0189, 0.0129, 0.0082, 0.4883, 0.0127, 0.4174, 0.0416]],
       grad_fn=<SoftmaxBackward>)
#####################
08/02 10:42:23 PM | Train: [ 9/210] Step 000/404 Loss 2.540 Prec@(1,5) (50.0%, 68.8%)
08/02 10:43:07 PM | Train: [ 9/210] Step 010/404 Loss 2.584 Prec@(1,5) (40.9%, 68.2%)
08/02 10:43:51 PM | Train: [ 9/210] Step 020/404 Loss 2.602 Prec@(1,5) (40.6%, 67.4%)
08/02 10:44:38 PM | Train: [ 9/210] Step 030/404 Loss 2.604 Prec@(1,5) (39.4%, 67.4%)
08/02 10:45:23 PM | Train: [ 9/210] Step 040/404 Loss 2.599 Prec@(1,5) (39.8%, 67.5%)
08/02 10:46:06 PM | Train: [ 9/210] Step 050/404 Loss 2.582 Prec@(1,5) (40.0%, 68.2%)
08/02 10:46:50 PM | Train: [ 9/210] Step 060/404 Loss 2.586 Prec@(1,5) (39.5%, 68.1%)
08/02 10:47:34 PM | Train: [ 9/210] Step 070/404 Loss 2.595 Prec@(1,5) (39.6%, 67.8%)
08/02 10:48:20 PM | Train: [ 9/210] Step 080/404 Loss 2.615 Prec@(1,5) (39.1%, 67.3%)
08/02 10:49:05 PM | Train: [ 9/210] Step 090/404 Loss 2.602 Prec@(1,5) (39.3%, 67.7%)
08/02 10:49:50 PM | Train: [ 9/210] Step 100/404 Loss 2.593 Prec@(1,5) (39.4%, 67.7%)
08/02 10:50:40 PM | Train: [ 9/210] Step 110/404 Loss 2.604 Prec@(1,5) (39.2%, 67.3%)
08/02 10:51:27 PM | Train: [ 9/210] Step 120/404 Loss 2.588 Prec@(1,5) (39.5%, 67.6%)
08/02 10:52:12 PM | Train: [ 9/210] Step 130/404 Loss 2.587 Prec@(1,5) (39.4%, 67.7%)
08/02 10:52:55 PM | Train: [ 9/210] Step 140/404 Loss 2.596 Prec@(1,5) (39.3%, 67.5%)
08/02 10:53:42 PM | Train: [ 9/210] Step 150/404 Loss 2.590 Prec@(1,5) (39.5%, 67.5%)
08/02 10:54:29 PM | Train: [ 9/210] Step 160/404 Loss 2.581 Prec@(1,5) (39.7%, 67.7%)
08/02 10:55:15 PM | Train: [ 9/210] Step 170/404 Loss 2.579 Prec@(1,5) (39.5%, 67.7%)
08/02 10:56:00 PM | Train: [ 9/210] Step 180/404 Loss 2.579 Prec@(1,5) (39.5%, 67.7%)
08/02 10:56:43 PM | Train: [ 9/210] Step 190/404 Loss 2.575 Prec@(1,5) (39.6%, 67.8%)
08/02 10:57:26 PM | Train: [ 9/210] Step 200/404 Loss 2.574 Prec@(1,5) (39.6%, 67.9%)
08/02 10:58:11 PM | Train: [ 9/210] Step 210/404 Loss 2.579 Prec@(1,5) (39.5%, 67.7%)
08/02 10:58:55 PM | Train: [ 9/210] Step 220/404 Loss 2.574 Prec@(1,5) (39.6%, 67.9%)
08/02 10:59:41 PM | Train: [ 9/210] Step 230/404 Loss 2.569 Prec@(1,5) (39.7%, 68.0%)
08/02 11:00:29 PM | Train: [ 9/210] Step 240/404 Loss 2.578 Prec@(1,5) (39.5%, 67.9%)
08/02 11:01:19 PM | Train: [ 9/210] Step 250/404 Loss 2.579 Prec@(1,5) (39.5%, 67.8%)
08/02 11:02:03 PM | Train: [ 9/210] Step 260/404 Loss 2.579 Prec@(1,5) (39.5%, 67.8%)
08/02 11:02:47 PM | Train: [ 9/210] Step 270/404 Loss 2.574 Prec@(1,5) (39.6%, 67.9%)
08/02 11:03:33 PM | Train: [ 9/210] Step 280/404 Loss 2.577 Prec@(1,5) (39.5%, 67.8%)
08/02 11:04:18 PM | Train: [ 9/210] Step 290/404 Loss 2.567 Prec@(1,5) (39.6%, 68.1%)
08/02 11:05:06 PM | Train: [ 9/210] Step 300/404 Loss 2.564 Prec@(1,5) (39.8%, 68.1%)
08/02 11:05:53 PM | Train: [ 9/210] Step 310/404 Loss 2.560 Prec@(1,5) (39.8%, 68.2%)
08/02 11:06:38 PM | Train: [ 9/210] Step 320/404 Loss 2.556 Prec@(1,5) (39.9%, 68.3%)
08/02 11:07:22 PM | Train: [ 9/210] Step 330/404 Loss 2.551 Prec@(1,5) (40.0%, 68.4%)
08/02 11:08:08 PM | Train: [ 9/210] Step 340/404 Loss 2.544 Prec@(1,5) (40.2%, 68.5%)
08/02 11:08:52 PM | Train: [ 9/210] Step 350/404 Loss 2.543 Prec@(1,5) (40.1%, 68.5%)
08/02 11:09:38 PM | Train: [ 9/210] Step 360/404 Loss 2.536 Prec@(1,5) (40.2%, 68.7%)
08/02 11:10:23 PM | Train: [ 9/210] Step 370/404 Loss 2.533 Prec@(1,5) (40.3%, 68.9%)
08/02 11:11:08 PM | Train: [ 9/210] Step 380/404 Loss 2.532 Prec@(1,5) (40.3%, 68.9%)
08/02 11:11:53 PM | Train: [ 9/210] Step 390/404 Loss 2.524 Prec@(1,5) (40.5%, 69.0%)
08/02 11:12:38 PM | Train: [ 9/210] Step 400/404 Loss 2.519 Prec@(1,5) (40.7%, 69.1%)
08/02 11:13:00 PM | Train: [ 9/210] Final Prec@1 40.7410%
08/02 11:14:51 PM | Valid: [ 9/210] Step 000/023 Loss 3.428 Prec@(1,5) (25.0%, 62.5%)
08/02 11:14:54 PM | Valid: [ 9/210] Step 010/023 Loss 3.727 Prec@(1,5) (28.1%, 59.9%)
08/02 11:14:57 PM | Valid: [ 9/210] Step 020/023 Loss 3.866 Prec@(1,5) (25.3%, 56.2%)
08/02 11:15:01 PM | Valid: [ 9/210] Final Prec@1 25.6793%, Prec@5 56.3859%, Prec@10 70.6522%
08/02 11:15:02 PM | Final best Prec@1 = 25.6793%
08/02 11:15:02 PM | genotype = Genotype(normal=[[('avg_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('sep_conv_3x3', 0)], [('max_pool_3x3', 2), ('sep_conv_3x3', 3)], [('sep_conv_3x3', 4), ('sep_conv_3x3', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1937, 0.3712, 0.1280, 0.0345, 0.0914, 0.1452, 0.0359],
        [0.0930, 0.1671, 0.1536, 0.1306, 0.1236, 0.2202, 0.1121]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.2903, 0.2739, 0.1849, 0.0686, 0.0940, 0.0371, 0.0513],
        [0.0342, 0.0366, 0.0373, 0.0561, 0.1265, 0.0522, 0.6572],
        [0.4164, 0.1334, 0.0509, 0.0940, 0.0810, 0.0872, 0.1371]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1767, 0.1438, 0.1367, 0.0822, 0.2334, 0.1356, 0.0917],
        [0.3230, 0.2361, 0.2431, 0.0404, 0.0382, 0.0729, 0.0463],
        [0.7398, 0.0524, 0.0428, 0.0827, 0.0217, 0.0299, 0.0308],
        [0.5534, 0.0722, 0.0498, 0.0939, 0.0523, 0.0334, 0.1451]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.4985, 0.2023, 0.1769, 0.0301, 0.0551, 0.0153, 0.0218],
        [0.1151, 0.1212, 0.1484, 0.0428, 0.1536, 0.0371, 0.3818],
        [0.8075, 0.0468, 0.0345, 0.0428, 0.0223, 0.0181, 0.0280],
        [0.3235, 0.1913, 0.0906, 0.0340, 0.2021, 0.0356, 0.1230],
        [0.0764, 0.0493, 0.0309, 0.4915, 0.2386, 0.0851, 0.0282]],
       grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.2334, 0.0084, 0.1273, 0.1360, 0.0328, 0.1855, 0.2766],
        [0.3301, 0.0067, 0.1122, 0.1190, 0.0262, 0.0905, 0.3153]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0741, 0.0088, 0.0942, 0.1975, 0.0193, 0.0819, 0.5242],
        [0.0367, 0.0061, 0.0899, 0.0706, 0.0212, 0.1160, 0.6596],
        [0.4824, 0.0033, 0.0038, 0.1359, 0.0201, 0.2887, 0.0657]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0643, 0.0081, 0.0983, 0.2170, 0.0128, 0.0668, 0.5328],
        [0.0521, 0.0077, 0.1076, 0.0768, 0.0121, 0.1200, 0.6236],
        [0.4283, 0.0048, 0.0058, 0.2742, 0.0172, 0.1623, 0.1075],
        [0.1694, 0.0096, 0.0066, 0.3732, 0.0223, 0.3562, 0.0628]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1933, 0.0163, 0.2701, 0.1927, 0.0099, 0.0598, 0.2579],
        [0.1689, 0.0214, 0.1194, 0.1513, 0.0096, 0.1033, 0.4262],
        [0.3426, 0.0165, 0.0143, 0.2968, 0.0108, 0.0806, 0.2384],
        [0.0466, 0.0215, 0.0134, 0.5474, 0.0105, 0.2811, 0.0794],
        [0.0114, 0.0081, 0.0054, 0.4806, 0.0094, 0.4631, 0.0221]],
       grad_fn=<SoftmaxBackward>)
#####################
08/02 11:18:47 PM | Train: [10/210] Step 000/404 Loss 2.564 Prec@(1,5) (40.6%, 71.9%)
08/02 11:19:33 PM | Train: [10/210] Step 010/404 Loss 2.225 Prec@(1,5) (48.3%, 76.7%)
08/02 11:20:20 PM | Train: [10/210] Step 020/404 Loss 2.262 Prec@(1,5) (45.4%, 73.2%)
08/02 11:21:04 PM | Train: [10/210] Step 030/404 Loss 2.278 Prec@(1,5) (45.5%, 72.9%)
08/02 11:21:52 PM | Train: [10/210] Step 040/404 Loss 2.293 Prec@(1,5) (45.1%, 72.4%)
08/02 11:22:38 PM | Train: [10/210] Step 050/404 Loss 2.290 Prec@(1,5) (45.0%, 72.6%)
08/02 11:23:23 PM | Train: [10/210] Step 060/404 Loss 2.294 Prec@(1,5) (45.2%, 72.5%)
08/02 11:24:07 PM | Train: [10/210] Step 070/404 Loss 2.307 Prec@(1,5) (45.4%, 72.4%)
08/02 11:24:51 PM | Train: [10/210] Step 080/404 Loss 2.334 Prec@(1,5) (44.8%, 72.3%)
08/02 11:25:37 PM | Train: [10/210] Step 090/404 Loss 2.320 Prec@(1,5) (45.0%, 73.0%)
08/02 11:26:21 PM | Train: [10/210] Step 100/404 Loss 2.317 Prec@(1,5) (45.1%, 73.1%)
08/02 11:27:06 PM | Train: [10/210] Step 110/404 Loss 2.337 Prec@(1,5) (44.7%, 72.6%)
08/02 11:27:50 PM | Train: [10/210] Step 120/404 Loss 2.325 Prec@(1,5) (44.8%, 72.8%)
08/02 11:28:41 PM | Train: [10/210] Step 130/404 Loss 2.326 Prec@(1,5) (44.8%, 72.8%)
08/02 11:29:25 PM | Train: [10/210] Step 140/404 Loss 2.334 Prec@(1,5) (44.7%, 72.5%)
08/02 11:30:08 PM | Train: [10/210] Step 150/404 Loss 2.328 Prec@(1,5) (44.7%, 72.4%)
08/02 11:30:55 PM | Train: [10/210] Step 160/404 Loss 2.320 Prec@(1,5) (44.9%, 72.6%)
08/02 11:31:39 PM | Train: [10/210] Step 170/404 Loss 2.321 Prec@(1,5) (44.8%, 72.6%)
08/02 11:32:23 PM | Train: [10/210] Step 180/404 Loss 2.321 Prec@(1,5) (44.9%, 72.4%)
08/02 11:33:08 PM | Train: [10/210] Step 190/404 Loss 2.321 Prec@(1,5) (45.0%, 72.4%)
08/02 11:33:57 PM | Train: [10/210] Step 200/404 Loss 2.318 Prec@(1,5) (44.9%, 72.5%)
08/02 11:34:44 PM | Train: [10/210] Step 210/404 Loss 2.322 Prec@(1,5) (44.7%, 72.5%)
08/02 11:35:27 PM | Train: [10/210] Step 220/404 Loss 2.316 Prec@(1,5) (44.7%, 72.6%)
08/02 11:36:11 PM | Train: [10/210] Step 230/404 Loss 2.308 Prec@(1,5) (45.0%, 72.8%)
08/02 11:36:59 PM | Train: [10/210] Step 240/404 Loss 2.317 Prec@(1,5) (45.0%, 72.7%)
08/02 11:37:47 PM | Train: [10/210] Step 250/404 Loss 2.317 Prec@(1,5) (45.1%, 72.6%)
08/02 11:38:32 PM | Train: [10/210] Step 260/404 Loss 2.318 Prec@(1,5) (45.0%, 72.7%)
08/02 11:39:17 PM | Train: [10/210] Step 270/404 Loss 2.315 Prec@(1,5) (45.2%, 72.8%)
08/02 11:40:04 PM | Train: [10/210] Step 280/404 Loss 2.316 Prec@(1,5) (45.1%, 72.9%)
08/02 11:40:49 PM | Train: [10/210] Step 290/404 Loss 2.306 Prec@(1,5) (45.3%, 73.1%)
08/02 11:41:35 PM | Train: [10/210] Step 300/404 Loss 2.303 Prec@(1,5) (45.3%, 73.1%)
08/02 11:42:21 PM | Train: [10/210] Step 310/404 Loss 2.297 Prec@(1,5) (45.4%, 73.1%)
08/02 11:43:08 PM | Train: [10/210] Step 320/404 Loss 2.295 Prec@(1,5) (45.5%, 73.2%)
08/02 11:43:51 PM | Train: [10/210] Step 330/404 Loss 2.291 Prec@(1,5) (45.6%, 73.2%)
08/02 11:44:38 PM | Train: [10/210] Step 340/404 Loss 2.283 Prec@(1,5) (45.7%, 73.3%)
08/02 11:45:25 PM | Train: [10/210] Step 350/404 Loss 2.282 Prec@(1,5) (45.7%, 73.3%)
08/02 11:46:09 PM | Train: [10/210] Step 360/404 Loss 2.276 Prec@(1,5) (45.8%, 73.5%)
08/02 11:46:56 PM | Train: [10/210] Step 370/404 Loss 2.272 Prec@(1,5) (45.9%, 73.6%)
08/02 11:47:45 PM | Train: [10/210] Step 380/404 Loss 2.271 Prec@(1,5) (46.0%, 73.6%)
08/02 11:48:29 PM | Train: [10/210] Step 390/404 Loss 2.264 Prec@(1,5) (46.1%, 73.8%)
08/02 11:49:17 PM | Train: [10/210] Step 400/404 Loss 2.263 Prec@(1,5) (46.2%, 73.8%)
08/02 11:49:38 PM | Train: [10/210] Final Prec@1 46.2639%
08/02 11:51:34 PM | Valid: [10/210] Step 000/023 Loss 2.814 Prec@(1,5) (40.6%, 68.8%)
08/02 11:51:37 PM | Valid: [10/210] Step 010/023 Loss 3.094 Prec@(1,5) (36.4%, 68.2%)
08/02 11:51:40 PM | Valid: [10/210] Step 020/023 Loss 3.278 Prec@(1,5) (33.5%, 65.0%)
08/02 11:51:45 PM | Valid: [10/210] Final Prec@1 33.8315%, Prec@5 65.3533%, Prec@10 78.5326%
08/02 11:51:45 PM | Final best Prec@1 = 33.8315%
08/02 11:51:45 PM | genotype = Genotype(normal=[[('avg_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('sep_conv_3x3', 0)], [('max_pool_3x3', 2), ('sep_conv_3x3', 3)], [('sep_conv_3x3', 4), ('sep_conv_3x3', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1928, 0.3748, 0.1194, 0.0382, 0.0977, 0.1419, 0.0352],
        [0.0931, 0.1744, 0.1571, 0.1194, 0.1271, 0.2177, 0.1110]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.2901, 0.2768, 0.1786, 0.0734, 0.0979, 0.0353, 0.0480],
        [0.0307, 0.0334, 0.0339, 0.0575, 0.1103, 0.0632, 0.6710],
        [0.3980, 0.1243, 0.0497, 0.1200, 0.0813, 0.0965, 0.1303]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.1750, 0.1429, 0.1342, 0.0895, 0.2423, 0.1313, 0.0847],
        [0.3377, 0.2496, 0.2435, 0.0360, 0.0301, 0.0623, 0.0409],
        [0.7691, 0.0448, 0.0340, 0.0778, 0.0199, 0.0237, 0.0307],
        [0.5831, 0.0646, 0.0417, 0.0840, 0.0425, 0.0278, 0.1565]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.4891, 0.1868, 0.1509, 0.0425, 0.0812, 0.0214, 0.0281],
        [0.0935, 0.0989, 0.1175, 0.0409, 0.1651, 0.0374, 0.4468],
        [0.8192, 0.0421, 0.0314, 0.0391, 0.0211, 0.0205, 0.0266],
        [0.3254, 0.1906, 0.0892, 0.0315, 0.2088, 0.0394, 0.1151],
        [0.0756, 0.0492, 0.0308, 0.5293, 0.2087, 0.0763, 0.0300]],
       grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.2364, 0.0080, 0.1299, 0.1333, 0.0298, 0.2194, 0.2432],
        [0.3592, 0.0058, 0.1074, 0.1295, 0.0248, 0.0972, 0.2761]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0718, 0.0085, 0.1055, 0.2062, 0.0207, 0.0845, 0.5028],
        [0.0286, 0.0049, 0.0743, 0.0679, 0.0205, 0.1112, 0.6925],
        [0.4867, 0.0028, 0.0032, 0.1467, 0.0188, 0.3034, 0.0384]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.0668, 0.0087, 0.1008, 0.2259, 0.0123, 0.0786, 0.5068],
        [0.0538, 0.0079, 0.1116, 0.0817, 0.0115, 0.1061, 0.6272],
        [0.4648, 0.0044, 0.0051, 0.2677, 0.0150, 0.1801, 0.0630],
        [0.1819, 0.0083, 0.0059, 0.3740, 0.0224, 0.3694, 0.0381]],
       grad_fn=<SoftmaxBackward>)
tensor([[0.2040, 0.0170, 0.3128, 0.1210, 0.0101, 0.0713, 0.2638],
        [0.1672, 0.0195, 0.1288, 0.1539, 0.0100, 0.0928, 0.4278],
        [0.3514, 0.0141, 0.0127, 0.3304, 0.0104, 0.0852, 0.1958],
        [0.0379, 0.0179, 0.0113, 0.5655, 0.0098, 0.3030, 0.0545],
        [0.0070, 0.0052, 0.0037, 0.4954, 0.0070, 0.4689, 0.0128]],
       grad_fn=<SoftmaxBackward>)
#####################
08/02 11:55:23 PM | Train: [11/210] Step 000/404 Loss 2.264 Prec@(1,5) (53.1%, 71.9%)
08/02 11:56:07 PM | Train: [11/210] Step 010/404 Loss 2.052 Prec@(1,5) (48.6%, 78.7%)
08/02 11:56:51 PM | Train: [11/210] Step 020/404 Loss 2.139 Prec@(1,5) (48.2%, 76.2%)
08/02 11:57:12 PM | Train: [11/210] Final Prec@1 48.1771%
08/02 11:58:54 PM | Valid: [11/210] Step 000/023 Loss 2.841 Prec@(1,5) (40.6%, 71.9%)
08/02 11:58:56 PM | Valid: [11/210] Step 010/023 Loss 3.097 Prec@(1,5) (35.5%, 64.5%)
08/02 11:58:59 PM | Valid: [11/210] Step 020/023 Loss 3.248 Prec@(1,5) (32.9%, 62.1%)
08/02 11:59:03 PM | Valid: [11/210] Final Prec@1 33.6957%, Prec@5 63.0435%, Prec@10 77.3098%
08/02 11:59:03 PM | Final best Prec@1 = 33.8315%
08/02 11:59:03 PM | genotype = Genotype(normal=[[('avg_pool_3x3', 0), ('partial_aware_1x1', 1)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 2), ('sep_conv_3x3', 0)], [('max_pool_3x3', 2), ('sep_conv_3x3', 3)], [('dil_conv_3x3', 4), ('sep_conv_3x3', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1531, 0.2531, 0.0849, 0.0524, 0.1399, 0.2337, 0.0828],
        [0.0878, 0.1527, 0.1402, 0.1442, 0.2408, 0.1612, 0.0730]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3050, 0.2651, 0.1726, 0.0674, 0.0807, 0.0405, 0.0686],
        [0.0429, 0.0451, 0.0461, 0.0833, 0.1853, 0.0570, 0.5403],
        [0.5216, 0.1459, 0.0563, 0.0561, 0.0605, 0.0714, 0.0880]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1337, 0.1080, 0.1019, 0.0814, 0.3248, 0.1531, 0.0971],
        [0.3056, 0.2148, 0.2201, 0.0623, 0.0525, 0.0972, 0.0476],
        [0.7356, 0.0433, 0.0370, 0.0844, 0.0232, 0.0423, 0.0342],
        [0.3689, 0.0468, 0.0402, 0.0827, 0.0652, 0.0206, 0.3756]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.6000, 0.1517, 0.1245, 0.0332, 0.0427, 0.0170, 0.0310],
        [0.1449, 0.1410, 0.1690, 0.0442, 0.1275, 0.0415, 0.3320],
        [0.9105, 0.0250, 0.0181, 0.0175, 0.0092, 0.0088, 0.0108],
        [0.3081, 0.1781, 0.0878, 0.0289, 0.2461, 0.0408, 0.1102],
        [0.1026, 0.0644, 0.0478, 0.3558, 0.2320, 0.1574, 0.0399]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.3231, 0.0096, 0.1398, 0.1217, 0.0378, 0.1463, 0.2217],
        [0.2640, 0.0045, 0.1108, 0.1629, 0.0321, 0.0794, 0.3462]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0687, 0.0079, 0.1090, 0.2212, 0.0233, 0.0935, 0.4765],
        [0.0207, 0.0035, 0.0398, 0.0779, 0.0157, 0.0694, 0.7730],
        [0.5563, 0.0028, 0.0035, 0.1297, 0.0257, 0.2406, 0.0414]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0808, 0.0103, 0.0708, 0.1862, 0.0120, 0.0563, 0.5835],
        [0.0356, 0.0055, 0.0792, 0.0748, 0.0108, 0.0887, 0.7055],
        [0.5168, 0.0043, 0.0061, 0.2532, 0.0209, 0.1181, 0.0806],
        [0.2049, 0.0084, 0.0065, 0.4022, 0.0220, 0.3149, 0.0412]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2430, 0.0195, 0.2662, 0.1005, 0.0109, 0.0552, 0.3047],
        [0.1053, 0.0133, 0.1341, 0.1325, 0.0103, 0.0847, 0.5198],
        [0.3136, 0.0132, 0.0167, 0.2466, 0.0141, 0.0811, 0.3147],
        [0.0397, 0.0189, 0.0142, 0.5649, 0.0144, 0.2781, 0.0698],
        [0.0066, 0.0049, 0.0037, 0.4641, 0.0087, 0.4984, 0.0136]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
08/03 12:02:28 AM | Train: [12/210] Step 000/404 Loss 1.645 Prec@(1,5) (62.5%, 87.5%)
08/03 12:03:14 AM | Train: [12/210] Step 010/404 Loss 1.064 Prec@(1,5) (74.1%, 93.2%)
08/03 12:04:00 AM | Train: [12/210] Step 020/404 Loss 1.043 Prec@(1,5) (75.0%, 93.3%)
08/03 12:04:48 AM | Train: [12/210] Step 030/404 Loss 1.295 Prec@(1,5) (69.3%, 89.1%)
08/03 12:05:35 AM | Train: [12/210] Step 040/404 Loss 1.515 Prec@(1,5) (64.1%, 85.3%)
08/03 12:06:19 AM | Train: [12/210] Step 050/404 Loss 1.625 Prec@(1,5) (60.5%, 83.9%)
08/03 12:07:04 AM | Train: [12/210] Step 060/404 Loss 1.728 Prec@(1,5) (57.8%, 82.2%)
08/03 12:07:48 AM | Train: [12/210] Step 070/404 Loss 1.791 Prec@(1,5) (56.4%, 81.2%)
08/03 12:08:32 AM | Train: [12/210] Step 080/404 Loss 1.865 Prec@(1,5) (54.6%, 80.2%)
08/03 12:09:18 AM | Train: [12/210] Step 090/404 Loss 1.887 Prec@(1,5) (54.0%, 80.1%)
08/03 12:10:02 AM | Train: [12/210] Step 100/404 Loss 1.907 Prec@(1,5) (53.7%, 79.4%)
08/03 12:10:49 AM | Train: [12/210] Step 110/404 Loss 1.937 Prec@(1,5) (52.8%, 78.9%)
08/03 12:11:33 AM | Train: [12/210] Step 120/404 Loss 1.937 Prec@(1,5) (52.9%, 79.0%)
08/03 12:12:17 AM | Train: [12/210] Step 130/404 Loss 1.955 Prec@(1,5) (52.6%, 78.6%)
08/03 12:13:02 AM | Train: [12/210] Step 140/404 Loss 1.970 Prec@(1,5) (52.3%, 78.2%)
08/03 12:13:48 AM | Train: [12/210] Step 150/404 Loss 1.975 Prec@(1,5) (52.2%, 78.1%)
08/03 12:14:31 AM | Train: [12/210] Step 160/404 Loss 1.974 Prec@(1,5) (52.2%, 78.1%)
08/03 12:15:16 AM | Train: [12/210] Step 170/404 Loss 1.978 Prec@(1,5) (52.0%, 78.0%)
08/03 12:16:04 AM | Train: [12/210] Step 180/404 Loss 1.984 Prec@(1,5) (51.9%, 77.9%)
08/03 12:16:50 AM | Train: [12/210] Step 190/404 Loss 1.995 Prec@(1,5) (51.8%, 77.9%)
08/03 12:17:35 AM | Train: [12/210] Step 200/404 Loss 1.998 Prec@(1,5) (51.6%, 77.9%)
08/03 12:18:25 AM | Train: [12/210] Step 210/404 Loss 2.004 Prec@(1,5) (51.4%, 77.8%)
08/03 12:19:15 AM | Train: [12/210] Step 220/404 Loss 2.002 Prec@(1,5) (51.4%, 77.9%)
08/03 12:20:04 AM | Train: [12/210] Step 230/404 Loss 2.000 Prec@(1,5) (51.6%, 78.0%)
08/03 12:20:48 AM | Train: [12/210] Step 240/404 Loss 2.009 Prec@(1,5) (51.3%, 77.9%)
08/03 12:21:33 AM | Train: [12/210] Step 250/404 Loss 2.013 Prec@(1,5) (51.3%, 77.9%)
08/03 12:22:20 AM | Train: [12/210] Step 260/404 Loss 2.015 Prec@(1,5) (51.2%, 77.9%)
08/03 12:23:05 AM | Train: [12/210] Step 270/404 Loss 2.017 Prec@(1,5) (51.2%, 77.8%)
08/03 12:23:49 AM | Train: [12/210] Step 280/404 Loss 2.021 Prec@(1,5) (51.1%, 77.8%)
08/03 12:24:35 AM | Train: [12/210] Step 290/404 Loss 2.013 Prec@(1,5) (51.2%, 78.0%)
08/03 12:25:24 AM | Train: [12/210] Step 300/404 Loss 2.013 Prec@(1,5) (51.2%, 78.0%)
08/03 12:26:10 AM | Train: [12/210] Step 310/404 Loss 2.009 Prec@(1,5) (51.2%, 78.0%)
08/03 12:26:53 AM | Train: [12/210] Step 320/404 Loss 2.009 Prec@(1,5) (51.3%, 78.0%)
08/03 12:27:39 AM | Train: [12/210] Step 330/404 Loss 2.006 Prec@(1,5) (51.4%, 78.0%)
08/03 12:28:26 AM | Train: [12/210] Step 340/404 Loss 2.001 Prec@(1,5) (51.4%, 78.1%)
08/03 12:29:12 AM | Train: [12/210] Step 350/404 Loss 2.000 Prec@(1,5) (51.4%, 78.1%)
08/03 12:29:58 AM | Train: [12/210] Step 360/404 Loss 1.997 Prec@(1,5) (51.4%, 78.3%)
08/03 12:30:48 AM | Train: [12/210] Step 370/404 Loss 1.995 Prec@(1,5) (51.4%, 78.3%)
08/03 12:31:33 AM | Train: [12/210] Step 380/404 Loss 1.997 Prec@(1,5) (51.4%, 78.2%)
08/03 12:32:24 AM | Train: [12/210] Step 390/404 Loss 1.992 Prec@(1,5) (51.6%, 78.3%)
08/03 12:33:11 AM | Train: [12/210] Step 400/404 Loss 1.992 Prec@(1,5) (51.6%, 78.3%)
08/03 12:33:36 AM | Train: [12/210] Final Prec@1 51.6399%
08/03 12:35:28 AM | Valid: [12/210] Step 000/023 Loss 3.281 Prec@(1,5) (31.2%, 68.8%)
08/03 12:35:31 AM | Valid: [12/210] Step 010/023 Loss 3.219 Prec@(1,5) (34.9%, 69.6%)
08/03 12:35:34 AM | Valid: [12/210] Step 020/023 Loss 3.483 Prec@(1,5) (33.5%, 64.4%)
08/03 12:35:42 AM | Valid: [12/210] Final Prec@1 33.1522%, Prec@5 64.5380%, Prec@10 76.9022%
08/03 12:35:42 AM | Final best Prec@1 = 33.8315%
08/03 12:35:42 AM | genotype = Genotype(normal=[[('avg_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('sep_conv_3x3', 0)], [('max_pool_3x3', 2), ('dil_conv_3x3', 3)], [('sep_conv_3x3', 4), ('sep_conv_3x3', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1897, 0.3374, 0.1094, 0.0359, 0.1072, 0.1685, 0.0519],
        [0.0956, 0.1816, 0.1616, 0.1337, 0.1395, 0.2127, 0.0754]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3015, 0.2512, 0.1593, 0.0951, 0.1032, 0.0373, 0.0525],
        [0.0337, 0.0355, 0.0357, 0.0729, 0.1210, 0.0772, 0.6240],
        [0.4620, 0.1151, 0.0446, 0.0803, 0.0817, 0.1075, 0.1089]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1498, 0.1187, 0.1119, 0.0804, 0.2637, 0.1821, 0.0934],
        [0.3512, 0.2321, 0.2290, 0.0448, 0.0311, 0.0742, 0.0376],
        [0.7650, 0.0384, 0.0299, 0.0890, 0.0222, 0.0293, 0.0262],
        [0.5385, 0.0601, 0.0575, 0.0752, 0.0437, 0.0272, 0.1977]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.5879, 0.1439, 0.1245, 0.0426, 0.0502, 0.0192, 0.0317],
        [0.1316, 0.1375, 0.1626, 0.0457, 0.1342, 0.0432, 0.3451],
        [0.8362, 0.0328, 0.0294, 0.0380, 0.0220, 0.0189, 0.0226],
        [0.3292, 0.1887, 0.0953, 0.0327, 0.1837, 0.0472, 0.1232],
        [0.0836, 0.0537, 0.0353, 0.4444, 0.2422, 0.1019, 0.0389]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.2777, 0.0078, 0.1310, 0.1413, 0.0280, 0.1989, 0.2153],
        [0.3743, 0.0049, 0.1083, 0.1224, 0.0248, 0.1086, 0.2567]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0599, 0.0072, 0.1122, 0.2128, 0.0199, 0.0759, 0.5120],
        [0.0212, 0.0041, 0.0834, 0.0671, 0.0188, 0.1025, 0.7029],
        [0.5065, 0.0026, 0.0030, 0.1387, 0.0183, 0.3045, 0.0263]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0587, 0.0081, 0.1126, 0.2270, 0.0129, 0.0885, 0.4922],
        [0.0404, 0.0066, 0.1084, 0.1021, 0.0110, 0.1007, 0.6308],
        [0.4786, 0.0039, 0.0047, 0.2671, 0.0140, 0.1831, 0.0486],
        [0.1781, 0.0066, 0.0048, 0.3798, 0.0194, 0.3881, 0.0233]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1864, 0.0154, 0.3414, 0.0895, 0.0101, 0.0786, 0.2784],
        [0.1556, 0.0181, 0.1238, 0.1603, 0.0106, 0.0874, 0.4443],
        [0.3621, 0.0127, 0.0124, 0.3170, 0.0108, 0.0934, 0.1916],
        [0.0319, 0.0154, 0.0105, 0.5634, 0.0092, 0.3258, 0.0438],
        [0.0045, 0.0036, 0.0029, 0.4881, 0.0063, 0.4865, 0.0081]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
08/03 12:39:16 AM | Train: [13/210] Step 000/404 Loss 1.659 Prec@(1,5) (65.6%, 81.2%)
08/03 12:39:59 AM | Train: [13/210] Step 010/404 Loss 1.561 Prec@(1,5) (62.2%, 85.8%)
08/03 12:40:45 AM | Train: [13/210] Step 020/404 Loss 1.586 Prec@(1,5) (62.1%, 85.0%)
08/03 12:41:31 AM | Train: [13/210] Step 030/404 Loss 1.654 Prec@(1,5) (59.9%, 83.6%)
08/03 12:42:15 AM | Train: [13/210] Step 040/404 Loss 1.686 Prec@(1,5) (58.8%, 82.9%)
08/03 12:43:02 AM | Train: [13/210] Step 050/404 Loss 1.707 Prec@(1,5) (58.1%, 82.9%)
08/03 12:43:47 AM | Train: [13/210] Step 060/404 Loss 1.750 Prec@(1,5) (56.8%, 82.4%)
08/03 12:44:32 AM | Train: [13/210] Step 070/404 Loss 1.774 Prec@(1,5) (56.6%, 82.3%)
08/03 12:45:17 AM | Train: [13/210] Step 080/404 Loss 1.810 Prec@(1,5) (56.3%, 81.3%)
08/03 12:46:00 AM | Train: [13/210] Step 090/404 Loss 1.799 Prec@(1,5) (56.5%, 81.6%)
08/03 12:46:48 AM | Train: [13/210] Step 100/404 Loss 1.798 Prec@(1,5) (56.4%, 81.6%)
08/03 12:47:34 AM | Train: [13/210] Step 110/404 Loss 1.818 Prec@(1,5) (55.5%, 81.1%)
08/03 12:48:22 AM | Train: [13/210] Step 120/404 Loss 1.813 Prec@(1,5) (55.7%, 81.1%)
08/03 12:49:06 AM | Train: [13/210] Step 130/404 Loss 1.821 Prec@(1,5) (55.7%, 80.9%)
08/03 12:49:53 AM | Train: [13/210] Step 140/404 Loss 1.830 Prec@(1,5) (55.3%, 80.7%)
08/03 12:50:40 AM | Train: [13/210] Step 150/404 Loss 1.828 Prec@(1,5) (55.3%, 80.6%)
08/03 12:51:24 AM | Train: [13/210] Step 160/404 Loss 1.825 Prec@(1,5) (55.3%, 80.7%)
08/03 12:52:09 AM | Train: [13/210] Step 170/404 Loss 1.820 Prec@(1,5) (55.3%, 80.8%)
08/03 12:52:54 AM | Train: [13/210] Step 180/404 Loss 1.822 Prec@(1,5) (55.3%, 80.8%)
08/03 12:53:39 AM | Train: [13/210] Step 190/404 Loss 1.828 Prec@(1,5) (55.3%, 80.7%)
08/03 12:54:26 AM | Train: [13/210] Step 200/404 Loss 1.826 Prec@(1,5) (55.2%, 80.9%)
08/03 12:55:11 AM | Train: [13/210] Step 210/404 Loss 1.831 Prec@(1,5) (54.9%, 80.7%)
